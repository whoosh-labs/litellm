
2.1.3-24-10-24 / 2024-10-24 10:23:34
No new changes in the release

2.1.4-24-11-04 / 2024-11-04 04:53:20
use litellm with user id
read readme
move custom code raga and sync changes to raga.Dockerfile
move custom code to raga folder
minor change to keep the changes in sync with remote
test: cleanup codestral tests - backend api unavailable
fix linting - remove # noqa PLR0915 from fixed function
def test_text_completion_with_echo(stream): (#6401)
Revert "(refactor) litellm.Router client initialization utils  (#6394)" (#6403)
(fix) Langfuse key based logging  (#6372)
(refactor) litellm.Router client initialization utils  (#6394)
(refactor) move convert dict to model response to llm_response_utils/ (#6393)
(docs + testing) Correctly document the timeout value used by litellm proxy is 6000 seconds + add to best practices for prod  (#6339)
build(deps): bump http-proxy-middleware in /docs/my-website (#6395)
bump: version 1.50.2 → 1.50.3
Litellm dev 10 22 2024 (#6384)
(feat) Arize - Allow using Arize HTTP endpoint  (#6364)
test(test_alangfuse.py): handle flaky langfuse test better
test(skip-flaky-google-context-caching-test): google is not reliable. their sample code is also not working
Add claude 3 5 sonnet 20241022 models for all provides (#6380)
add new 35 mode lcard (#6378)
Revert "(fix) standard logging metadata + add unit testing  (#6366)" (#6381)
(fix) standard logging metadata + add unit testing  (#6366)
fix docs configs.md
(refactor) remove berrispendLogger - unused logging integration  (#6363)
Refactor: apply early return (#6369)
langfuse use helper for get_langfuse_logging_config
bump: version 1.50.1 → 1.50.2
docs(sidebars.js): add jina ai to left nav
docs(sidebars.js): add jina ai embedding to docs
feat(proxy_cli.py): add new 'log_config' cli param (#6352)
refactor(redis_cache.py): use a default cache value when writing to r… (#6358)
fix(proxy_server.py): add 'admin' user to db (#6223)
fix(litellm-helm): correctly use dbReadyImage and dbReadyTag values (#6336)
(fix) get_response_headers for Azure OpenAI  (#6344)
bump: version 1.50.0 → 1.50.1
(testing) add test coverage for init custom logger class  (#6341)
working test for init custom logger
fix init logger tests
add unit tests for init callbacks
fix - unhandled jsonDecodeError in `convert_to_model_response_object`  (#6338)
feat(custom_logger.py): expose new `async_dataset_hook` for modifying… (#6331)
LiteLLM Minor Fixes & Improvements (10/18/2024) (#6320)
Litellm openai audio streaming (#6325)
(refactor) `get_cache_key` to be under 100 LOC function (#6327)
 doc - using gpt-4o-audio-preview (#6326)
bump: version 1.49.7 → 1.50.0
(feat) - allow using os.environ/ vars for any value on config.yaml  (#6276)
(feat) Support audio param in responses streaming (#6312)
(feat) Support `audio`,  `modalities` params (#6304)
doc fix Turn on / off caching per Key. (#6297)
(code quality) add ruff check PLR0915 for `too-many-statements`  (#6309)
add gpt-4o-audio models to model cost map (#6306)
bump: version 1.49.6 → 1.49.7
docs(argilla.md): add sampling rate to argilla calls
docs(argilla.md): add doc on argilla logging
docs(user_keys.md): add regex doc for clientside auth params
LiteLLM Minor Fixes & Improvements (10/17/2024)  (#6293)
Revert "fix(ui_sso.py): fix faulty admin check"
fix(ui_sso.py): fix faulty admin check
(testing) add unit tests for LLMCachingHandler Class (#6279)
test_awesome_otel_with_message_logging_off
(testing) add test coverage for LLM OTEL logging (#6227)
fix otel tests
Revert "(perf) move s3 logging to Batch logging + async [94% faster p… (#6275)
bump: version 1.49.5 → 1.49.6
LiteLLM Minor Fixes & Improvements (10/16/2024)  (#6265)
remove ask mode (#6271)
Litellm router code coverage 3 (#6274)
(testing) add router unit testing for `send_llm_exception_alert` , `router_cooldown_event_callback` , cooldown utils (#6258)
(testing) Router add testing coverage (#6253)
LiteLLM Minor Fixes & Improvements (10/15/2024)  (#6242)
(router testing) Add testing coverage for `run_async_fallback` and `run_sync_fallback`  (#6256)
(refactor) - caching use separate files for each cache class (#6251)
(refactor) sync caching - use `LLMCachingHandler` class for get_cache (#6249)
(testing - litellm.Router ) add unit test coverage for pattern matching / wildcard routing  (#6250)
fix RerankResponse make meta optional (#6248)
bump: version 1.49.4 → 1.49.5
(refactor) caching - use _sync_set_cache  (#6224)
Bump hono from 4.5.8 to 4.6.5 in /litellm-js/spend-logs (#6245)
fix arize handle optional params (#6243)
(fix) prompt caching cost calculation OpenAI, Azure OpenAI  (#6231)
(refactor) OTEL - use safe_set_attribute for setting attributes (#6226)
(refactor) use helper function `_assemble_complete_response_from_streaming_chunks` to assemble complete responses in caching and logging callbacks (#6220)
bump: version 1.49.3 → 1.49.4
test(router_code_coverage.py): check if all router functions are dire… (#6186)
Litellm dev 10 14 2024 (#6221)
fix importing Cache from litellm (#6219)
(refactor router.py ) - PR 3 - Ensure all functions under 100 lines (#6181)
fix code cov components
bump: version 1.49.2 → 1.49.3
(refactor caching) use common `_retrieve_from_cache` helper  (#6212)
add coverage for proxy auth
fix config.yml ci/cd
add caching component to code cov
fix config.yml
update code cov yaml
bump (#6187)
(refactor caching) use LLMCachingHandler for caching streaming responses  (#6210)
fix config.yml
fix prom testing
(feat) prometheus have well defined latency buckets (#6211)
(refactor) caching use LLMCachingHandler for async_get_cache and set_cache  (#6208)
run ci/cd again
fix codecov.yaml
aded codecov yml (#6207)
docs: make it easier to find anthropic/openai prompt caching doc
docs(configs.md): document all environment variables (#6185)
ci(config.yml): add further testing coverage to codecov (#6184)
ci(config.yml): add local_testing tests to codecov coverage check (#6183)
ci(config.yml): run all router tests
build(config.yml): add codecov to repo (#6172)
Litellm Minor Fixes & Improvements (10/12/2024)  (#6179)
bump: version 1.49.1 → 1.49.2
(fix) provider wildcard routing - when models specificed without provider prefix  (#6173)
(fix) batch_completion fails with bedrock due to extraneous [max_workers] key   (#6176)
LiteLLM Minor Fixes & Improvements (10/10/2024)  (#6158)
add azure/gpt-4o-2024-05-13 (#6174)
Revert "(perf) move s3 logging to Batch logging + async [94% faster perf under 100 RPS on 1 litellm instance] (#6165)"
(perf) move s3 logging to Batch logging + async [94% faster perf under 100 RPS on 1 litellm instance] (#6165)
docs fix
Feat: Add Langtrace integration (#5341)
update (#6160)
Add literalai in the sidebar observability category (#6163)
fix pattern match router
bump: version 1.49.0 → 1.49.1
drop imghdr (#5736) (#6153)
fix typing on opik.py
fix _opik logger
fix opik types
[Feat] Observability integration - Opik by Comet (#6062)
(feat) use regex pattern matching for wildcard routing  (#6150)
LiteLLM Minor Fixes & Improvements (10/09/2024)  (#6139)
docs(configs.md): add litellm config / s3 bucket object info in configs.md
doc onboarding orgs
docs rbac
fix rbac doc
ui new build
uo fixes for default team (#6134)
bump: version 1.48.20 → 1.49.0
fix get_all_team_memberships
remove unused file from root
doc - move rbac under auth
fix schema.prisma change
fix literal ai typing errors
(feat proxy) [beta] add support for organization role based access controls  (#6112)
build: bump version
LiteLLM Minor Fixes & Improvements (10/08/2024)  (#6119)
Fix: Literal AI llm completion logging (#6096)
(fix) Fix Groq pricing for llama3.1 (#6114)
trigger ci/cd run
(fix) clean up root repo - move entrypoint.sh and build_admin_ui to /docker  (#6110)
docs(azure.md): add o1 model support to config
bump: version 1.48.18 → 1.48.19
LiteLLM Minor Fixes & Improvements (10/07/2024)  (#6101)
fix using Dockerfile
fix config.yml
(clean up) move docker files from root to `docker` folder (#6109)
(docs) add remaining litellm settings on configs.md doc  (#6108)
(docs) key based callbacks (#6107)
fix links due to broken list (#6103)
bump: version 1.48.17 → 1.48.18
Update readme.md
Update readme.md
(feat proxy) add v2 maintained LiteLLM grafana dashboard (#6098)
(bug fix) TTL not being set for embedding caching requests  (#6095)
ui new build
(proxy ui sso flow) - fix invite user sso flow  (#6093)
(proxy ui) - fix view user pagination (#6094)
ui - fix view user pagination
Proxy: include customer budget in responses (#5977)
docs key logging
correct use of healthy / unhealthy
fix(utils.py): fix  fix pydantic obj to schema creation for vertex en… (#6071)
bump: version 1.48.16 → 1.48.17
Litellm expose disable schema update flag (#6085)
LiteLLM Minor Fixes & Improvements (10/05/2024)  (#6083)
Add pyright to ci/cd + Fix remaining type-checking errors (#6082)
bump: version 1.48.15 → 1.48.16
(code clean up) use a folder for gcs bucket logging + add readme in folder  (#6080)
docs fix
openrouter/openai's litellm_provider should be openrouter, not openai (#6079)
(feat) add azure openai cost tracking for prompt caching (#6077)
linting error fix
(docs) reference router settings general settings etc (#6078)
docs routing config table
add o-1 to Azure docs
(feat) add cost tracking for OpenAI prompt caching  (#6055)
add azure o1 models to model cost map (#6075)
(feat) add /key/health endpoint to test key based logging (#6073)
fix(gcs_bucket.py): show error response text in exception (#6072)
bump: version 1.48.14 → 1.48.15
LiteLLM Minor Fixes & Improvements (10/04/2024)  (#6064)
(feat)  OpenAI prompt caching models to model cost map (#6063)
(docs) router settings - on litellm config  (#6037)
update load test doc
bump: version 1.48.13 → 1.48.14
ci/cd run again
bump: version 1.48.12 → 1.48.13
(fixes) docs + qa - gcs key based logging  (#6061)
(docs) add 1k rps load test doc  (#6059)
fix prometheus track cooldown events on custom logger (#6060)
fix handle case when key based logging vars are set as os.environ/ vars
(fixes)  gcs bucket key based logging  (#6044)
docs(realtime.md): add new /v1/realtime endpoint
fix(utils.py): return openai streaming prompt caching tokens (#6051)
build: version bump
🔧 (model_prices_and_context_window.json): rename gemini-pro-flash to gemini-flash-experimental to reflect updated naming convention (#5980)
Litellm Minor Fixes & Improvements (10/03/2024) (#6049)
OpenAI `/v1/realtime` api support  (#6047)
bump: version 1.48.10 → 1.48.11
(feat) openai prompt caching (non streaming) - add prompt_tokens_details in usage response  (#6039)
fix(factory.py): bedrock:  merge consecutive tool + user messages (#6028)
(contributor PRs) oct 3rd, 2024  (#6034)
(feat) add nvidia nim embeddings  (#6032)
docs prometheus metrics
(feat proxy) add key based logging for GCS bucket  (#6031)
(load testing) add vertex_ai embeddings load test  (#6004)
(azure): Enable stream_options for Azure OpenAI. (#6024) (#6029)
bump: version 1.48.9 → 1.48.10
LiteLLM Minor Fixes & Improvements (10/02/2024)  (#6023)
(testing): Enable testing us.anthropic.claude-3-haiku-20240307-v1:0. (#6018)
docs(code_quality.md): add doc on litellm code qa
bump: version 1.48.8 → 1.48.9
Litellm ruff linting enforcement (#5992)
build(custom_guardrail.py): include missing file
bump: version 1.48.7 → 1.48.8
(performance improvement - vertex embeddings)  ~111.11% faster  (#6000)
docs(data_security.md): cleanup docs
docs(data_security.md): update faq doc
(feat proxy slack alerting) - allow opting in to getting key / internal user alerts  (#5990)
Fix: skip slack alert if there was no spend (#5998)
(docs) prometheus metrics document all prometheus metrics  (#5989)
add Azure OpenAI entrata id docs (#5985)
add docs on privacy policy
fix grammar on health check docs (#5984)
docs(response_headers.md): add response headers to docs
Fixed minor typo in bash command to prevent overwriting .env file (#5902)
bump: version 1.48.6 → 1.48.7
Litellm Minor Fixes & Improvements (09/24/2024) (#5963)
ci/cd run again
ci/cd run again
(feat prometheus proxy) track remaining team and key alias in deployment failure metrics (#5967)
fix(parallel_request_limiter.py): only update hidden params, don't set new (can lead to errors for responses where attribute can't be set)
test: run test first
test: refactor test
fix(parallel_request_limiter.py): make sure hidden params is dict before dereferencing
fix(caching.py): cleanup print_stack()
docs(reliability.md): add tutorial on setting wildcard models as fallbacks
fix(router.py): skip setting model_group response headers for now
fix(utils.py): fix updating hidden params
fix(router.py): handle setting response headers during retries
fix(azure): return response headers for sync embedding calls
fix(utils.py): guarantee openai-compatible headers always exist in response
fix(return-openai-compatible-headers): v0 is openai, azure, anthropic
fix(parallel_request_limiter.py): return remaining tpm/rpm in openai-compatible way
build(config.yml): fix build_and_test part of tests
refactor(test_stream_chunk_builder.py): fix import
refactor: fix imports
refactor: move all testing to top-level of repo
refactor(test_proxy_utils.py): place at root level test folder
fix(litellm_logging.py): fix linting error
fix(batch_redis_get.py): handle custom namespace
ci/cd run again
add sentry sdk to litellm docker (#5965)
ci/cd run again
fix sso sign in tests
bump: version 1.48.5 → 1.48.6
(feat prometheus proxy) track remaining team and key alias in deployment failure metrics (#5967)
(proxy prometheus) track api key and team in latency metrics (#5966)
(feat proxy prometheus) track virtual key, key alias, error code, error code class on prometheus  (#5968)
docs clean up langfuse.md
[Feat Proxy] Allow using hypercorn for http v2  (#5950)
fix redis async_set_cache_pipeline when empty list passed to it (#5962)
(perf improvement proxy) use one redis set cache to update spend in db (30-40% perf improvement)  (#5960)
(fix proxy) model_group/info support rerank models (#5955)
fix use one async async_batch_set_cache (#5956)
bump: version 1.48.4 → 1.48.5
LiteLLM Minor Fixes & Improvements (09/27/2024)  (#5938)
fix(proxy/utils.py): fix create missing views check (#5953)
fix test_vertexai_multimodal_embedding_base64image_in_input
bump 1.48.3 -> 1.48.4
bump: version 1.48.3 → 1.48.4
bump: version 1.49.0 → 1.49.1
bump: version 1.48.3 → 1.49.0
[Vertex Multimodal embeddings] Fixes to work with Langchain OpenAI Embedding  (#5949)
LiteLLM Minor Fixes & Improvements (09/26/2024)  (#5925) (#5937)
allow setting LANGFUSE_FLUSH_INTERVAL (#5944)
[Perf Proxy] parallel request limiter - use one cache update call (#5932)
docs(vertex.md): fix codestral fim placement (#5946)
docs - show correct rpm - > tpm conversion for Azure
docs: resolve imports
docs(data_security.md): add legal/compliance faq's
bump: version 1.48.2 → 1.48.3
handle streaming for azure ai studio error
[Fix Azure AI Studio]  drop_params_from_unprocessable_entity_error (#5936)
Upgrade dependencies in dockerfile (#5862)
[docs] updated langfuse integration guide (#5921)
[Fix] Perf use only async functions for get cache  (#5930)
[Fix proxy perf] Use correct cache key when reading from redis cache  (#5928)
track api key and alias in remaining tokens metric (#5924)
LiteLLM Minor Fixes & Improvements (09/25/2024)  (#5893)
LiteLLM Minor Fixes & Improvements (09/24/2024) (#5880)
build(model_prices_and_context_window.json): add new gemini - google ai studio models
Update litellm helm envconfigmap (#5872)
Add Llama 3.2 90b model on Vertex AI. (#5908)
ci/cd run again
bump: version 1.48.1 → 1.48.2
[Perf improvement Proxy] Use Dual Cache for getting key and team objects (#5903)
build(model_prices_and_context_window.json): add new gemini models
Add gemini-1.5-pro-002 and gemini-1.5-flash-002 (#5879)
[Feat] add fireworks llama 3.2 models + cost tracking  (#5905)
docs service accounts (#5900)
fix - add stricter type check for OTEL when args[0] is not dict
[Feat] Improve OTEL Tracking - Require all Redis Cache reads to be logged on OTEL  (#5881)
[Feat-Router] Allow setting which environment to use a model on  (#5892)
feat(vertex): Use correct provider for response_schema support check (#5815) (#5829)
docs show all configs
docs show relevant litellm_settings
docs(user_keys.md): add docs on configurable clientside auth credentials
bump: version 1.48.0 → 1.48.1
chore - handle case when otel metadata field value is None
[Perf Fix] Don't always read from Redis by Default  (#5877)
[Fix] OTEL - Don't log messages when callback settings disable message logging  (#5875)
LiteLLM Minor Fixes & Improvements (09/23/2024)  (#5842) (#5858)
add debian fixes to non root image
ui add deepseek provider (#5857)
Update the dockerignore to minimise the amount of data transfered to the docker context (#5863)
bump: version 1.47.3 → 1.48.0
fix linting
fix imports
bump: version 1.47.2 → 1.47.3
ui new build
docker - handle debian issue on docker builds (#5752)
[Feat] Admin UI - Add Service Accounts  (#5855)
[Feat UI sso] store 'provider' in user metadata (#5856)
[Feat-Proxy]  add service accounts backend (#5852)
[Feat] SSO - add `provider` in the OpenID field for custom sso  (#5849)
ui new build
ui networking list all teams (#5851)
[Testing-Proxy] Add E2E Admin UI testing  (#5845)
testing - nvidia nim api use mock testing
fix prometheus track input and output tokens (#5780)
test(test_otel.py): fix test
ui new build
docs(virtual_keys.md): add enable/disable virtual keys to docs + refactor sidebar
bump: version 1.47.1 → 1.47.2
Cost tracking improvements (#5828)
LiteLLM Minor Fixes & Improvements (09/21/2024)  (#5819)
mark test_completion_sagemaker_prompt_template_non_stream as flaky
handle hf rate limit error
ci/cd run again
fix re-add virtual key auth checks on vertex ai pass thru endpoints (#5827)
bump: version 1.47.0 → 1.47.1
[fix-sso] Allow internal user viewer to view usage routes  (#5825)
fix premium user check for tags on keys (#5826)
[SSO-UI] Set new sso users as internal_view role users  (#5824)
[Feat] Allow setting custom arize endpoint (#5709)
mark test_async_completion_azure_caching as flaky
[Feat] Prometheus - show status code and class type on prometheus  (#5806)
[Feat] Add testing for prometheus failure metrics  (#5823)
[Feat] Allow setting `supports_vision` for Custom OpenAI endpoints + Added testing  (#5821)
Fixed DeepSeek input and output tokens (#5718)
Correct casing (#5817)
[Feat] Add fireworks AI embedding (#5812)
docs(proxy/configs.md): add CONFIG_FILE_PATH tutorial to docs
bump: version 1.46.8 → 1.47.0
LiteLLM Minor Fixes & Improvements (09/20/2024)  (#5807)
refactor: cleanup root of repo (#5813)
build(schema.prisma): add column 'blocked' for litellm keys
[Feat-Proxy] Allow using custom sso handler  (#5809)
use .debug for update_database() (#5810)
fix model cost map fireworks embeddings
add fireworks_ai-embedding-models
add fireworks embedding pricing
[Fix] Tag Based Routing not work with wildcard routing  (#5805)
LiteLLM Minor Fixes & Improvements (09/19/2024)  (#5793)
ui new build
ui fix correct team not loading (#5804)
bump: version 1.46.7 → 1.46.8
ui new build
[Feat] Add Error Handling for /key/list endpoint (#5787)
[ Proxy - User Management]: If user assigned to a team don't show Default Team (#5791)
[Feat] Add proxy level prometheus metrics (#5789)
test fix test_multiple_deployments_sync
fix azure gpt-4o test
fix curl on /get team info (#5792)
test: replace gpt-3.5-turbo-0613 (deprecated model) (#5794)
docs docker quick start
docs fix  link on root page
docs add docker quickstart to litellm proxy getting started
bump: version 1.46.6 → 1.46.7
docs(vertex.md): fix example with GOOGLE_APPLICATION_CREDENTIALS
fix root of docs page
[Feat] Add Azure gpt-35-turbo-0301 pricing (#5790)
LiteLLM Minor Fixes & Improvements (09/18/2024)  (#5772)
add gemma2 9b it (#5788)
fix use converse for all llama3 models (#5729)
feat(prometheus_api.py): support querying prometheus metrics for all-up + key-level spend on UI (#5782)
set timeout on predibase test
bump: version 1.46.5 → 1.46.6
docs fallback/login
docs add info on `/fallback/login`
[Chore-Proxy] enforce jwt auth as enterprise feature (#5770)
[Chore LiteLLM Proxy] enforce prometheus metrics as enterprise feature  (#5769)
[Feat-Proxy] Add Azure Assistants API - Create Assistant, Delete Assistant Support  (#5777)
[Prometheus] track requested model (#5774)
[Feat - GCS Bucket Logger] Use StandardLoggingPayload  (#5771)
fix(litellm_logging.py): fix merge conflict
update gcs bucket to use standard logging payload
docs update what gets logged on gcs buckets
docs update standard logging object
docs clarify how virtual key is read from cache / db
docs(azure_ai.md): add rerank api endpoint to docs
bump: version 1.46.4 → 1.46.5
Additional Fixes (09/17/2024)  (#5759)
LiteLLM Minor Fixes & Improvements (09/17/2024)  (#5742)
bump: version 1.46.3 → 1.46.4
bump: version 1.46.2 → 1.46.3
update datadog docs
[Feat] Log Request metadata on gcs bucket logging (#5743)
[Fix] Router/ Proxy - Tag Based routing, raise correct error when no deployments found and tag filtering is on  (#5745)
[Feat-Proxy-DataDog] Log Redis, Postgres Failure events on DataDog  (#5750)
[Fix] o1-mini causes pydantic warnings on `reasoning_tokens`  (#5754)
Bump next from 14.1.1 to 14.2.10 in /ui/litellm-dashboard (#5753)
Litellm fix router testing (#5748)
test(test_router_debug_logs.py): move to mock response
Revert "fix - deal with case when check view exists returns None (#5740)" (#5741)
fix - deal with case when check view exists returns None (#5740)
bump: version 1.46.1 → 1.46.2
LiteLLM Minor Fixes & Improvements (09/16/2024)  (#5723) (#5731)
fix guardrail linting change
fix gemini 1.5 flash test
ci/cd run again
fix linting
bump: version 1.46.0 → 1.46.1
[Feat-Proxy] Slack Alerting - allow using os.environ/ vars for alert to webhook url  (#5726)
[Fix-Proxy] Azure Key Management - Secret Manager  (#5728)
fix gemini 1.5 flash supports_response_schema
fix test_all_model_config
fix test_all_model_configs
[Fix-Proxy] log exceptions from azure key vault on verbose_logger.exceptions  (#5719)
[Feat-Proxy] Add upperbound key duration param (#5727)
Warning fix for Pydantic 2.0 (#5679) (#5707)
Add unsupported params. (#5722)
docs(docker_quick_start.md): update quick start with azure connection error
build(model_prices_and_context_window.json): bump claude-3-5-sonnet max tokens
(models): Enable JSON Schema Support for Gemini 1.5 Flash Models (#5708)
Litellm stable dev (#5711)
mark test as flaky
docs
docs max_completion_tokens
bump: version 1.45.0 → 1.46.0
[Feat-Prometheus] Add prometheus metric for tracking cooldown events  (#5705)
[Feat-Prometheus] Track exception status on `litellm_deployment_failure_responses` (#5706)
fic otel load test %
[Fix] Router cooldown logic - use % thresholds instead of allowed fails to cooldown deployments  (#5698)
sambanova support (#5547) (#5703)
[Feat] Add `max_completion_tokens` param  (#5691)
Update model_prices_and_context_window.json (#5700)
LiteLLM Minor Fixes and Improvements (09/14/2024)  (#5697)
LiteLLM Minor Fixes and Improvements (09/13/2024)  (#5689)
(models): Added missing gemini experimental models + fixed pricing for gemini-1.5-pro-exp-0827 (#5693)
[Feat - Perf Improvement] DataDog Logger 91% lower latency  (#5687)
[Fix] Performance -  use in memory cache when downloading images from a url  (#5657)
build: bump from 1.44.28 -> 1.45.0
LiteLLM Minor Fixes and Improvements (09/12/2024)  (#5658)
Add o1 models on OpenRouter. (#5676)
fix(user_dashboard.tsx): don't call /global/spend on startup (#5668)
fix(proxy/utils.py): auto-update if required view missing from db. raise warning for optional views. (#5675)
bump: version 1.44.27 → 1.44.28
[Fix-Router] Don't cooldown when only 1 deployment exists  (#5673)
docs add o1 to docs
[Feat-Perf] Use Batching + Squashing  (#5645)
fix gcs logging
fix type errors
fix handle user message
bump openai to 1.45.0
fix linting
fix handle o1 not supporting system message
bump: version 1.44.26 → 1.44.27
fix pricing
add o1 reasoning tests
Refactor 'check_view_exists' logic  (#5659)
Fix token and remove dups. (#5662)
add OpenAI o1 config
(models): Add o1 pricing. (#5661)
gpt o1 and o1 mini
mark test as flaky
fix config.yml
ci/cd run again
fix testing
ci/cd run again
make separate assistants testing pipeline
fix respx
fix router tests
fix ci/cd tests
fix config.yml
add litellm router testing
bump: version 1.44.25 → 1.44.26
LiteLLM Minor Fixes and Improvements (11/09/2024)  (#5634)
bump: version 1.44.24 → 1.44.25
fix otel load test
fix otel tests
fix langsmith load tests
fix langsmith load test
fix load test
print load test results
add load tests to ci/cd
add otel load test
add langsmith logging test
fix move logic to custom_batch_logger
Add the option to specify a schema in the postgres DB, also modify docs (#5640)
use vars for batch size and flush interval seconds
fix otel use sensible defaults
fix vtx test
fix langsmith tenacity
fix requirements.txt
fix testing + req.txt
use lock to flush events to langsmith
add better debugging for flush interval
fix installing litellm
use tenacity for langsmith
fix langsmith clear logged queue on success
LiteLLM Minor Fixes and Improvements (09/10/2024) (#5618)
Add gemini 1.5 flash exp 0827 (#5636)
langsmith use batching for logging
fix langsmith_batch_size
stash - langsmith use batching for logging
docs: update ai21 docs
ci/cd run again
Bump body-parser and express in /docs/my-website
Bump serve-static and express in /docs/my-website
Bump send and express in /docs/my-website
bump: version 1.44.23 → 1.44.24
add doc string to vertex llm base
fix gemini streaming test
fix test get token url
fix gemini test
fix case when gemini is used
fix vertex use async func to set auth creds
fix bedrock get async client
fix types for vertex project id
fix getting params
fix vertex only refresh auth when required
fix linting error
fix get_async_httpx_client
fix test
use get async httpx client
use get_async_httpx_client for logging httpx
pass llm provider when creating async httpx clients
fix rps / rpm values on load testing
Updating Cohere models, prices, and documentation
add enum for all llm providers LlmProviders
rename get_async_httpx_client
fix vertex ai use _get_async_client
fix #5614 (#5615)
add test test_regenerate_key_ui
fix regen keys when no duration is passed
bump: version 1.44.22 → 1.44.23
LiteLLM Minor Fixes and Improvements (09/09/2024)  (#5602)
LiteLLM Minor Fixes and Improvements (09/07/2024)  (#5580)
fix test_awesome_otel_with_message_logging_off
fix otel logging test
run test in verbose mode
fix test otel message logging off
fix team based logging doc
add test for using success and failure
Properly use `allowed_fails_policy` when it has fields with a value of 0 (#5604)
fix log failures for key based logging
fix otel test
fix otel defaults
add doc on redacting otel message / response
use callback_settings  when intializing otel
fix init custom logger when init OTEL runs
use redact_message_input_output_from_custom_logger
refactor redact_message_input_output_from_custom_logger
add message_logging on Custom Logger
update test_default_tagged_deployments
add "default" tag
test test_default_tagged_deployments
support default deployments
build(deployment.yaml): Fix port + allow setting database url in helm chart (#5587)
fix taf based routing debugging
fix debug statements
fix test_async_prometheus_success_logging_with_callbacks
fix create script for pre-creating views
support using "callbacks" for prometheus
docs architecture
bump: version 1.44.21 → 1.44.22
Revert "fix(router.py): return model alias w/ underlying deployment on router.get_model_list()"
fix(router.py): return model alias w/ underlying deployment on router.get_model_list()
docs(deploy.md): add published non-root docker image to docs
add /key/list endpoint
High Level architecture
ui new build
docs add arch diagram
add arch diagram
ui allow setting input / output cost per M tokens
add doc on spend report frequency
add spend_report_frequency as a general setting
fix slack alerting allow setting custom spend report frequency
docs better sidebar
docs cleanup
docs organize sidebar
ui cleanup
mark test_langfuse_masked_input_output
add config for setting up redis cluster
allow setting password for redis cluster
add test_redis_cache_cluster_init_with_env_vars_unit_test
fix allow using .env vars for redis cluster
litellm-helm: fix missing resource definitions in initContainer and missing DBname value for envVars in deployment.yaml (#5562)
fix missing class object instantiation in custom_llm_server provider documentation's quick start (#5578)
allow setting REDIS_CLUSTER_NODES in .env
fix(langsmith.py): support sampling langsmith traces (#5577)
 Allow client-side credentials to be sent to proxy (accept only if complete credentials are given) (#5575)
bump: version 1.44.20 → 1.44.21
bump: version 1.44.19 → 1.44.20
ui new build
fix otel set max_queue_size, max_queue_size
LiteLLM Minor Fixes and Improvements (08/06/2024)  (#5567)
fix(navbar.tsx): only show 'get enterprise license' if user is not already a premium user (#5568)
fix otel max batch size
fix ui type
fix linting
fix azure batches test - don't have more quota
fix linting errors
fix use view for getting tag usage
use view for getting tag usage on ui
add cost tracking for rerank+ test
fix RerankResponse type
add cost tracking for rerank
basic cohere rerank logging
Clean formatting
Update backup sheet
fix datadog log exceptions
Update pricing and add cohere refresh models
fix regen keys
working regen flow
ui allow rotating keys
use form correctly
allow correct fields on regenerate key
allow passing expiry time to /key/regenerate
run ci/cd again
run ci cd again
fix otel type
bump: version 1.44.18 → 1.44.19
LiteLLM Minor Fixes and Improvements  (#5537)
Update lago.py to accomodate API change (#5495) (#5543)
LiteLLM Merged PR's (#5538)
docs add video for key based logging
ui new build
ui show when key expires
fix on /user/info show all keys - even expired ones
run ci/cd on main
move prisma test to correct location
add error message on test
run ci - cd again
run ci/cd again
use requirements txt
run ci/cd agaiin
move folder key gen prisma is in
run test again
run again
add step for ui testing
add test for ui usage endpoints
add test for internal vs admin user
fix tests on viewing spend logs
add ui testing folder
fix test_call_with_key_over_budget
ui add a check for isAdminOrAdminViewer
fix allow internal user to view their own usage
fix /global/spend/provider
add global/spend/provider
allow internal user to view global/spend/models
allow internal user to view their own spend
add usage endpoints for internal user
show /spend/logs for internal users
fix create view - MonthlyGlobalSpendPerUserPerKey
use helper functions per endpoint
add /spend/tags as allowed route for internal user
fix allow internal user and internal viewer to view usage
LiteLLM Minor Fixes and Improvements  (#5537)
Update lago.py to accomodate API change (#5495) (#5543)
LiteLLM Merged PR's (#5538)
docs add video for key based logging
ui new build
fix log /audio to langfuse
ui show when key expires
move prisma test to correct location
fix typing error on test
run ci/cd on main
fix typing error on test
add error message on test
run ci - cd again
run ci/cd on main
run ci/cd again
use requirements txt
fix on /user/info show all keys - even expired ones
run test again
run ci/cd agaiin
move folder key gen prisma is in
run test again
docs(configs.md): update to clarify you can use os.environ/ for any config value
run again
add step for ui testing
add test for ui usage endpoints
fix import
test(test_function_call_parsing.py): handle anthropic internal server error
Update utils.py (#5530)
add test for internal vs admin user
fix tests on viewing spend logs
add ui testing folder
fix test_call_with_key_over_budget
ui add a check for isAdminOrAdminViewer
fix allow internal user to view their own usage
fix /global/spend/provider
add global/spend/provider
fix import
docs(configs.md): update to clarify you can use os.environ/ for any config value
allow internal user to view global/spend/models
allow internal user to view their own spend
add usage endpoints for internal user
show /spend/logs for internal users
fix create view - MonthlyGlobalSpendPerUserPerKey
use helper functions per endpoint
add /spend/tags as allowed route for internal user
fix import
fix import error
fix import error
fix allow internal user and internal viewer to view usage
fix import error
test(test_function_call_parsing.py): handle anthropic internal server error
fix linting error
use correct type hints for audio transcriptions
Update utils.py (#5530)
docs(pass_through/bedrock.md): add bedrock agents support
bump: version 1.44.17 → 1.44.18
fix(pass_through_endpoints): support bedrock agents via pass through (#5527)
LiteLLM Minor fixes + improvements (08/04/2024) (#5505)
mark test_team_logging as flaky
fix allow general guardrails on free tier
bump: version 1.44.16 → 1.44.17
fix /health error
show more descriptive error messages on /health checks
return error from /global/spend endpoint
return error client side from spend endpoints
show error from /spend/tags
rename type
add doc on PassthroughStandardLoggingObject
feat log request / response on pass through endpoints
security - Prevent sql injection in `/team/update` query (#5513)
stream response (#5516)
fix init presidio guardrail
allow init guardrails with output parsing logic
docs(logging.md): fix name in docs
handle logging_only logic for guardrails
docs update presidio
doc setting language per request
docs new presidio language controls
fix allow setting language per call to presidio
fix presidio calling logic
fix test_proxy_logging_setup
remove conversational-task
migrate presidio to new guardrails
Add azure/gpt-4o-2024-08-06 pricing. (#5510)
fix get llm provider logic
test get llm provider
style: ci/cd run again
test: skip flaky test
dual cache use always read redis as True by default
test for pl obj
add always read redis test
ci/cd run again
LiteLLM Minor fixes + improvements (08/03/2024)  (#5488)
fix router debug logs
fix route debug logs
bump: version 1.44.15 → 1.44.16
fix req.txt
bump langfuse sdk version on docker
docs control routes on proxy
reset general settings post test
add test for admin only routes
add check for admin only routes
add test for allowed routes
fix test google secret manager
allow setting allowed routes on proxy
Bump pagefind from 1.1.0 to 1.1.1 in /docs/my-website
mark test as flaky
docs secret manager link
test secret manager
refactor secret managers
read from .env for secret manager
refactor get_secret
add sync_construct_request_headers
fix(proxy/_types.py): add lago 'charge_by' env var to proxy ui
docs(bedrock.md): add multimodal embedding support to docs
docs(batches.md): add loadbalancing multiple azure deployments on batches api to docs
docs(azure.md): add docs on azure token refresh
docs(routing.md): add proxy loadbalancing tutorial
refactor: ci/cd run again
docs(enterprise.md): clarify how enterprise deployments work
test: fix test
docs(json_mode.md): update docs
fix(router.py): fix inherited type (#5485)
feat(router.py): Support Loadbalancing batch azure api endpoints (#5469)
bump: version 1.44.14 → 1.44.15
fix always read redis
mark test as flaky
fix success handler typing
fix linting errors
fix linting
add doc with support imagen models
fix linting error
fix linting error
fix linting error
add cost tracking for pass through imagen
fix get llm provider for imagen
fix get_llm_provider for imagegeneration@006
track image gen in spend logs
refactor vtx image gen
refactor vertex to use spearate image gen folder
fix lining
fix linting error
track /embedding in spendLogs
code cleanup
add test for pass through streaming usage tracking
pass through track usage for streaming endpoints
use chunk_processort
new streaming handler fn
LiteLLM Minor Fixes + Improvements  (#5474)
Azure Service Principal with Secret authentication workflow. (#5131) (#5437)
build(model_prices_and_context_window.json): fix token information
Add pricing for ft:gpt-3.5-turbo-* (#5471)
fix pass through construct_target_url when vertex_proj is None
docs add docs on supported params
docs update ai21 doc
fix linting
add ai21 model test
add streaming test for ai21
add all ai21 params
test ai21
refactor ai21
add ai21 provider
add ai21_chat as new provider
docs - update /health docs to show correct info
(gemini): Fix Cloudflare AI Gateway typo. (#5429)
docs(security.md): Adds security.md file to project root
LiteLLM minor fixes + improvements (31/08/2024)  (#5464)
fix response_format={'type': 'json_object'} not working for Azure models (#5468)
Bedrock Embeddings refactor + model support  (#5462)
Minor LiteLLM Fixes and Improvements (#5456)
bump: version 1.44.13 → 1.44.14
add cerebras cost tracking
mark flaky test as flaky
docs add cerebras
test: skip test on end of life model
anthropic prompt caching cost tracking (#5453)
test: skip test on end of life model
anthropic prompt caching cost tracking (#5453)
skip end of life model in test
docs add litellm_error_code_metric_total
test: skip test on end of life model
anthropic prompt caching cost tracking (#5453)
feat prometheus add metric for failure / model
ci/cd run again
fix test
add cerebras api
add cerebras config
fix cost tracking for vertex ai native
forget to keep existing search - bring it back
update canary
fix /spend logs call
fix vertex ai test
mark as async
bump: version 1.44.12 → 1.44.13
call spend logs endpoint
fix tests
add test for vertex basic pass throgh
fix use existing custom_auth.py
allow pass through routes as LLM API routes
fix test_vertexai_embedding_embedding_latest_input_type
use helper class for pass through success handler
add example custom
ci/cd run again
docs add task type for vertex ai
add VertexAITextEmbeddingConfig
fix map input_type to task_type for vertex ai
fix dir structure for tts
fix allow qdrant api key to be optional
vertex forward all headers from vertex
update doc
doc using gcs bucket config.yaml
add gcs bucket base
use helper to get_config_file_contents_from_gcs
add test for test_vertexai_multimodal_embedding_text_input
- merge - fix TypeError: 'CompletionUsage' object is not subscriptable #5441  (#5448)
chore: Clarify support-related Exceptions in utils.py (#5447)
docs(routing.md): add weight-based shuffling to docs
test: mark flaky tests
add tests to check ai21 models cost is calculated correct
Add pricing for Openai ft:gpt-4o
bump: version 1.44.11 → 1.44.12
fix: Minor LiteLLM Fixes + Improvements (29/08/2024)  (#5436)
update docs
use correct vtx ai21 pricing
add pricing for vertex ai 21
show all error types on swagger
mark test_cost_tracking_with_caching as flaky
bump: version 1.44.10 → 1.44.11
fix indentation
fix auth checks for provider routes
add docs on pass thtough
add test for vertex sdk foward headers
vertex add vertex endpoints
docs(docker_quick_start.md): add new quick start doc for litellm proxy
mark test_key_info_spend_values_streaming as flaky
fix team based tag routing
docs tag based routing per team
enable_tag_filtering
doc Tag Based Routing
fix missing link on docs
add test_chat_completion_with_no_tags
fix get_deployments_for_tag
add test for tag based routing
define tags on model list
add_team_based_tags_to_metadata
add set / update tags for a team
allow settings tags per team
add test_team_tags to set / update tags
add test for health check
(bedrock): Add new cross-region inference support for Bedrock.
add support for fireworks ai health check
add util to pick_cheapest_model_from_llm_provider
add fireworks_ai_models
fix(google_ai_studio): working context caching (#5421)
re-add TGI
fix(utils.py): correctly log streaming cache hits (#5417) (#5426)
bump: version 1.44.9 → 1.44.10
fix(team_endpoints.py): update to include the budget in the response
test(test_amazing_vertex_completion.py): fix test
feat(team_endpoints.py): return team member budgets in /team/info call
fix(vertex_ai_partner_models.py): fix vertex import
fix(router.py): fix cooldown check
bump: version 1.44.8 → 1.44.9
prometheus - safe update start / end time
add hook for oauth2 proxy
fix vertex ai test
(models): Add gemini-1.5-pro-exp-0827 pricing.
fix batch creation azure
fix failing vertex test
new ui build
test(test_amazing_vertex_completion.py): ignore experimental gemini models in test
test(test_amazing_vertex_completion.py): update test to not pick experimental gemini models
add test_user_api_key_auth_fails_with_prohibited_params
fix checking request body
add checks for safe request body
add check for is_request_body_safe
docs(reliability.md): cleanup docs
retry flaky tests 3 times
doc add ssml usage
build(model_prices_and_context_window.json): bedrock/llama3 models - region-based pricing
build(model_prices_and_context_window.json): fix bedrock/llama3-1 pricing
simpify ssml usage
update validate_vertex_input
add ssml support on docs
use ssml with litellm vertex
fix(key_management_endpoints.py): expose 'key' param, for setting your own key value
fix(rds_iam_token.py): support common aws env var's - AWS_ROLE_ARN, AWS_WEB_IDENTITY_TOKEN_FILE
add vertex ssml test
add ssml input on vertex tts
fix(rds_iam_token.py): fix boto3 client init for rds
test: fix assert string on test
fix(proxy/utils.py): fix model dump to exclude none values
fix ui
ui fixes
ui fix viewing budget info
ui show Budget Reset
Also have pricing details
Support for gemini experimental models
test: rename test to run earlier
test: fix test
build(model_prices_and_context_window.json): add bedrock mistral small
fix pass through rerank requests tests
stash change
huggingface -> Hugging Face
update outdated readme
mark vertex tests as flaky
docs(vertex_ai.md): fix dead link
mark test as flaky
allow editing budget duration
update cookbook
test(test_embeddings.py): fix test
docs: add time.sleep() between streaming calls
ci/cd run again
ci/cd run again
test test_image_generation_azure_dall_e_3
fix flaky tests
handle flaky pytests
test: fix test
undo chage on config.yaml
allow running contributor PRs
def test_get_bearer_token(): fix
fix(main.py): simplify to just use `/batchEmbedContent`
skip litellm.Timeout error
update doc on palm provider
fix palm api is deactivated by google
fix(__init__.py): fix import
test: fix test
feat(batch_embed_content_transformation.py): support google ai studio /batchEmbedContent endpoint
fix test_completion_vllm
feat(embeddings_handler.py): support async gemini embeddings
add test for rerank on custom api base
fix(embeddings_handler.py): initial working commit for google ai studio text embeddings /embedContent endpoint
docs add tg ai rerank docs
add rerank on cohere docs
build(deps): bump webpack from 5.93.0 to 5.94.0 in /docs/my-website
bump: version 1.44.7 → 1.44.8
docs add rerank api to docs
fix linting
add /rerank test
test(test_exceptions.py): loosen test
feat - add rerank on proxy
fix: initial commit
v0 add rerank on litellm proxy
fix install on 3.8
fix(openai.py): fix error re-raising
add async support for rerank
feat(vertex_ai_and_google_ai_studio): Support Google AI Studio Embeddings endpoint
add rerank params
add rerank api tests
add tg ai rerank support
test(test_proxy_exception_mapping): loosen assert
fix(bedrock_httpx.py): support 'Auth' header as extra_header
add main cohere ai rerank handler + test
add basic cohere rerank
fix(azure_text.py): fix streaming parsing
test(test_router_debug_logs.py): simplify test
add mock test for ai21
fix(router.py): fix aembedding type hints
fix(openai.py): fix post call error logging for aembedding calls
add jamba-1.5
docs(bedrock.md): add doc on passing extra headers + custom api endpoints to bedrock
use cost per token for jamba
add doc on using jamba-1.5-large
docs(anthropic.md): cleanup docs
add jamba-1.5-mini models
fix(anthropic.py): support setting cache control headers, automatically
fix(azure.py): fix raw response dump
add test for test_partner_models_httpx_ai21
refactor partner models to include ai21
fix: fix linting errors
test: fix test
docs langfuse link
fix(sagemaker.py): fix streaming logic
docs(gemini.md): add context caching on google ai studio to docs
fix(cooldown_cache.py): fix linting errors
build(config.yml): bump anyio version
fix(asyncify.py): fix linting errors
fix(asyncify.py): fix linting errors
perf(sagemaker.py): asyncify hf prompt template check
test: fix test
fix(factory.py): handle missing 'content' in cohere assistant messages
fix: fix imports
fix: fix imports
fix: fix unbound var
feat(vertex_ai_context_caching.py): check gemini cache, if key already exists
feat(vertex_ai_context_caching.py): support making context caching calls to vertex ai in a normal chat completion call (anthropic caching format)
fix(types/utils.py): map finish reason to openai compatible
fix: fix imports
fix(streaming_utils.py): fix generic_chunk_has_all_required_fields
run ci/cd again
fix created_at and updated_at not existing error
fix: fix unbound var
ci/cd run again
fix entrypoint
feat(vertex_ai_context_caching.py): check gemini cache, if key already exists
build(deps): bump micromatch in /ui/litellm-dashboard
bump: version 1.44.6 → 1.44.7
ui new build
fix regen api key flow
enforce regenerating keys in enterprise tier
make regenerating api keys enterprise
feat(vertex_ai_context_caching.py): support making context caching calls to vertex ai in a normal chat completion call (anthropic caching format)
regenerate key
update ui regen key
update key name when regenerating a key
ui regenerate an api key
working regenerate key flow
allow using hashed api keys on regen key
test test_regenerate_api_key
add regenerate_key_fn
fix schema
add key_state created at to token
update schema
fix refactor cohere
fix(utils.py): fix message replace
fix(sagemaker.py): support streaming for messages api
Add pricing for imagen-3 and imagen-3-fast
use common folder for cohere
refactor cohere to be in a folder
fix(utils.py): support 'PERPLEXITY_API_KEY' in env
vertex add finetuned models
docs: fix dead links
add test for test_completion_fine_tuned_model
add fine tuned vertex model support
fix(vertex_httpx.py): use special param
test(test_function_calling.py): fix test
fix(utils.py): fix value check
fix(main.py): fix linting errors
feat(utils.py): support gemini/vertex ai streaming function param usage
fix link on getting started
docs using litellm sdk with litellm proxy
feat(vertex_httpx.py): support functions param for gemini google ai studio + vertex ai
docs use litellm proxy with litellm python sdk
fix qdrant semantic cache
fix(vertex_httpx.py): return project id, if given
docs - explain how custom guardrail is mounted
ci/cd run again
fix(vertex_httpx.py): use dynamic project id
fix: fix tests
fix(utils.py): fix linting errors
test test_cooldown_same_model_name
fix(cooldown_cache.py): fix linting errors
fix(router.py): enable dynamic retry after in exception string
bump: version 1.44.5 → 1.44.6
ui new build
fix linting errors when adding a new team member
fix allow setting per model tpm rpm limits
test(test_router.py): add test to ensure retry-after matches received value
test(test_router.py): skip test - create separate pr to match retry after
fix(azure.py): add response header coverage for azure models
fix(main.py): cover openai /v1/completions endpoint
fix(openai.py): coverage for correctly re-raising exception headers on openai chat completion + embedding endpoints
ui allow setting tpm / rpm limits on ui
fix(utils.py): correctly re-raise the headers from an exception, if present
feat use identity_pool for vertex
test(test_router.py): add test to ensure error is correctly re-raised
fix(router.py): don't cooldown on apiconnectionerrors
fix pynacl error
fix set Caching Default Off
fix should_use_cache
feat(team_endpoints.py): expose 2 new fields - updated_users and updated_team_memberships, on `/team/member_add`
test caching default on /off
feat - allow setting cache mode
fix(huggingface_restapi.py): fix tests
docs(input.md): update docs on together ai response_format params support
fix(utils.py): support passing response_format for together ai calls
docs(aws_sagemaker.md): cleanup sagemaker messages api docs
fix show user_id on table
test(test_sagemaker.py): fix test
fix(proxy_server.py): fix post /v1/batches endpoint
build(deps): bump micromatch from 4.0.5 to 4.0.8 in /ui
bump: version 1.44.4 → 1.44.5
docs tts
add mock testing for vertex tts
docs on using vertex tts
test vertex ai text to speech
fix(proxy_server.py): support env vars for controlling global max parallel request retry/timeouts
fix linting
add example using tts on vertex ai
docs text to speech
fix linting errors
init comit for vtx text to speech
add test_audio_speech_litellm_vertex
fix(utils.py): only filter additional properties if gemini/vertex ai
feat add test for custom guardrails
fix custom guardrail test
init custom guardrail class
feat(sagemaker.py): add sagemaker messages api support
custom_callbacks
docs custom guardrails
doc custom guardrail
fix use guardrail for pre call hook
fix(utils.py): handle additionalProperties is False for vertex ai / gemini calls
add custom guardrail reference
add the ability to init a custom guardrail
fix(factory.py): support 'add_generation_prompt' field for hf chat templates
fix(litellm_pre_call_utils.py): don't override k-v pair sent in spend_logs_metadata by user
fix prom latency metrics
docs(caching.md): add redis cluster support to docs
docs(bedrock.md): add docs on alternating user/assistant messages
use with base64
allow load testing sagemaker url
bump: version 1.44.3 → 1.44.4
docs fix
docs moderation
docs(batches.md): add more examples to docs
docs move pass thru endpoints
ci/cd run again
bump: version 1.44.2 → 1.44.3
docs(sidebars.js): refactor docs
add test for test_azure_tenant_id_auth
test bedrock guardrails
docs(configs.md): add global_max_parallel_requests to docs
fix(proxy_server.py): expose flag to disable retries when max parallel request limit is hit
fix(files_endpoints.py): fix multiple args error
feat(auth_checks.py): allow team to call all models, when explicitly set via /*
add async_post_call_success_hook
doc bedrock guardrails
fix azure_ad_token_provider
feat(azure.py): support health checking azure deployments
add bedrock guardrails support
fix: fix linting errors
add types for BedrockMessage
feat(proxy_server.py): support azure batch api endpoints
docs(batches.md): add docs on calling azure batches api
feat(batches): add azure openai batches endpoint support
add dbally project
add prom docs for Request Latency Metrics
update promtheus metric names
track litellm_request_latency_metric
track api_call_start_time
fix init correct prometheus metrics
docs use entrata id with litellm proxy
add new litellm params for client_id, tenant_id etc
docs(azure_ai.md): add azure ai jamba instruct to docs
docs(utils.py): cleanup docstring
use azure_ad_token_provider to init clients
feat(factory.py): enable 'user_continue_message' for interweaving user/assistant messages when provider requires it
add azure_ad_token_provider as all litellm params
fix(cohere_chat.py): support passing 'extra_headers'
fix allow setting LiteLLM license as .env
fix(ollama_chat.py): fix passing assistant message with tool call param
fix test_vertexai_multimodal_embedding use magicMock requests
fix allow setting license in config.yaml
build(deps): bump hono from 4.2.7 to 4.5.8 in /litellm-js/spend-logs
add docstring for /embeddings and /completions
add doc string for /chat/completions swagger
test(test_custom_callback_input.py): skip flaky ci/cd test
fix /user/delete doc string
test(test_custom_callback_input.py): fix test
test: fix test
fix: rerun ci/cd
docs(enterprise.md): add key/team level spend tags to docs
test(test_function_calling.py): remove redundant gemini test (causing ratelimit errors)
test(test_image_generation.py): handle azure api error
test: test_function_calling.py
docs vertex
fix team_member_add
fix test_master_key_hashing
use litellm proxy with vertex ai sdk
docs(vertex.md): add vertex global safety settings to doc
feat(utils.py): support global vertex ai safety settings param
docs add example using litellm with vertex python sdk
fix pass through endpoints
test(test_amazing_vertex_completion.py): handle vertex api instability
refactor vertex endpoints to pass through all routes
add test vtx embedding
docs(logging.md): add standard logging payload to docs
proxy - print embedding request when recieved
add docs using litellm multi modal embeddings
feat(litellm_logging.py): add 'saved_cache_cost' to standard logging payload (s3)
bump: version 1.44.1 → 1.44.2
docs(users.md): add doc on setting max budget for internal users
docs(users.md): add doc on setting max budget for internal users
docs(custom_llm_server.md): add streaming example for custom llm call
fix(router.py): fix linting error
fix test test_vertexai_multimodal_embedding
fix(vertex_httpx.py): fix json schema call to pass in response_mime_type=="application/json"
feat add multimodal embeddings on vertex
add multi modal vtx embedding
test(test_caching.py): skip local test
feat(caching.py): redis cluster support
add initial support for multimodal_embedding vertex
add VertexMultimodalEmbeddingRequest type
fix(litellm_pre_call_utils.py): handle dynamic keys via api correctly
Support LangSmith parent_run_id, trace_id, session_id
docs semantic caching qdrant
fix qdrant litellm on proxy
build(config.yml): pin openai version
fix(internal_user_endpoints.py): pass in user api key dict value
fixes for using qdrant with litellm proxy
feat(proxy_server.py): support disabling storing master key hash in db, for spend tracking
fix tg ai -deprecated model
fix qdrant semantic caching test
fix drant url
fix(vertex_httpx.py): Fix tool calling with empty param list
fix(utils.py): support openrouter streaming
docs - use litellm on gcp cloud run
fix configmap name in print
test(test_proxy_server.py): fix test to specify user role
add checksum annotation
fix groq/3.1 reasoning model
feat(litellm_pre_call_utils.py): support passing tags/spend logs metadata from keys/team metadata to request
test(test_completion.py): fix gemini rate limit error
test(test_completion.py): fix test
openrouter/anthropic/claude-3.5-sonnet: supports_assistant_prefill:true
fix: was missing openrouter beta model from claude sonnet
docs(caching.md): add doc on enabling caching for just rate limiting features
fix(litellm_logging.py): add stricter check for special param being non none
fix(utils.py): ensure consistent cost calc b/w returned header and logged object
bump: version 1.44.0 → 1.44.1
enforece guardrails per API Key as enterprise
fix lakera ai tests
fix doc guardrails
fix doc lakera ai
support lakera ai category thresholds
refactor(team_endpoints.py): refactor auth checks for team member endpoints to ui team admin to manage it
docs move lakera to free
feat(_types.py): allow team admin to delete member from team
fix(litellm_pre_call_utils.py): only pass api_version if set
add docker image for non-root
fix dockerfile
working lakera ai during call hook
fix make lakera ai free guardrail
Fixed code snippet import typo in Structured Output docs
feat(user_api_key_auth.py): allow team admin to add new members to team
rename lakera ai
rename Aporia Guardrail
test test_using_default_working_fallback
fix router retries tests
test router fallbacks test
run test on sync function too
fix run sync fallbacks
fix fallbacks dont recurse on the same fallback
fix(azure.py): fix optional param elif statement
fix don't retry errors when no healthy deployments available
bring back model_prices_and_context_window update yml job
test + never retry on 404 errors
fetch-depth=0
bump: version 1.43.19 → 1.44.0
feat(ollama.py): support ollama `/api/embed` endpoint
Delete .github/workflows/auto_update_price_and_context_window.yml
fix(proxy_server.py): fix invalid login message to not show passed in pwd
fix(factory.py): fix merging consecutive tool blocks for bedrock converse
fix test codestral api
test guardrails with API Key
feat control guardrails per API Key
fix(main.py): response_format typing for acompletion
docs update Aporia Example
fix test_ollama_chat_function_calling
Update Huggingface provider to utilize the SSL verification through 'SSL_VERIFY' env var or 'litellm.ssl_verify'.
Update 'init_bedrock_client' to use 'litellm.ssl_verify' or 'SSL_VERIFY' environment variable.
Fix using sync 'litellm.client_session' for async calls in azure.py
Update handling of 'litellm.ssl_verify' in HTTP handlers to allow for custom, self-signed certificates.
fix(pass_through_endpoints.py): fix query param pass through
docs(langfuse.md): add doc on pass through
bump: version 1.43.18 → 1.43.19
docs fix aporia
fix(litellm_pre_call_utils.py): handle no query params in request
fix _get_request_ip_address
fix import error guardrails
fix _get_spend_report_for_time_range
fix importing _ENTERPRISE_Aporia
fix log to OTEL
fix test guardrails
add testing for guardrails
test_llm_guard_triggered
add testing for aporia guardrails
fix aporia typo
docs aporia
feat - guardrails v2
docs fix typo
doc update typo
docs updates based on feedback
doc aporia_w_litellm
run during_call_hook
feat - return applied guardrails in response headers
feat - allow accessing data post success call
feat run aporia as post call success hook
feat(langfuse_endpoints.py): support team based logging for langfuse pass-through endpoints
feat(langfuse_endpoints.py): support langfuse pass through endpoints by default
test(test_custom_callback.py): add test for message redaction to standard logging object
fix(proxy_cli.py): support database_host, database_username, database_password, database_name
docs: add docs on bedrock + cohere pass through endpoints
fix(user_api_key_auth.py): fix client_ip
ci/cd run again
fix import error guardrails
fix _get_spend_report_for_time_range
fix importing _ENTERPRISE_Aporia
fix log to OTEL
fix test guardrails
add testing for guardrails
test_llm_guard_triggered
add testing for aporia guardrails
fix aporia typo
docs aporia
feat - guardrails v2
feat(langfuse_endpoints.py): support team based logging for langfuse pass-through endpoints
feat(langfuse_endpoints.py): support langfuse pass through endpoints by default
test(test_custom_callback.py): add test for message redaction to standard logging object
fix(proxy_cli.py): support database_host, database_username, database_password, database_name
feat(cost_calculator.py): only override base model if custom pricing is set
docs fix typo
doc update typo
docs updates based on feedback
doc aporia_w_litellm
docs: add docs on bedrock + cohere pass through endpoints
feat(azure.py): support dynamic api versions
run during_call_hook
added testing for qdrant semantic caching
feat - return applied guardrails in response headers
feat - allow accessing data post success call
feat run aporia as post call success hook
fix(user_api_key_auth.py): log requester ip address to logs on request rejection
test(test_caching.py): re-introduce testing for s3 cache w/ streaming
add 0.2.3 helm
fix(ollama.py): fix ollama embeddings - pass optional params
fix(ollama_chat.py): fix sync tool calling
implemented RestAPI and added support for cloud and local Qdrant clusters
docs(json_mode.md): add azure openai models to doc
inly write model tpm/rpm tracking when user set it
sleep before checi g
feat(Support-pass-through-for-bedrock-endpoints): Allows pass-through support for bedrock endpoints
fix test pass through
docs access groups
docs virtual key access groups
feat add model access groups for teams
feat(pass_through_endpoints.py): add pass-through support for all cohere endpoints
fix test update tpm / rpm limits for a key
use model access groups for teams
feat(azure.py): support 'json_schema' for older models
docs cleanup
fix proxy all models test
docs clean up virtual key access
bump: version 1.43.17 → 1.43.18
update tpm / rpm limit per model
style(vertex_httpx.py): make vertex error string more helpful
docs rate limits per model per api key
fix parallel request limiter tests
fix parallel request limiter
docs(google_ai_studio.md): add docs on google ai studio pass through endpoints
docs clean up emojis
docs cleanup - reduce emojis
add tpm limits per api key per model
feat(pass_through_endpoints.py): support streaming requests
fix async_pre_call_hook in parallel request limiter
feat return rmng tokens for model for api key
feat(google_ai_studio_endpoints.py): support pass-through endpoint for all google ai studio requests
feat - use commong helper for getting model group
show correct metric
add litellm-key-remaining-tokens on prometheus
feat add settings for rpm/tpm limits for a model
fix(pass_through_endpoints.py): fix returned response headers for pass-through endpoitns
docs(vertex_ai.md): cleanup docs
bump: version 1.43.16 → 1.43.17
track rpm/tpm usage per key+model
user api key auth rpm_limit_per_model
docs(langfuse_integration.md): add disable logging for specific calls to docs
fix(health_check.py): return 'missing mode' error message, if error with health check, and mode is missing
fix databricks streaming test
fix(litellm_logging.py): fix price information logging to s3
Add the "stop" parameter to the mistral API interface, it is now supported
feat(litellm_logging.py): support logging model price information to s3 logs
fix predictions image generation response
skip InternalServerError on vertex test
docs oauth2
docs correct link to oauth 2.0
docs oauh 2.0 enterprise feature
docs on oauth 2.0
add debugging for oauth2.0
allow using oauth2 checks for logging into proxy
(oidc): Improve docs for unofficial provider.
add init commit for oauth 2 checks
(oidc): Add support for loading tokens via a file, environment variable, and from a file path set in an env var.
ui new build
fix endpoint name on router
docs add example on setting temp=0 for sagemaker
fix(utils.py): fix get_image_dimensions to handle more image types
docs sagemaker - add example using with proxy
docs cleanup
add provider_specific_fields to GenericStreamingChunk
return traces in bedrock guardrails when enabled
fix(__init__.py): fix models_by_provider to include cohere_chat models
refactor: replace .error() with .exception() logging for better debugging on sentry
pass trace through for bedrock guardrails
add provider_specific_fields to GenericStreamingChunk
bump: version 1.43.15 → 1.43.16
feat(ui): for adding pass-through endpoints
fix sagemaker old used test
feat(pass_through_endpoints.py): initial working CRUD endpoints for /pass_through_endoints
fix test sagemaker config test
fix using prompt caching on proxy
bump: version 1.43.14 → 1.43.15
bump: version 1.43.13 → 1.43.14
allow index to not exist in sagemaker chunks
assume index is not always in stream chunk
feat add support for aws_region_name
show bedrock, sagemaker creds in verbose mode
add verbose logging on test
fix sagemaker tests
fix sagemaker test
refactor sagemaker to be async
fix(litellm_logging.py): wrap function to safely fail
feat(litellm_logging.py): cleanup payload + add response cost to logged payload
fix(litellm_logging.py): fix standard payload
fix(s3.py): fix s3 logging payload to have valid json values
test sync sagemaker calls
run mock tests for test_completion_sagemaker
add non-stream mock tests for sagemaker
use BaseAWSLLM for bedrock getcredentials
fix ImportError
Fixes the `tool_use` chunk mapping
fix - don't require boto3 on the cli
litellm always log cache_key on hits/misses
fix /moderations endpoint
fix test proxy exception mapping
refactor use 1 util for llm routing
simplify logic for routing llm request
use route_request for making llm call
Make helm chart listen on IPv6 (and IPv4).
Use sepecific llama2 and llama3 model names in Ollama
Fix incorrect message length check in cost calculator
docs(bedrock.md): add guardrails on config.yaml to docs
docs(team_logging.md): add key-based logging to docs
docs(pass_through.md): add doc on using langfuse client sdk w/ litellm proxy
feat(pass_through_endpoints.py): initial commit of crud endpoints for pass through endpoints
feat(proxy_server.py): support returning available fields for pass_through_endpoints via `/config/field/list
fix langfuse log_provider_specific_information_as_span
bump: version 1.43.12 → 1.43.13
docs using proxy with context caaching anthropic
docs add examples with litellm proxy
docs add examples doing context caching anthropic sdk
add test for large context in system message for anthropic
move claude prompt caching to diff file
add test for caching tool calls
Use AZURE_API_VERSION as default azure openai version
fix(factory.py): handle assistant null content
fix bedrock test
docs Caching - Continuing Multi-Turn Convo
pass cache_control in tool call
test test_anthropic_api_prompt_caching_basic
test amnthropic prompt caching
add anthropic cache controls
docs(model_management.md): add section on adding additional model information to proxy config
fix(factory.py): support assistant messages as a list of dictionaries - cohere messages api
test(test_pass_through_endpoints.py): fix langfuse base
build(model_prices_and_context_window.json): add 'supports_assistant_prefill' to all vertex ai anthropic models
add testing for test_anthropic_cache_controls_pt
fix(utils.py): fix is_azure_openai_model helper function
test passing cache controls through anthropic msg
fix(utils.py): support calling openai models via `azure_ai/`
feat - anthropic api context caching v0
Update prices/context windows for Perplexity Llama 3.1 models
test(test_pass_through_endpoints.py): correctly reset test
bump: version 1.43.11 → 1.43.12
bump: version 1.43.10 → 1.43.11
vertex_ai/claude-3-5-sonnet@20240620 support prefill
test(test_function_call_parsing.py): fix test
test(test_pass_through_endpoints.py): fix test
allow running as non-root user
fix use normal prisma
use litellm_ prefix for new deployment metrics
docs(sidebar.js): cleanup docs
log failure calls on gcs + testing
fix test for gcs bucket
feat log fail events on gcs
Mismatch in example fixed
fix(user_api_key_auth.py): more precisely expand scope to handle 'basic' tokens
test(test_proxy_server.py): skip local test
return detailed error message on check_valid_ip
test(test_proxy_server.py): refactor test to work on ci/cd
bump: version 1.43.9 → 1.43.10
fix use s3 get_credentials to get boto3 creds
fix(bedrock_httpx.py): fix error code for not found provider/model combo to be 404
fix ci/cd pipeline
comment on using boto3
docs - set litellm config as s3 object
feat(user_api_key_auth.py): support calling langfuse with litellm user_api_key_auth
feat read config from s3
add helper to load config from s3
(models): Add chatgpt-4o-latest.
Fix not sended json_data_for_triton
fic docker file to run in non root model
fix(bedrock_httpx.py): raise bad request error if invalid bedrock model given
fix prisma issues
ui new build
temp set prisma pems
skip prisma gen step
ui - handle session expired on ui
fix make prisma readable
fix(teams.tsx): add team_id to team table on ui
Improving the proxy docs for configuring with vllm
add test for test_check_valid_ip_sent_with_x_forwarded_for
use _check_valid_ip
feat log use_x_forwarded_for
fix(langsmith.py): support langsmith 'extra' field object
check use_x_forwarded_for
fix(utils.py): ignore none chunk in stream infinite loop check
docs(user_keys.md): cleanup instructor docs
docs(user_keys.md): cleanup docs
docs control langfuse specific tags
add langfuse_default_tags
allow using langfuse_default_tags
feat allow controlling logged tags on langfuse
fix(bedrock_httpx.py): handle empty stop string
fix(bedrock_httpx.py): handle bedrock empty system message
fix(langfuse.py'): cleanup
fix(litellm_pre_call_utils.py): support routing to logging project by api key
docs(team_logging.md): cleanup docs
refactor(test_users.py): refactor test for user info to use mock endpoints
fix gcs logging test
fix gcs test
tes logging to gcs buckets
feat log responses in folders
test gcs logging payload
feat gcs log user api key metadata
refactor(test_users.py): refactor test for user info to use mock endpoints
remove aws_sagemaker_allow_zero_temp from the parameters passed to inference
fix gcs logging test
fix(cost_calculator.py): fix cost calc
fix gcs test
tes logging to gcs buckets
feat log responses in folders
test gcs logging payload
feat gcs log user api key metadata
fix(cost_calculator.py): fix cost calc
fix gcs test
tes logging to gcs buckets
fix(utils.py): if openai model, don't check hf tokenizers
feat log responses in folders
test gcs logging payload
feat gcs log user api key metadata
bump: version 1.43.8 → 1.43.9
fix(cost_calculator.py): handle openai usage pydantic object
fix(utils.py): Break out of infinite streaming loop
bump: version 1.43.7 → 1.43.8
fix internal user tests to pass
log new_user, delete user
send slack alert on team events
fix doc string for team endpoints
send alert on all key events
fix(azure.py): return response headers acompletion + acompletion w/ streaming
fix(proxy_server.py): add info log when spend logs is skipped because `disable_spend_logs=True`.
make send_management_endpoint_alert a premium feature
send management endpoint alert
use management_endpoint_wrapper for key endpoints
v0 log KeyCreatedEvent
fix(user_api_key_auth.py): move warning to debug log
docs mark oidc as beta
docs(perplexity.md): show how to get 'return_citations'
allow setting PROMETHEUS_SELECTED_INSTANCE
doc new prometheus metrics
fix(management/utils.py): fix add_member to team when adding user_email
feat add cron job for sending stats from prometheus
add slack alerting on proxy_config.yaml
fix(internal_user_endpoints.py): return all teams if user is admin
feat - use api to get prometheus api metrics
add fallback_reports in slack alert types
feat add prometheus api to get data from endpoint
add fallback_reports as slack alert
fix(teams.tsx): reduce network calls to /team/info
build(model_prices_and_context_window.json): add 'supports_assistant_prefill' to model info map
bump: version 1.43.6 → 1.43.7
prometheus log_success_fallback_event
feat - log fallbacks events on prometheus
docs(prefix.md): add prefix support to docs
v0 track fallback events
v0 add event handlers for logging fallback events
v0 add helper for loging success/fail fallback events
docs(custom_llm_server.md): clarify what to use for modifying incoming/outgoing calls
test for prom metrics
feat - track latency per llm deployment
fix(main.py): safely fail stream_chunk_builder calls
track llm_deployment_success_responses
fix(openai.py): fix position of invalid_params param
prometheus add basic testing for success
refactor prometheus to be a customLogger class
fix(types/utils.py): handle null completion tokens
use customLogger for prometheus logger
feat - refactor prometheus metrics
Follow redirects
bump: version 1.43.5 → 1.43.6
fix _hidden_params is None case
ci/cd run again
fix linting error
fix(litellm_logging.py): fix calling success callback w/ stream_options true
docs clean sidebar
docs migration policy
docs add migration policy
correctly add modified tool names to cache
test bedrock tool call names
init bedrock_tool_name_mappings
test(test_optional_params.py): use num_retries instead of 'max_retries' if given
docs(main.py): clarify 'num_retries' usage
fix(utils.py): set max_retries = num_retries, if given
fix(utils.py): only return non-null default values
log provider specific metadata as a span
test invalid tool namehandling
bedrock make_valid_bedrock_tool_name
build(test_completion.py): ci/cd run again
ui new build
fix(router.py): fix types
fix(router.py): fallback on 400-status code requests
ui add cohere embedding models
fix cohere / cohere_chat when timeout is None
add testing for cohere embeddings
ui allow adding cohere models
add cohere embed-multilingual-v2.0
build(deps): bump aiohttp from 3.9.4 to 3.10.2
fix(huggingface_restapi.py): support passing 'wait_for_model' param on completion calls
fix(huggingface_restapi.py): fix hf embeddings optional param processing
docs prometheus metrics
refactor prom metrics
feat(vertex_httpx.py): return vertex grounding, citation, and safety results
fix(huggingface_restapi.py): fixes issue where 'wait_for_model' was not being passed as expected
doc Grounding vertex ai
docs fix typo
docs(self_serve.md): add internal_user_budget_duration to docs
bump: version 1.43.4 → 1.43.5
ui new build
sort providers alphabetical order
ui add mistral ai
ui new build
add groq through admin ui
ui show litellm model name
docs vertex context caching
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
fix(anthropic.py): fix translation from /v1/messages format to openai format
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
feat: hash prompt when caching
handle anthropic internal server errors
testing skip internal server errors
ci/cd skip ServiceUnavailableError
ui new build
sort providers alphabetical order
ui don't require azure api version
ui add mistral ai
ui new build
add groq through admin ui
ui show litellm model name
bump: version 1.43.3 → 1.43.4
docs vertex context caching
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
fix(anthropic.py): fix translation
fix log to otel test
docs vertex context caching
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
fix(anthropic.py): fix translation from /v1/messages format to openai format
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
feat: hash prompt when caching
handle anthropic internal server errors
testing skip internal server errors
ci/cd skip ServiceUnavailableError
ui new build
sort providers alphabetical order
ui don't require azure api version
ui add mistral ai
ui new build
add groq through admin ui
ui show litellm model name
bump: version 1.43.3 → 1.43.4
docs vertex context caching
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
fix(anthropic.py): fix translation
fix log to otel test
docs vertex context caching
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
fix(anthropic.py): fix translation from /v1/messages format to openai format
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
fix(management_helpers/utils.py): use user_default max_budget, budget duration on new user upsert during team member add
handle anthropic internal server errors
testing skip internal server errors
ci/cd skip ServiceUnavailableError
ui new build
sort providers alphabetical order
ui don't require azure api version
ui add mistral ai
ui new build
test(test_proxy_server.py): unit testing to make sure internal user params don't impact admin
add groq through admin ui
fix(proxy_server.py): ensure internal_user params only apply to internal_user role
ui show litellm model name
build(requirements.txt): pin orjson dep
bump: version 1.43.3 → 1.43.4
pin orjson dep on req.txt
fix(anthropic.py): fix translation
fix log to otel test
fix(proxy_server.py): respect internal_user_budget_duration for sso user
fix(utils.py): handle anthropic overloaded error
docs vertex context caching
fix use get_file_check_sum
fix(factory.py): handle openai function message having tool call id
docs readme
docs use (LLM Gateway)  in some places
docs vertex ai
add default_vertex_config
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
add native cachedContents endpoint
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
add tests to make sure correct vertex ai route is used
use v1beta1 when using cached_content
fix handle case when service logger has no attribute prometheusServicesLogger
fix(anthropic.py): fix translation from /v1/messages format to openai format
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test: skip flaky langsmith tests
test(test_langsmith.py): fix test
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
fix(utils.py): handle anthropic overloaded error
docs vertex context caching
fix(factory.py): handle openai function message having tool call id
docs readme
docs use (LLM Gateway)  in some places
docs vertex ai
add default_vertex_config
docs vertex
docs cachedContent endpoint
auto update the model in cachedContents request
fix(anthropic.py): fix translation from /v1/messages format to openai format
add native cachedContents endpoint
test: skip flaky langsmith tests
test: skip flaky langsmith tests
test(test_langsmith.py): skip flaky test
fix(internal_user_endpoints.py): expose new 'internal_user_budget_duration' flag
test(test_langsmith.py): fix test
feat: set max_internal_budget for user w/ sso
doc on using litellm proxy with vertex ai content caching
add example using litellm proxy with gemini context caching
add tests to make sure correct vertex ai route is used
use v1beta1 when using cached_content
fix(user_api_key_auth.py): Fixes https://github.com/BerriAI/litellm/issues/5111
build(model_prices_and_context_window.json): Fixes https://github.com/BerriAI/litellm/issues/5113
build: ui - update to include max budget per team
fix handle case when service logger has no attribute prometheusServicesLogger
fix: wrong order of arguments for ollama
fix use get_file_check_sum
feat: hash prompt when caching
docs(scheduler.md): cleanup docs to use /chat/completion endpoint
bump: version 1.43.2 → 1.43.3
test(test_completion.py): fix merge error
docs(self_serve.md): cleanup docs on how to onboard new users + teams
fix: use more descriptive flag
fix(internal_user_endpoints.py): respect 'max_user_budget' for new internal user's
image gen catch when predictions not in json response
run that ci cd again
test(test_amazing_vertex_completion.py): fix test for json schema validation in openai schema
run that ci/cd again
fix(vertex_ai_partner.py): pass model for llama3 param mapping
fix all optional param tests
fix test for wildcard routing
fix test_drop_params_parallel_tool_calls
fix(vertex_ai_partner.py): pass model for llama3 param mapping
fix all optional param tests
feat(router.py): allow using .acompletion() for request prioritization
fix test for wildcard routing
fix test_drop_params_parallel_tool_calls
docs prom
test(test_completion.py): handle gemini instability
Add deepseek-coder-v2(-lite), mistral-large, codegeex4 to ollama
ci/cd run again
fixinstalling openai on ci/cd
fix test_team_update_redis
refactor(user_api_key_auth.py): refactor to replace user_id_information list with pydantic user_obj
bump: version 1.43.1 → 1.43.2
fix - someone resolved a merge conflict badly
fix getting provider_specific_deployment
fix: fix tests
docs provider specific wildcard routing
fix use provider specific routing
fix(user_api_key_auth.py): respect team budgets over user budget, if key belongs to team
support provider wildcard routing
test_router_provider_wildcard_routing
router use provider specific wildcard routing
test provider wildcard routing
add + test provider specific routing
fix(config.yml): fix build and test
fix(main.py): fix linting error for python3.8
fix(utils.py): fix linting error for python3.8
gemini test skip internal server error
test: update build requirements
fix(router.py): add reason for fallback failure to client-side exception string
docs prom metrics
docs prometheus
show warning about prometheus moving to enterprise
docs link to enteprise pricing
docs prometheus
fix logging cool down deployment
fix(utils.py): support deepseek tool calling
fix(vertex_ai_partner.py): default vertex ai llama3.1 api to use all openai params
use router_cooldown_handler
allow setting outage metrics
test(test_completion.py): handle internal server error in test
test: add vertex claude to streaming valid json str test
Clarifai : Fixed model name
emit deployment_partial_outage on prometheus
fix(bedrock_httpx.py): handle empty arguments returned during tool calling streaming
rename to set_llm_deployment_success_metrics
add set_remaining_tokens_requests_metric
fix(anthropic.py): handle scenario where anthropic returns invalid json string for tool call while streaming
prom svc logger init if it's None
fix use extra headers for open router
build(requirements.txt): bump openai version
Revert "Fix: Add prisma binary_cache_dir specification to pyproject.toml"
feat add ft:gpt-4o-mini-2024-07-18
docs(ui.md): add restrict email subdomains w/ sso
build(model_prices_and_context_window.json): remove duplicate entries
run ci / cd again
ci/cd run again
docs run ui on custom server root path
test test_basic_passthrough
fix pass through endpoint tests
clean up unused func
feat(utils.py): support validating json schema client-side if user opts in
feat: Translate openai 'response_format' json_schema to 'response_schema' for vertex ai + google ai studio
explain ui base path
docs(json_mode.md): add example of calling openai with pydantic model via litellm
docs(sidebars.js): cleanup sidebar title
feat(utils.py): support passing response_format as pydantic model
fix forward ui requests when base url set
add redirect_ui_middleware
build custom ui path docker
docs(json_mode.md): update json mode docs to show structured output responses
fix(lakera_ai.py): fix hardcoded prompt_injection string in lakera integration
update alerting settings on ui
add error when trying to send emails
fix email health checks
add debug statements on docker
feat(lakera_ai.py): support lakera custom thresholds + custom api base
use file size _ name to get file check sum
docs(deploy.md): add iam-based auth to rds
bump: version 1.43.0 → 1.43.1
use file_checksum
caching use file_checksum
feat(proxy_server.py): allow restricting allowed email domains for the UI
readme - clarify helm is community maintained
fix(utils.py): fix types
test pass through endpoint
doc forward_headers
init pass through endpoints
docs on gpt-4o-2024-08-06
fix pricing
add gpt-4o-2024-08-06
fix(encrypt_decrypt_utils.py): add helper line, explaining why there might be a key decryption error
use helper to forward headers from request
docs(prod.md): add litellm salt key to prod docs
fix(utils.py): fix dynamic api base
use prints
feat(utils.py): check env var for api base for openai-compatible endpoints
ci/cd run again
fix: fix test
test(test_router_init.py): cleanup tests
forward headers from request
fix: fix test to specify allowed_fails
fix(main.py): log hidden params for text completion calls
ci/cd run again
fix test cache_key
fix cache_key check
fix(rds_iam_token.py): support sts based auth
use safe init_verbose_loggers
fix(user_api_key_auth.py): fix _get_user_role
test(test_key_generate_prisma.py): cleanup test
otel log failures
otel log service errors
otel fix async_service_failure_hook
run ci/cd again
log event_metadata on otel
otel log event_metadata
build(config.yml): pin prisma version
use otel callbacks
build(config.yml): pin prisma version
build(config.yml): pin prisma version
log event_metadata on otel service loggers
test: improve debugging for test
add debugging utils to print when connecting to prisma
add debug statements when connected to prisma db
feat(proxy_cli.py): support iam-based auth to rds
run ci/cd again
fix(__init__.py): bump default allowed fails
build ui on custom path
use correct build paths
set PROXY_BASE_URL when server root path set
working sh script
Revert "[FIX] allow setting UI BASE path"
fix get_request_route
add get_request_route
build ui on custom path
use correct build paths
set PROXY_BASE_URL when server root path set
working sh script
Revert "[FIX] allow setting UI BASE path"
fix edit docker file ui base path
fix allow setting UI _BASE path
fix test fine tuning api azure
fix get_request_route
test proxy server routes
use get_request_route
add get_request_route
fix: bump default allowed_fails + reduce default db pool limit
fix: cleanup test
fix(init.py): rename feature_flag
feat(caching.py): enable caching on provider-specific optional params
fix test fine tuning api azure
fix get_request_route
test proxy server routes
use get_request_route
add get_request_route
fix(types/router.py): remove model_info pydantic field
docs(azure.md): cleanup docs
docs: cleanup docs
fix(ollama_chat.py): fix passing auth headers to ollama
build(model_prices_and_context_window.json): update gpt-4o-mini max_output_tokens
add sample spec to model cost map
docs show fields logged on gcs bucket
fix(user_api_key_auth.py): handle older user_role's
test gcs bucket
fix linting errors
simplify GCS payload
add ALL_LITELLM_RESPONSE_TYPES
use util convert_litellm_response_object_to_dict
docs(gemini.md): add json mode, response schema, supported openai params to gemini docs
feat(vertex_httpx.py): Support gemini 'response_schema' param
bump: version 1.42.12 → 1.43.0
docs(sidebar.js): cleanup
fix(anthropic_adapter.py): fix sync streaming
test: fix test
feat(anthropic_adapter.py): support streaming requests for `/v1/messages` endpoint
docs add when to use litellm
fix fine tune api tests
ci/cd run again
fix fine tuning tests
fix test test_aimage_generation_vertex_ai
bump: version 1.42.11 → 1.42.12
docs default vertex
docs - fix merge conflicts
docs tuning api
docs add example curl command
docs link to vertex ai endpoints
docs add vertex ai endpoints
add vertex ai countTokens endpoint
add vertex embeddings endpoints
add vertex generateContent
working code for vertex ai routes
use native endpoints
set native vertex endpoints
docs secret manager
fix(vertex_httpx.py): fix linting error
docs - use consistent name for LiteLLM proxy server
organize docs
docs clean up organization
docs supported models / providers
docs secret manager
fix(vertex_httpx.py): fix linting error
fix(router.py): move deployment cooldown list message to error log, not client-side
docs - use consistent name for LiteLLM proxy server
organize docs
docs(proxy/reliability.md): add docs on testing if loadbalancing is working as expected
docs clean up organization
feat(router.py): add flag for mock testing loadbalancing for rate limit errors
docs supported models / providers
fix(utils.py): parse out aws specific params from openai call
Use correct key name
docs native vertex ft endpoint
fix(types/utils.py): fix linting errors
add support for pass through vertex ai ft jobs
support v1/projects/tuningJobs
fix(bedrock.py): fix response format for bedrock image generation response
fix ft api test
docs create fine tuning jobs
docs(github.md): cleanup docs
docs: add github provider to docs
feat(utils.py): Add github as a provider
docs - vtx ft api
fix vertex ai
build(model_prices_and_context_window.json): add deepseek cache hit pricing
fix linting errors
test translating to vertex ai params
fix vertex credentials
add vertex example on config.yaml
test vertex ft jobs
Add unit test
fix typing
fix typing
add vertex ai ft on proxy
add support for sync vertex ft
fix translating response
convert response obj from vertex ai
test ft response vertex ai
translate response from vertex to openai
fix(utils.py): handle scenario where model="azure/*" and custom_llm_provider="azure"
example vertex ai.jsonl
test vertex ft
add vertex ft support
add vertex FT spec
add fine tuning for vertex
add vertex_credentials in router param
fix(user_api_key_auth.py): fix linting errors
build(ui): allow admin_viewer to view teams tab
docs caching
docs call types
docs supported call types
fix test
test whisper
return cache hit True on cache hits
use file name when getting cache key
log correct file name on langfuse
Fix tool call coalescing
fix(main.py): Handle bedrock tool calling in stream_chunk_builder
use regular ci/cd pipeline
ci/cd run again
fix config.yaml
fix config
bump: version 1.42.10 → 1.42.11
temp testing ci/cd
queue stable release testing after new GH release
fix(anthropic.py): fix linting error
refactor(openai/azure.py): move to returning openai/azure response headers by default
fix(types/utils.py): support passing prompt cache usage stats in usage object
test: handle anthropic rate limit error
qdrant semantic caching added
add step to ghcr deploy
fix(utils.py): fix codestral streaming
fix langfuse hardcoded public key
fix(bedrock_httpx.py): fix ai21 streaming
bump: version 1.42.9 → 1.42.10
fix(langfuse.py): cleanup
docs spend tracking enteprrise
fix(langfuse.py): cleanup
ci/cd run again
fix test traceloop.py
feat(litellm_logging.py): log exception response headers to langfuse
Update prompt_injection.md
ci/cd - anyscale discontinued their API endoints - skip test
fix model prices formatting
Add new model for gemini-1.5-pro-exp-0801.
bump: version 1.42.8 → 1.42.9
fix test_traceparent_not_added_by_default
docs dbrx
docs add new dbrx models
add correct context window
add new dbrx models
docs gcs buckets
update helm chart
enforce premium user cheks on gcs bucket
docs logging to GCS
docs setting service accounts
init gcs using gcs_bucket
docs using gcs
fix type errors
delete object from gcs
test writing logs to GCS bucket
feat gcs bucket log payload
add better debugging statements for vertex logging
basic gcs logging test
fix(vertex_ai_partner.py): add /chat/completion codestral support
fix(google.py): fix cost tracking for vertex ai mistral models
build(model_prices_and_context_window.json): add mistral nemo latest to model cost map
fix(databricks.py): fix error handling
fix: add type hints for APIError and AnthropicError status codes
fix(cost_calculator.py): respect litellm.suppress_debug_info for cost calc
testing fix - skip rate limit errors from anthropic api
docs enterprise
docs enterprise feature
docs enterprise feature
docs - team based logging
fix _handle_failure for otel
forward_traceparent_to_llm_provider
test_traceparent_not_added_by_default
docs add info on forward_traceparent_to_llm_provider
use itellm.forward_traceparent_to_llm_provider
ui new build
ui fix entering custom model names for azure
feat(litellm_logging.py): log exception response headers to langfuse
Update prompt_injection.md
ci/cd - anyscale discontinued their API endoints - skip test
fix model prices formatting
Add new model for gemini-1.5-pro-exp-0801.
bump: version 1.42.8 → 1.42.9
fix test_traceparent_not_added_by_default
docs dbrx
docs add new dbrx models
add correct context window
add new dbrx models
docs gcs buckets
update helm chart
enforce premium user cheks on gcs bucket
docs logging to GCS
docs setting service accounts
init gcs using gcs_bucket
docs using gcs
fix type errors
delete object from gcs
test writing logs to GCS bucket
feat gcs bucket log payload
add better debugging statements for vertex logging
basic gcs logging test
feat(litellm_logging.py): log exception response headers to langfuse
Update prompt_injection.md
ci/cd - anyscale discontinued their API endoints - skip test
fix model prices formatting
Add new model for gemini-1.5-pro-exp-0801.
bump: version 1.42.8 → 1.42.9
fix test_traceparent_not_added_by_default
docs dbrx
docs add new dbrx models
add correct context window
add new dbrx models
docs gcs buckets
update helm chart
enforce premium user cheks on gcs bucket
docs logging to GCS
docs setting service accounts
init gcs using gcs_bucket
docs using gcs
fix type errors
delete object from gcs
test writing logs to GCS bucket
feat gcs bucket log payload
add better debugging statements for vertex logging
basic gcs logging test
fix(vertex_ai_partner.py): add /chat/completion codestral support
fix(google.py): fix cost tracking for vertex ai mistral models
build(model_prices_and_context_window.json): add mistral nemo latest to model cost map
fix(databricks.py): fix error handling
fix: add type hints for APIError and AnthropicError status codes
feat(litellm_logging.py): log exception response headers to langfuse
Update prompt_injection.md
ci/cd - anyscale discontinued their API endoints - skip test
fix: fix linting errors
fix(main.py): fix linting error
fix(litellm_logging.py): fix linting erros
fix model prices formatting
docs(vertex.md): add docs on calling codestral via vertex for FIM tasks
bump: version 1.42.8 → 1.42.9
fix test_traceparent_not_added_by_default
feat(vertex_ai_partner.py): add vertex ai codestral FIM support
docs dbrx
docs add new dbrx models
add correct context window
add new dbrx models
docs gcs buckets
enforce premium user cheks on gcs bucket
docs logging to GCS
fix(vertex_ai_partner.py): add /chat/completion codestral support
docs setting service accounts
init gcs using gcs_bucket
docs using gcs
fix(google.py): fix cost tracking for vertex ai mistral models
Add new model for gemini-1.5-pro-exp-0801.
build(model_prices_and_context_window.json): add mistral nemo latest to model cost map
fix(databricks.py): fix error handling
fix: add type hints for APIError and AnthropicError status codes
fix(utils.py): fix togetherai streaming cost calculation
fix type errors
fix(utils.py): fix anthropic streaming usage calculation
delete object from gcs
test writing logs to GCS bucket
feat gcs bucket log payload
add better debugging statements for vertex logging
basic gcs logging test
fix(cost_calculator.py): respect litellm.suppress_debug_info for cost calc
testing fix - skip rate limit errors from anthropic api
docs enterprise
docs enterprise feature
docs enterprise feature
docs - team based logging
fix _handle_failure for otel
fix(litellm_logging.py): use 1 cost calc function across response headers + logging integrations
ui new build
ui fix entering custom model names for azure
build(model_prices_and_context_window.json): add azure gpt-4o-mini regional + global standard pricing to model cost map
forward_traceparent_to_llm_provider
test_traceparent_not_added_by_default
fix(anthropic.py): respect timeouts
docs add info on forward_traceparent_to_llm_provider
use itellm.forward_traceparent_to_llm_provider
bump: version 1.42.7 → 1.42.8
feat(ui): add ability to enable traceloop + langsmith via ui
update helm chart
feat(ui): add braintrust logging to ui
test: handle predibase api failures
fix(_types.py): fix finetuning endpoints
update
fix(utils.py): fix special keys list for provider-specific items in response object
try / except rate limit errors
fix: support vertex path
docs ft api
docs ft api
ft api docs
link to ft api
docs ft api
docs ft api
docs ft api
docs add example doing ft
add link to fine tuning api
docs fine tuning
fix setup for endpoints
bump: version 1.42.6 → 1.42.7
fix fine tuning endpoint postion on swagger
enforce ft endpoints as premium feature
fix mark fine tuning endpoints as enteprrise
fix routes order
fix linting errors
fix POST files
fix reading files/ft config
fix cancel ft job route
add ft jobs in list of allowed routes
test cancel ft jobs
add cancel endpoint
add GET fine_tuning/jobs
fix(utils.py): return additional kwargs from openai-like response body
add test for ft endpoints on azure
add examples on config
allow setting files config
fix(utils.py): map cohere timeout error
fix(http_handler.py): correctly re-raise timeout exception
feat support azure ft create endpoint
fix test_completion_function_plus_pdf
add /fine_tuning/jobs routes
validation for passing config file
read ft config
fix endpoint to create fine tuning jobs
fix pydantic obj for FT endpoints
feat add POST /v1/fine_tuning/jobs
test: fix testing
fix test_team_disable_guardrails
fix test_team_disable_guardrails
(test_bedrock_completion.py) - Use FIPS endpoints for testing.
(bedrock_httpx.py) - Add support for custom STS endpoints, e.g. for FIPS.
fix predibase mock test
fix: fix linting errors
ci/cd run again
use timeouts for predibase - never use them in prod !
fix predibase timeout exceptions
fix predibase tests
support timeouts on http handler
fix timeouts for predibase - they are unstable af
test: cleanup duplicate tests + add error handling for backend api errors
test(test_completion.py): handle gemini internal server error
test(test_streaming.py): fix streaming test
ci cd run again
fix test proxy routes check
fix test proxy routes
fix(utils.py): fix linting errors
fix(user_api_key_cache): fix check to not raise error if team object is missing
fix(utils.py): fix model registeration to model cost map
handle predibase failing streaming tests
fix(auth_checks.py): fix redis usage for team cached objects
ci/cd run again
fix create ft jobs api test
fix linting checks
fix custom auth test
test azure fine tune job create
feat FT cancel and LIST endpoints for Azure
test - fine tuning apis
add azure files api
add azure fine tuning apis
add support for fine tuning azure
add azure ft test file
test(test_streaming.py): move to mock implementation for sagemaker streaming tests
test(test_streaming.py): handle predibase instability
fix(router.py): gracefully handle scenario where completion response doesn't have total tokens
fix(ollama.py): correctly raise ollama streaming error
fix(cohere.py): support async cohere embedding calls
fix(huggingface_restapi.py): fix linting errors
fix(utils.py): fix cost tracking for vertex ai partner models
fix(main.py): fix linting error
style(cohere.py): point to cohere_chat docs for updated doc info
docs(supported_embedding.md): add specifying input_type for huggingface embedding calls
rename fine tuning apis
ui new build
feat(huggingface_restapi.py): Support multiple hf embedding types + async hf embeddings
switch off prod logs on ui
swithc off console log in prod
fix linting errors
add docs on status code from exceptions
return ProxyException code as str
docs(input.md): update docs to show ollama tool calling
docs LIST batches
test batches endpoint on proxy
test - list batches
feat add support for alist_batches
bump: version 1.42.5 → 1.42.6
fix(cohere_chat.py): handle tool_result + user message being passed in
ui new build
docs(scheduler.md): update docs with request timeout
fix inc langfuse flish time
fix type errors
fix type errors
fix linting
test - async ft jobs
test - list_fine_tuning_jobs
add list fine tune endpoints
docs(deploy.md): support running litellm docker container without internet connection
async cancel ft job
fix doc string
test cancel cancel_fine_tuning_job
feat - add cancel_fine_tuning_job
add test_create_fine_tune_job
add types for FineTuningJobCreate OpenAI
add acreate_fine_tuning_job
add create_fine_tuning
add litellm.create_fine_tuning_job
fix(factory.py): handle special keys for mistral chat template
fix linting error - cohere_chat
check litellm header in login on ui
better debugging for custom headers
ui use setGlobalLitellmHeaderName
ui - allow entering custom model names
docs(main.py): update acompletion_with_retries docstring
build(model_prices_and_context_window.json): update model info for llama3.1 on bedrock - supports tool calling, not tool choice
build(deps): bump fast-xml-parser in /docs/my-website
docs guardrailConfig
Update cohere_chat.py
fixes: #4947 Bedrock context exception does not have a response
Update cohere_chat.py
proxy server
docs - Bedrock Guardrails
test - bedrock guardrailConfig
feat - support guardrailConfig
types add GuardrailConfigBlock
fix(utils.py): fix trim_messages to handle tool calling
build(pre-commit.yaml): update
fix(utils.py): correctly re-raise azure api connection error
Fix: #4942. Remove verbose logging when exception can be handled
fix(exceptions.py): use correct status code for content policy exceptions
fix(utils.py): check if tools is iterable before indexing into it
log output from /audio on langfuse
fix(caching.py): support /completion caching by default
test - logging litellm-atranscription
fix default input/output values for /audio/trancription logging
log file_size_in_mb in metadata
fix(auth_checks.py): handle writing team object to redis caching correctly
Allow zero temperature for Sagemaker models based on config
docs(vertex.md): add mistral api to docs
fix(factory.py): support mistral ai prefix:true in messages
fix: utils.py
feat(databricks.py): support vertex mistral cost tracking
fix checking mode on health checks
allow setting max request / response size on admin UI
set max_response_size_mb
feat check check_response_size_is_safe
feat - check max response size
fix(databricks.py): handle DONE chunk from databricks
docs(user_keys.md): improve openai migration docs
docs set max_request_size
build: cookbook on migrating to litellm proxy from openai/azure sdk
security - check max request size
fix(utils.py): support fireworks ai finetuned models
fix(utils.py): support fireworks ai finetuned models
feat(utils.py): fix openai-like streaming
fix otel test
fix(databricks.py): fix client used
fix(utils.py): add exception mapping for databricks errors
feat(vertex_ai_partner.py): initial working commit for calling vertex ai mistral
fix otel logging
test otel for batch_write_to_db
use common helpers for writing to otel
refactor use common helper
add new BATCH_WRITE_TO_DB type for service logger
use _get_parent_otel_span_from_kwargs
move _get_parent_otel_span_from_kwargs to otel.py
feat - use log_to_opentelemetry for _PROXY_track_cost_callback
build(model_prices_and_context_window.json): add mistral nemo + codestral pricing
build(model_prices_and_context_window.json): add mistral-large on vertex ai pricing
feat - clearly show version litellm enterprise
fix update public key
docs(debugging.md): cleanup docs
bump: version 1.42.4 → 1.42.5
Fix Datadog JSON serialization
fix
docs(ollama.md): add ollama tool calling to docs
feat(ollama_chat.py): support ollama tool calling
bump: version 1.42.3 → 1.42.4
docs fix link https://models.litellm.ai/
feat link to model cost map on swagger
add litellm_header_name endpoint
feat(vertex_httpx.py): support logging citation metadata
feat(vertex_httpx.py): support logging vertex ai safety results to langfuse
fix(utils.py): fix cache hits for streaming
docs batches
docs batches API
use correct link on  http://localhost:4000
docs batches api
test get batches by id
add verbose_logger.debug to retrieve batch
fix for GET /v1/batches{batch_id:path}
test - batches endpoint
fix batches inserting metadata
fix /v1/batches POST
fix raise better error when crossing tpm / rpm limits
Better JSON serialization for Datadog logs
Use milliseconds for response_time in Datadog logs
Use underscores
fix(proxy_server.py): fix get secret for environment_variables
docs(stream.md): add streaming token usage info to docs
fix(bedrock_httpx.py): fix streaming error message
docs(docusaurus.config.js): add llm model cost map to docs
docs(config.md): update wildcard docs
feat(proxy_server.py): handle pydantic mockselvar error
remove ui shift on reload
fix import and add fallback
wrap existing search bar
update to latest
docs(custom_llm_server.md): cleanup docs
fix(vertex_ai_llama3.py): Fix llama3 streaming issue
bump: version 1.42.2 → 1.42.3
fix(litellm_cost_calc/google.py): support meta llama vertex ai cost tracking
bump: version 1.42.1 → 1.42.2
Update README.md
update lock
read me link to using litellm
deploy link to using litellm
improvements
docs using litellm proxy
test(test_router.py): handle azure api instability
docs -quick start
fix(utils.py): don't raise error on openai content filter during streaming - return as is
example mistral sdk
add mistral sdk usage
Add mistral.mistral-large-2407-v1:0 on Amazon Bedrock.
fix: now supports single tokens prediction
fix(custom_llm.py): pass input params to custom llm
feat(proxy_server.py): support custom llm handler on proxy
docs(custom_llm_server.md): add calling custom llm server to docs
feat(utils.py): support async streaming for custom llm provider
feat(utils.py): support sync streaming for custom llm provider
fix(custom_llm.py): support async completion calls
feat(custom_llm.py): initial working commit for writing your own custom LLM handler
docs - add info about routing strategy on load balancing docs
feat  support audio health checks for azure
docs add example on using text to speech models
feat - support health check audio_speech
fix whisper health check with litellm
fix(router.py): add support for diskcache to router
fix(proxy_server.py): check if input list > 0 before indexing into it
Check for converse support first.
Support tool calling for Llama 3.1 on Amazon bedrock.
Add Llama 3.1 405b for Bedrock
docs(enterprise.md): cleanup docs
docs(enterprise.md): cleanup docs
docs(caching.md): update caching docs to include ttl info
fix(main.py): fix calling openai gpt-3.5-turbo-instruct via /completions
fix(internal_user_endpoints.py): support updating budgets for `/user/update`
docs add mistral api large 2
feat - add mistral large 2
bump: version 1.42.0 → 1.42.1
docs groq models
feat - add groq/llama-3.1
test: cleanup testing
feat(auth_check.py): support using redis cache for team objects
fix using pass_through_all_models
fix logfire - don't load_dotenv
docs on pass through support
fix(custom_llm.py): pass input params to custom llm
docs - anthropic
test proxy all model
support using */*
customize
eject default UI
install canary (default UI)
add ANTHROPIC_API_KEY on build and test
router support setting pass_through_all_models
router support setting pass_through_all_models
feat(proxy_server.py): support custom llm handler on proxy
docs(custom_llm_server.md): add calling custom llm server to docs
docs - add info about routing strategy on load balancing docs
feat  support audio health checks for azure
docs add example on using text to speech models
feat - support health check audio_speech
fix whisper health check with litellm
feat(utils.py): support async streaming for custom llm provider
feat(utils.py): support sync streaming for custom llm provider
fix(custom_llm.py): support async completion calls
feat(custom_llm.py): initial working commit for writing your own custom LLM handler
fix(router.py): add support for diskcache to router
fix(proxy_server.py): check if input list > 0 before indexing into it
Check for converse support first.
Support tool calling for Llama 3.1 on Amazon bedrock.
Add mistral.mistral-large-2407-v1:0 on Amazon Bedrock.
Add Llama 3.1 405b for Bedrock
anthropic gateway fixes
docs(enterprise.md): cleanup docs
docs(enterprise.md): cleanup docs
fix: now supports single tokens prediction
docs(caching.md): update caching docs to include ttl info
fix(main.py): fix calling openai gpt-3.5-turbo-instruct via /completions
fix(internal_user_endpoints.py): support updating budgets for `/user/update`
docs add mistral api large 2
feat - add mistral large 2
bump: version 1.42.0 → 1.42.1
docs groq models
feat - add groq/llama-3.1
test: cleanup testing
support dynamic api base
add support for friendli dedicated endpoint
test(test_completion.py): update azure extra headers
feat(auth_check.py): support using redis cache for team objects
tab
tools_call to Helicone
Update README.md
Allow not displaying feedback box
fix(key_management_endpoints.py): if budget duration set, set budget_reset_at
doc example using litellm proxy with groq
test(test_embedding.py): add simple azure embedding ad token test
test(test_completion.py): add basic test to confirm azure ad token flow works as expected
fix(bedrock_httpx.py): fix async client check
test UnsupportedParamsError
add UnsupportedParamsError to litellm exceptions
feat use UnsupportedParamsError as litellm error type
build(docker-compose.yml): add prometheus scraper to docker compose
(test_embedding.py) - Re-enable embedding test with Azure OIDC.
Fix test_prompt_factory flake8 warning
(test_secret_manager.py) - Improve and add CircleCI v1 test with Amazon.
update azure_ai llamav31 prices with sources
(tests) - Skip embedding Azure AD test for now.
(tests) - Try azure AD auth directly.
test(test_embedding.py): fix base url
test - logging langsmith tags
docs - logging langsmith tags
langsmith - support logging tags
Add Llama 3.1 for Bedrock.
Check existence of multiple views in 1 query
bump: version 1.41.28 → 1.42.0
fix(anthropic.py): support openai system message being a list
fix(__init__.py): update init
build(model_prices_and_context_window.json): add model pricing for vertex ai llama 3.1 api
feat(vertex_ai_llama.py): vertex ai llama3.1 api support
bump: version 1.41.27 → 1.41.28
fix DB accept null values for api_base, user, etc
feat - add azure_ai llama v3.1 8B 70B and 405B
docs(guardrails.md): add team-based controls to guardrails
Pass litellm proxy specific metadata
fix add better debugging _PROXY_track_cost_callback
test_anthropic_completion_input_translation_with_metadata
(test - azure): Add test for Azure OIDC auth.
fix(utils.py): support raw response headers for streaming requests
docs(raw_request_response.md): show how to get openai headers from response
feat(utils.py): support passing openai response headers to client, if enabled
doc alert_to_webhook_url
fix triton linting
doc - using anthropic with litellm proxy server
docs(sidebar.js): add oidc to left nav
docs alerting
feat alert_to_webhook_url
feat - set alert_to_webhook_url
update alert_to_webhook_url
docs - alert to webhook_url
docs - slack alerting
docs Debugging / Troubleshooting
(docs): Make it more obvious where the group name is set in the example.
(docs): Add OIDC doc.
feat - add success_Callback per request
add debug logging for team callback settings
add endpoint to disable logging for a team
test: re-run ci/cd
docs: cleanup docs
fix(init_callbacks.py): fix presidio optional param
fix(proxy/utils.py): add stronger typing for litellm params in failure call logging
docs - team logging endpoints
bump: version 1.41.26 → 1.41.27
doc - team based logging
docs - control team logging
feat(redact_messages.py): allow remove sensitive key information before passing to logging integration
GET endpoint to get team callbacks
only allow unique callbacks for team callbacks
fix(main.py): check if anthropic api base ends with required url
feat add return types on team/callback
feat(lakera_ai.py): support running prompt injection detection lakera check pre-api call
feat(lakera_ai.py): control running prompt injection between pre-call and in_parallel
control team callbacks using API
feat - return team_metadata in user_api_key_auth
feat - add endpoint to set team callbacks
fix(openai.py): check if error body is a dictionary before indexing in
types - AddTeamCallback
test(test_braintrust.py): add testing for braintrust integration
docs(braintrust.md): add braintrust.md to docs
feat(braintrust_logging.py): working braintrust logging for successful calls
test - openai content policy errors
fix raise correct provider on content policy violation
fix checking if _known_custom_logger_compatible_callbacks
set _known_custom_logger_compatible_callbacks in _init
fix using arize as success callback
docs - langsmith
feat(braintrust.py): initial commit for braintrust integration
add test for anthropic routes
update tests
check is_llm_api_route
track anthropic_routes
doc arize ai
fix(vertex_httpx.py): Change non-blocking vertex error to warning
feat - arize ai log llm i/o
otel - log to arize ai
test - arize ai basic logging
add arize.py
feat - arize ai open inference types
feat - add support to init arize ai
Fix errors with docker-compose file
Revert "Fix: use Bedrock region from environment variables before other region definitions"
feat(auth_checks.py): Allow admin to disable team from turning on/off guardrails.
Add support for Triton streaming & triton async completions
Update utils.py
Update model_prices_and_context_window.json
remove unused env var
skip helm build for dev builds
remove fetch-depth
use git describe to find latest tag
move helm after build and use litellm for chart_name
refactor(main.py): migrate vertex gemini calls to vertex_httpx
fix edit docker file ui base path
fix allow setting UI _BASE path
#added type ignore for httpx and requests
#Fixed mypy errors. The requests package and stubs need to be imported - waiting to hear from Ishaan/Krrish before changing requirements.txt

2.1.5-24-11-18 / 2024-11-18 12:30:56
Handled model
handled provoider
add model in error info
throw exception if API Keys are not in UI

2.1.6-24-12-09 / 2024-12-09 12:20:58
update openai package version
Updated config.yml
bump: version 1.53.1 → 1.53.2
LiteLLM Minor Fixes & Improvements (11/29/2024)  (#6965)
fix(key_management_endpoints.py): support 'tags' param on `/key/update` (#6945)
(feat) Allow disabling ErrorLogs written to the DB  (#6940)
fix doc string
(fix) tag merging / aggregation logic   (#6932)
(feat) add enforcement for unique key aliases on /key/update and /key/generate  (#6944)
(docs + fix) Add docs on Moderations endpoint, Text Completion  (#6947)
Revert "Revert "(feat) Allow using include to include external YAML files in a config.yaml (#6922)""
(fix) handle json decode errors for DD exception logging (#6934)
(bug fix) /key/update was not storing `budget_duration` in the DB  (#6941)
docs: update the docs (#6923)
LiteLLM Minor Fixes & Improvements (11/27/2024) (#6943)
LiteLLM Minor Fixes & Improvements (11/26/2024)  (#6913)
Revert "(feat) Allow using include to include external YAML files in a config.yaml (#6922)"
bump: version 1.53.0 → 1.53.1
build(ui/): update ui build
(feat) dd logger - set tags according to the values set by those env vars  (#6933)
bump: version 1.52.16 → 1.53.
(feat) Allow using include to include external YAML files in a config.yaml (#6922)
(feat) log proxy auth errors on datadog  (#6931)
(feat) DataDog Logger - Add Failure logging + use Standard Logging payload (#6929)
sonnet supports pdf, haiku does not (#6928)
(feat) pass through llm endpoints - add `PATCH` support (vertex context caching requires for update ops)  (#6924)
fix(key_management_endpoints.py): fix user-membership check when creating team key (#6890)
run ci/cd again for new release
test: temporarily comment out doc test - fix ci/cd issue in separate pr
test: fix test
test: fix documentation tests
bump: version 1.52.15 → 1.52.16
(docs) Simplify `/vertex_ai/` pass through docs   (#6910)
docs(router_architecture.md): add router architecture docs
(redis fix) - fix `AbstractConnection.__init__() got an unexpected keyword argument 'ssl'` (#6908)
(fix) pass through endpoints - run logging async + use thread pool executor for sync logging callbacks  (#6907)
ui new build
(UI fix) UI does not reload when you login / open a new tab (#6909)
(feat) Add support for using @google/generative-ai JS with LiteLLM Proxy  (#6899)
feat - allow sending `tags` on vertex pass through requests  (#6876)
(feat) - provider budget improvements - ensure provider budgets work with multiple proxy instances + improve latency to ~90ms  (#6886)
(QOL improvement) Provider budget routing - allow using 1s, 1d, 1mo, 2mo etc  (#6885)
update doc title
build(ui/): update ui build
docs - have 1 section for routing +load balancing (#6884)
bump: version 1.52.14 → 1.52.15
build: update ui build
Litellm dev 11 23 2024 (#6881)
fix e2e ui testing, only run e2e ui testing in playwright
fix e2e ui testing
fix e2e ui testing deps
fix playwright e2e ui test
LiteLLM Minor Fixes & Improvements (11/23/2024)  (#6870)
(Perf / latency improvement) improve pass through endpoint latency to ~50ms (before PR was 400ms)  (#6874)
Bump cross-spawn from 7.0.3 to 7.0.6 in /ui/litellm-dashboard (#6865)
fix tests (#6875)
(feat) use `@google-cloud/vertexai` js sdk with litellm  (#6873)
fix coverage
add pass_through_unit_testing
test: skip flaky test
test - also try diff host for langfuse
fix test_aaateam_logging
fix doc format
bump: version 1.52.13 → 1.52.14
ci/cd run again
test_langfuse_masked_input_output
test_langfuse_masked_input_output
test_langfuse_masked_input_output
test_langfuse_logging_audio_transcriptions
fix test_aaateam_logging
fix test_aaapass_through_endpoint_pass_through_keys_langfuse
test_team_logging
test_aaalangfuse_logging_metadata
docs - Send `litellm_metadata` (tags)
(Feat) Allow passing `litellm_metadata` to pass through endpoints + Add e2e tests for /anthropic/ usage tracking  (#6864)
(feat) Add usage tracking for streaming `/anthropic` passthrough routes (#6842)
(fix) add linting check to ban creating `AsyncHTTPHandler` during LLM calling  (#6855)
fix latency issues on google ai studio (#6852)
docs: update json mode docs
bump: version 1.52.12 → 1.52.13
Litellm dev 11 21 2024 (#6837)
(fix) passthrough - allow internal users to access /anthropic  (#6843)
test: cleanup mistral model
(fix) don't block proxy startup if license check fails & using prometheus  (#6839)
(testing) - add e2e tests for anthropic pass through endpoints  (#6840)
(feat) add usage / cost tracking for Anthropic passthrough routes (#6835)
(refactor) anthropic -  move _process_response in transformation.py  (#6834)
Litellm dev 11 20 2024 (#6838)
build: update ui build
bump: version 1.52.11 → 1.52.12
Litellm dev 11 20 2024 (#6831)
Add gpt-4o-2024-11-20. (#6832)
LiteLLM Minor Fixes & Improvements (11/19/2024)  (#6820)
build: run new build
Feat/provider apis (#4)
test: fix test
ci/cd run again
use correct name for test file
fix test_prometheus_metric_tracking
(feat) provider budget routing improvements  (#6827)
(Feat) Add provider specific budget routing (#6817)
build: fix test
Litellm stable pr 10 30 2024 (#6821)
feat - add qwen2p5-coder-32b-instruct (#6818)
(Proxy) add support for DOCS_URL and REDOC_URL (#6806)
docs(gemini.md): add embeddings as a supported endpoint for gemini models
bump: version 1.52.10 → 1.52.11
Litellm lm studio embedding params (#6746)
(docs) add docstrings for all /key, /user, /team, /customer endpoints  (#6804)
Docs -  use 1 page for all logging integrations on proxy + add logging features at top level  (#6805)
Bump cross-spawn from 7.0.3 to 7.0.5 in /ui (#6779)
(docs) simplify left nav names + use a section for `making llm requests`  (#6799)
(docs improvement) remove emojis, use `guides` section, categorize uncategorized docs  (#6796)
(fix) httpx handler - bind to ipv4 for httpx handler  (#6785)
build: add gemini-exp-1114 (#6786)
handle vertex ServiceUnavailableError for codestral
vertex_ai/codestral@2405 is very unstable - handle their instability in our tests
handle codestral@2405 instability
bump: version 1.52.9 → 1.52.10
new ui build
(fix) Azure AI Studio - using `image_url` in content with both text and image_url  (#6774)
(patch) using image_urls with `vertex/anthropic` models  (#6775)
fix test_completion_codestral_fim_api_stream
(docs) add doc string for /key/update  (#6778)
(UI) fix - allow editing key alias on Admin UI  (#6776)
(Admin UI) - Remain on Current Tab when user clicks refresh  (#6777)
(Doc) Add section on what is stored in the DB + Add clear section on key/team based logging  (#6769)
Update routing references (#6758)
add openrouter/qwen/qwen-2.5-coder-32b-instruct (#6731)
(feat) Use `litellm/` prefix when storing virtual keys in AWS secret manager  (#6765)
(fix) Fix - don't allow `viewer` roles to create virtual keys  (#6764)
(Feat) Add Vertex Model Garden llama 3.1 models  (#6763)
feat - add us.llama 3.1 models (#6760)
LiteLLM Minor Fixes & Improvements (11/13/2024)  (#6729)
bump: version 1.52.8 → 1.52.9
(feat) Vertex AI - add support for fine tuned embedding models  (#6749)
fix imagegeneration output_cost_per_image on model cost map (#6752)
fix: import audio check (#6740)
[Feature]: json_schema in response support for Anthropic  (#6748)
[Feature]: Stop swallowing up AzureOpenAi exception responses in litellm's implementation for a BadRequestError (#6745)
(feat) add bedrock/stability.stable-image-ultra-v1:0 (#6723)
docs(logging.md): add 'trace_id' param to standard logging payload
docs(reliability.md): add tutorial on disabling fallbacks per key
docs: add docs on jina ai rerank support
bump: version 1.52.7 → 1.52.8
LiteLLM Minor Fixes & Improvement (11/14/2024)  (#6730)
(Feat) Add support for storing virtual keys in AWS SecretManager  (#6728)
mark Helm PreSyn as BETA
fix test_supports_response_schema
Update prefix.md (#6734)
Update code blocks huggingface.md (#6737)
(docs) add instructions on how to contribute to docker image
test - handle eol model claude-2, use claude-2.1 instead
fix prisma migration
fix migration job
fix migrations-job.yaml
update doc on pre sync hook
fix migration job
fix yaml on migrations job
use existing spec for migrations job
fix DATABASE_URL
fix migration job.yaml
docs helm pre sync hook
helm run DISABLE_SCHEMA_UPDATE
docs proxy_budget_rescheduler_min_time
bump: version 1.52.6 → 1.52.7
Litellm key update fix (#6710)
(build) helm db sync hook
(build) helm db pre sync hook
(build) update db helm hook
fix remove dup test (#6718)
(feat) Add cost tracking for Azure Dall-e-3 Image Generation  + use base class to ensure basic image generation tests pass  (#6716)
(fix) using Anthropic `response_format={"type": "json_object"}`  (#6721)
Fix: Update gpt-4o costs to that of gpt-4o-2024-08-06 (#6714)
(fix proxy redis) Add redis sentinel support  (#6154)
doc fix Using Http/2 with Hypercorn
fix migration job
fix db migration helm hook
fix argo cd annotations
handle standalone DB on helm hook
fix migrations job.yml
(feat) helm hook to sync db schema  (#6715)
bump: version 1.52.5 → 1.52.6
LiteLLM Minor Fixes & Improvements (11/12/2024)  (#6705)
add defaults used for GCS logging
bump: version 1.52.4 → 1.52.5
fix raise correct error 404 when /key/info is called on non-existent key  (#6653)
(feat) add cost tracking stable diffusion 3 on Bedrock  (#6676)
(docs) add benchmarks on 1K RPS  (#6704)
add xAI on Admin UI (#6680)
(fix) OpenAI's optional messages[].name  does not work with Mistral API  (#6701)
(Feat) Add langsmith key based logging (#6682)
Add docs to export logs to Laminar (#6674)
add clear doc string for GCS bucket logging
Litellm dev 11 11 2024 (#6693)
bump: version 1.52.3 → 1.52.4
(Feat) 273% improvement GCS Bucket Logger - use Batched Logging (#6679)
fix model cost map stability.sd3-large-v1:0
(feat) Add Bedrock Stability.ai Stable Diffusion 3 Image Generation models  (#6673)
(feat) Add support for logging to GCS Buckets with folder paths  (#6675)
move image gen testing
added async support for bedrock image gen
add bedrock image gen async support
bump: version 1.52.2 → 1.52.3
(pricing): Fix multiple mistakes in Claude pricing, and also increase context length allowed for Claude 3.5 Sonnet v2 on Bedrock. (#6666)
Litellm dev 11 08 2024 (#6658)
Litellm dev 11 07 2024 (#6649)
ci(conftest.py): reset conftest.py for local_testing/ (#6657)
build: update backup model prices map
Update several Azure AI models in model cost map (#6655)
(feat) log error class, function_name on prometheus service failure hook + only log DB related failures on DB service hook  (#6650)
(QOL improvement) add unit testing for all static_methods in litellm_logging.py  (#6640)
Update gpt-4o-2024-08-06, and o1-preview, o1-mini models in model cost map  (#6654)
chore: comment for maritalk (#6607)
fix(pattern_match_deployments.py): default to user input if unable to… (#6632)
fix(pattern_match_deployments.py): default to user input if unable to map based on wildcards (#6646)
fix code quality check
fix test_get_gcs_logging_config_without_service_account
(feat) Allow failed DB connection requests to allow virtual keys with `allow_failed_db_requests`  (#6605)
Update team_budgets.md (#6611)
fix test_get_gcs_logging_config_without_service_account
(fix) ProxyStartup - Check that prisma connection is healthy when starting an instance of LiteLLM  (#6627)
Update opentelemetry_integration.md - Fix typos (#6618)
(feat) GCS Bucket logging. Allow using IAM auth for logging to GCS  (#6628)
bump: version 1.52.0 → 1.52.1
LiteLLM Minor Fixes & Improvements (11/06/2024) (#6624)
LiteLLM Minor Fixes & Improvements (11/05/2024) (#6590)
ci: remove redundant lint.yml workflow (#6622)
fix flake8 checks
doc fix team based logging with langfuse
docs fix clarify team_id on team based logging
LiteLLM Minor Fixes & Improvements (11/04/2024)  (#6572)
(DB fix) don't run apply_db_fixes on startup (#6604)
test: handle anthropic api instability
test: mark flaky test
fix(lowest_tpm_rpm_routing.py): fix parallel rate limit check (#6577)
bump: version 1.51.5 → 1.52.0
(fix) Vertex Improve Performance when using `image_url`  (#6593)
(feat) add `Predicted Outputs` for OpenAI  (#6594)
testing fix bedrock deprecated cohere.command-text-v14
fix allow using 15 seconds for premium license check
(fix) litellm.text_completion raises a non-blocking error on simple usage (#6546)
fix ImageObject conversion (#6584)
build: fix json for model map
build: fix map
build: fix map
Litellm perf improvements 3 (#6573)
Add 3.5 haiku (#6588)
(proxy fix) - call connect on prisma client when running setup (#6534)
docs(virtual_keys.md): update Dockerfile reference (#6554)
build(deps): bump cookie and express in /docs/my-website (#6566)
bump: version 1.51.4 → 1.51.5
Litellm dev 11 02 2024 (#6561)
bump: version 1.51.3 → 1.51.4
docs(lm_studio.md): add tutorial on finding supported params
docs(lm_studio.md): add doc on lm studio support
LiteLLM Minor Fixes & Improvements (11/01/2024)  (#6551)
bump: version 1.51.2 → 1.51.3
LiteLLM Minor Fixes & Improvements (10/30/2024) (#6519)
(feat) add XAI ChatCompletion Support  (#6373)
(fix) slack alerting - don't spam the failed cost tracking alert for the same model  (#6543)
Add retry strat (#6520)
ui new build
(UI) Fix viewing members, keys in a team + added testing  (#6514)
test: refactor gemini test to use mock, prevent ratelimit error
(UI) fix + test displaying number of keys an internal user owns  (#6507)
fix ui check when budget is 0 (#6506)
fix: fix linting error
bump: version 1.51.1 → 1.51.2
Litellm router max depth (#6501)
Litellm dev 10 29 2024 (#6502)
LiteLLM Minor Fixes & Improvements (10/28/2024)  (#6475)
(perf) Litellm redis router fix - ~100ms improvement (#6483)
Update utils.py (#6468)
Add `azure/gpt-4o-mini-2024-07-18` to model_prices_and_context_window.json (#6477)
bump: version 1.51.0 → 1.51.1
(fix) `PrometheusServicesLogger` `_get_metric` should return metric in Registry  (#6486)
(fix) proxy - fix when `STORE_MODEL_IN_DB` should be set (#6492)
(router_strategy/) ensure all async functions use async cache methods (#6489)
docs clarify vertex vs gemini
(fix) Prometheus - Log Postgres DB latency, status on prometheus  (#6484)
redis otel tracing + async support for latency routing (#6452)
(Testing) Add unit testing for DualCache - ensure in memory cache is used when expected  (#6471)
Litellm dev 10 26 2024 (#6472)
LiteLLM Minor Fixes & Improvements (10/24/2024) (#6441)
add pricing for amazon.titan-embed-image-v1 (#6444)
(Feat) New Logging integration - add Datadog LLM Observability support  (#6449)
(testing) increase prometheus.py test coverage to 90%  (#6466)
(UI) Delete Internal Users on Admin UI  (#6442)
LiteLLM Minor Fixes & Improvements (10/24/2024) (#6421)
bump: version 1.50.4 → 1.51.0
ui new build
fix linting
fix type error
fix test audit logs
unit testing test_create_audit_log_in_db
fix code quality
fix create_audit_log_for_update
fix StandardLoggingMetadata with user_api_key_org_id
fix LitellmTableNames type
use separate file for create_audit_log_for_update
add unit testing for non_proxy_admin_allowed_routes_check
fix typing on StandardLoggingMetadata
fix RouteChecks  test
unit test route checks
fix name of tests on config
test_is_ui_route_allowed
use helper for _route_matches_pattern
use static methods for Routechecks
add key/{token_id}/regenerate to internal user routes
ui show created at date
feat(litellm_pre_call_utils.py): support 'add_user_information_to_llm… (#6390)
feat(proxy_server.py): check if views exist on proxy server startup +… (#6360)
allow configuring httpx hooks for AsyncHTTPHandler (#6290) (#6415)
add created_at, updated_at for verification token
track created, updated at virtual keys
LiteLLM Minor Fixes & Improvements (10/23/2024) (#6407)
feat(litellm_logging.py): refactor standard_logging_payload function … (#6388)
perf: remove 'always_read_redis' - adding +830ms on each llm call (#6414)
bump: version 1.50.3 → 1.50.4
(code cleanup) remove unused and undocumented logging integrations - litedebugger, berrispend  (#6406)
(refactor) router - use static methods for client init utils  (#6420)
(refactor) prometheus async_log_success_event to be under 100 LOC  (#6416)
fix comment
add code cov checks

2.1.7-25-01-06 / 2025-01-06 07:02:17
fix: :bug: fixed response structure (#12)
feat: :sparkles: raise 401 if required keys not present (#11)
Added json validation in config

2.1.8-25-01-09 / 2025-01-09 13:12:31
fixed (#13)

2.1.9-25-01-23 / 2025-01-23 12:46:08
No new changes in the release

2.2.0-25-02-06 / 2025-02-06 11:16:43
No new changes in the release

2.3.0-25-02-12 / 2025-02-12 12:19:16
No new changes in the release

2.4.0-25-02-14 / 2025-02-14 04:21:12
No new changes in the release

2.5.0-25-02-14 / 2025-02-14 13:40:43
No new changes in the release

2.6.0-25-02-17 / 2025-02-17 06:09:27
No new changes in the release

2.7.0-25-02-28 / 2025-02-28 09:23:08
No new changes in the release

2.8.0-25-03-04 / 2025-03-04 13:39:37
No new changes in the release

2.9.0-25-03-10 / 2025-03-10 11:57:25
No new changes in the release

2.10.0-25-03-19 / 2025-03-19 07:26:26
No new changes in the release

2.11.0-25-03-31 / 2025-03-31 09:07:18
handle output schema
api gateway changes

2.11.1-25-04-03 / 2025-04-03 11:44:46
No new changes in the release

2.11.2-25-04-08 / 2025-04-08 08:58:29
No new changes in the release

2.12.0-25-04-09 / 2025-04-09 06:08:07
add api gateway logic to raga-dev
hvac requirements
raise exception in setting api keys
fix(common_utils.py): fix linting error
fix: cleanup
Connect UI to "LiteLLM_DailyUserSpend" spend table - enables usage tab to work at 1m+ spend logs (#9603)
fix(proxy_server.py): Fix https://github.com/BerriAI/litellm/issues/9576
test: run test earlier to catch error
Support discovering gemini, anthropic, xai models by calling their `/v1/model` endpoint (#9530)
fix(mistral_chat_transformation.py): add missing comma (#9606)
test: fix test
test: skip flaky test - failing due to db timeouts - unrelated to test
test: mark flaky test
Revert "Support max_completion_tokens on Mistral (#9589)" (#9604)
Support max_completion_tokens on Mistral (#9589)
Litellm new UI build (#9601)
Litellm fix db testing (#9593)
docs(index.md): document new team model flow
bump: version 1.64.1 → 1.65.0
Allow team admins to add/update/delete models on UI + show api base and model id on request logs (#9572)
docs(openai.md): add gpt-4o-transcribe to docs
Allow viewing keyinfo on request logs (#9568)
Add OpenAI gpt-4o-transcribe support (#9517)
build(model_prices_and_context_window.json): add gemini multimodal embedding cost
docs prod.md
test_db_health_readiness_check_with_prisma_error
test_handle_db_exception_with_connection_error
fix order of _setup_prisma_client
fix _setup_prisma_client
refactor tests
allow proxy to startup on DB unavailable
bug fix - allow pods to startup when DB is unavailable
refactor PrismaDBExceptionHandler
allow_requests_on_db_unavailable
docs fix
test -  auth exception handler
test_is_database_connection_error_prisma_errors
Support Gemini audio token cost tracking + fix openai audio input token cost tracking (#9535)
test: fix test
build(migration.sql): add migration file for new dailyusertable
test(test_db_schema_migration.py): ci/cd test to enforce schema migrations are documented in .sql files
Add Daily User Spend Aggregate view - allows UI Usage tab to work > 1m rows (#9538)
ci(config.yml): add pytest-postgres to ci/cd
feat(prisma-migrations): add baseline db migration file (#9565)
fix ProxyException
fix auth checks
fix get_key_object
is_database_connection_error
fix test changes
undo config.yml changes
rename _is_model_gemini_spec_model
test_get_supports_system_message
unit tests for VertexGeminiConfig
test_gemini_fine_tuned_model_request_consistency
test_gemini_fine_tuned_model_request_consistency
doc fix Fine-tuned Models
docs vertex ft model
docs verte ft models
docs vertex ft models
docs litellm vertex ai ft models
_get_model_name_from_gemini_spec_model
Nova Canvas complete image generation tasks (#9177) (#9525)
rename _is_model_gemini_spec_model
test fix test_pick_cheapest_chat_model_from_llm_provider
undo changes to utils
pick_cheapest_chat_models_from_llm_provider
undo code changes
fix llm request utils
_is_model_gemini_gemini_spec_model
fix vertex embedding perf test
DNS lookup for Redis host
setup_google_dns
DNS lookup for Redis host
build(model_prices_and_context_window.json): add commercial rate limits for gemini 2.0 flash lite
fix DNS resolution
fix load_testing
fix caching unit tests
fix user_api_key_auth example config
run ci/cd again
select_model_for_request_transformation
fix util vertex
_transform_request_body
fix get_optional_params
fix base_model in param mapping
test_gemini_fine_tuned_model_request_consistency
Support `litellm.api_base` for vertex_ai + gemini/ across completion, embedding, image_generation (#9516)
fix gemini/gemini-2.0-flash-lite on model cost map
handle failed db connections
docs(config_settings.md): cleanup docs
ci: update github action
setup_google_dns
setup_google_dns
litellm_assistants_api_testing bump python
Set DNS
test_create_delete_assistants
run ci/cd again
test_litellm_proxy_server_config_no_general_settings
fix startup
add test config
test_litellm_proxy_server_config_no_general_settings
litellm_proxy_reliability_tests
fix setup toxi proxy
litellm_proxy_reliability_tests
run toxi proxy tests
Setup Toxiproxy
litellm_proxy_reliability_tests
TOXI_PROXY_DATABASE_URL
ci(publish-migrations.yml): add action for publishing prisma db migrations (#9537)
fix path
fix setup
add toxi proxy tests to ci/cd
fixes for auth checks
fix ProxyErrorTypes
UserAPIKeyAuthExceptionHandler
docs: cleanup docs
docs(admin_ui_sso.md): add logout url
docs: update release note with patch
add gemini/gemini-2.0-flash-lite
add vertex gemini-2.0-flash-lite
fix docker compose
fix vertex ai multimodal embedding translation (#9471)
test: improve flaky test
Update model_prices_and_context_window.json (#9459)
Add vertexai topLogprobs support (#9518)
Expose MCP tools
mcp docs, exposing tools now live
fix mcp test deps
bump: version 1.64.0 → 1.64.1
pip install "langchain_mcp_adapters==0.0.5"
fix import mcp router
fix code quality
fix mcp import
fix setup
langchain_mcp_adapters
add "mcp==1.5.0" to optional pyproject for litellm proxy
fix linting errors
bump deps
fix using mcp router
fix(invoke_handler.py): remove hard code
ci: update with clean db
fix mcp version
bump MCP
Show current pydantic version
fix py version
update requirements.txt
litellm_utils_testing
update to cimg/python:3.13.1 on ci/cd
fix order of load tests on config.yml
fix flaky test test_openai_responses_api_web_search_cost_tracking
assert_gcs_pubsub_request_matches_expected_standard_logging_payload
test(test_spend_management_endpoints.py): add unit testing for router + spend logs
test(test_spend_management_endpoints.py): guarantee consistent spend logs
default to use SLP for GCS PubSub
test: add e2e testing
test: add unit test
fix(litellm_logging.py): always log the api base
fix mcp type imports
bump: version 1.63.15 → 1.64.0
fix pydantic import error
update redisvl dependency
build: add new vertex text embedding model
docs: cleanup
docs: cleanup
docs: cleanup docs
docs: cleanup docs
docs: cleanup docs
docs: update docs
docs: document streaming iterator guardrail
docs(index.md): completed documentation for new stable release
bump: version 1.63.14 → 1.63.15
test tool call cost tracking
docs(index.md): document new standard logging payload param
docs(arize_integration.md): update docs with more details
docs web search
docs web search
docs web search
docs web search
docs(openai.md): add doc on pdf parsing for openai
docs web search
docs web search
bump pydantic
docs(release_notes/): add more to docs
add gpt-4o-2024-08-06 pricing for web tools
fix bug when unable to look up model info
docs - update this to upcoming features
docs clarify /mcp endpoint readiness on litellm
test_langfuse_logging_completion
_get_file_search_tool_call
_get_file_search_tool_call
fix: fix linting error
FileSearchTool
fix StandardBuiltInToolsParams
chat_completion_response_includes_annotations
fix code quality check
test_openai_responses_api_web_search_cost_tracking
test_openai_responses_api_web_search_cost_tracking
test_openai_responses_api_web_search_cost_tracking
get_cost_for_built_in_tools
move web search cost tracking
add web search to gpt-4o-2024-08-06
fix model cost map
fix model cost map
fixes for web search cost tracking
StandardBuiltInToolCostTracking
initialize_standard_built_in_tools_params
add cost tracking for StandardBuiltInToolCostTracking
fix(router.py): fix get_model_list to return all wildcard models
WebSearchOptions
add WebSearchOptions as supported chat completion param
fix(llm_passthrough_endpoints.py): fix raising helpful debug error message
search_context_cost_per_query
gpt-4o-mini-search-preview-2025-03-11
add gpt-4o-search-preview-2025-03-11 to model cost map
test_openai_web_search_logging_cost_tracking
docs(vertex_ai.md): document supported vertex passthrough flows
fix supports web search
fix supports_web_search
fix supports_web_search
test_supports_web_search
fix supports_web_search
add supports_web_search
search_context_cost_per_query test
search_context_cost_per_query
search_context_cost_per_1k_calls
supports_web_search
add supports_web_search
search_context_cost_per_1k_calls
test: update tests
add search_context_cost_per_1k_calls to model cost map spec
build: cleanup unused files
test: fix test
test: migrate testing
fix(llm_passthrough_endpoints.py): raise verbose error if credentials not found on proxy
test_is_chunk_non_empty_with_annotations
Add annotations to the delta
test_openai_web_search_streaming
test: add more e2e testing
fix(vertex_ai/common_utils.py): fix handling constructed url with default vertex config
feat(llm_passthrough_endpoints.py): base case passing for refactored vertex passthrough route
test_openai_web_search
test open ai web search
feat - add openai web search
refactor(llm_passthrough_endpoints.py): refactor vertex passthrough to use common llm passthrough handler.py
add search_context_cost_per_1k_calls
fix typo in predibase.md
test: fix test
bump gunicorn - fix security issue on gunicorn
security fix - bump gunicorn==23.0.0 # server dep
mcp servers.json
bump to pip install "openai==1.68.2"
fix ModelParamHelper
build(deps): bump next from 14.2.21 to 14.2.25 in /ui/litellm-dashboard
bump: version 1.63.13 → 1.63.14
bump version
bump: version 1.63.12 → 1.63.13
bug fix azure/gpt-4.5-preview was added as litellm_provider=openai, should be azure
_get_litellm_supported_transcription_kwargs
ci_cd_server_path
pip install "mcp==1.4.1"
pip install "pydantic==2.7.2"
test_tools.py
test_transform_openai_tool_call_to_mcp_tool_call_request tests
fix: fix linting error
fix mcp client
test: update tests
fix: fix linting error
test(test_internal_user_endpoints.py): add unit testing to handle user_email=None
test mcp on ci/cd
test mcp agent
fix(proxy/_types.py): handle user_email=None
litellm mcp interface
litellm mcp example
fix: remove unused import
fix: remove unused import
docs mcp docs update
refactor(user_api_key_auth.py): move is_route_allowed to inside common_checks
add experimental mcp client
doc mcp example
Add Azure GPT-4.5-Preview
Add Azure Mistral Small 3.1
litellm mcp bridge docs
fix(model_param_helper.py): update `_get_litellm_supported_transcription_kwargs()` to use proper annotations from `TranscriptionCreateParamsNonStreaming` & ``TranscriptionCreateParamsStreaming`
add diagram for litellm mcp integration
docs litellm mcp bridge
test: initial e2e testing to ensure non admin jwt token cannot create new teams
fix(model_param_helper.py): change TranscriptionCreateParams.__annotations__ to TranscriptionCreateParams.__dict__ to clean logging error // handle typeddict
test: mock sagemaker tests
build: update pull_request_template.md
test: mock sagemaker tests
test: update tests
test: add unit testing
fix(in_memory_cache.py): add max value limits to in-memory cache. Prevents OOM errors in prod
clean up
call_openai_tool on MCP client
docs(image_handling.md): architecture doc on image handling on the proxy
fix: fix linting error
fix beta caps
test: fix test - handle llm api inconsistency
fix llm responses
transform_mcp_tool_to_openai_tool
fix(handle_error.py): make cooldown error more descriptive
test: add unit testing
litellm MCP client 1
change location of MCP client
basic MCP client structure
fix(anthropic/chat/transformation.py): correctly update response_format to tool call transformation
simple MCP interface
docs litellm mcp
docs litellm mcp
docs add central platform team control on MCP
doc litellm MCP client
test tool registry
fix naming
bump requirements.txt
add mcp==1.4.1
litellm mcp support
docs mcp tool spec
litellm MCP example
litellm MCP
add litellm mcp endpoints
fix(team_endpoints.py): consistently return 404 if team not found in DB
feat(pass_through_endpoints.py): return api base on pass-through exception
feat(pass_through_endpoints.py): support returning api-base on pass-through endpoints
fix mcp router
cleanup
remove stale file
remove stale file
add mcp routes
litellm mcp routes
init global_mcp_tool_registry
example mcp tools
add mcp tool registry
load load_tools_from_config
add MCPToolRegistry
mock langchain MCP interface
mock config for MCP tools
sample mcp server
docs(release_cycle.md): clarify release cycle for stable releases on docs
test_prepare_fake_stream_request
fix: fix testing
build(deps): bump litellm in /cookbook/litellm-ollama-docker-image
fix code quality checks
supports_native_streaming
fix(main.py): fix OR import
supports_native_streaming
test_openai_o1_pro_response_api_streaming
MockResponsesAPIStreamingIterator
transform_responses_api_request
MockResponsesAPIStreamingIterator
add AsyncMockResponsesAPIStreamingIterator
add fake_stream to llm http handler
add should_fake_stream
bump to openai==1.67.0
test_openai_o1_pro_response_api
test_openai_o1_pro_incomplete_response
bump to openai==1.67.0
docs(response_api.md): update docs to use new o1-pro model example
fix(types/utils.py): support openai 'file' message type
feat(azure/gpt_transformation.py): add azure audio model support
fix import hashlib
get_chat_completion_prompt
docs custom prompt management
docs custom prompt management
docs custom prompt management
build(model_prices_and_context_window.json): fix native streaming flag
build(model_prices_and_context_window.json): add o1-pro pricing
fix(internal_user_endpoints.py): re-introduce upsert on user not found
Revert "Fix latency redis"
Handle empty valid_deployments in LowestLatencyLoggingHandler
docs custom prompt management
TestCustomPromptManagement
get_custom_logger_for_prompt_management
CustomPromptManagement
get_custom_loggers_for_type
example X42PromptManagement
define CustomPromptManagement
docs prompt mgtm
fix: remove unused import
docs prompt management
remove linter changes to match old code style
custom Prompt Management - Overview
feat(prisma_client.py): initial commit add prisma migration support to proxy
docs(config_settings.md): cleanup
test(test_proxy_server.py): make test work on ci/cd
docs(config_settings.md): update env var information
test(test_proxy_server.py): add unit test to ensure get credentials only called behind feature flag
fix(proxy_server.py): move db credential check to only run behind feature flag
docs(litellm_proxy): update parameter assignment for streaming call
docs(litellm_proxy): correct parameter assignment in sample litellm proxy call docs
docs(enterprise.md): clarify sla's
sync_latest_changes
set_local_icons
feat: Add support for custom OPENROUTER_API_BASE via get_secret in completion function
fix: VertexAI outputDimensionality configuration
fix(common_utils.py): handle cris only model
test_arize_dynamic_params
test_arize_dynamic_params
fix(base_routing_strategy.py): refactor for cleaner code
refactor(base_routing_strategy.py): fix function names
build(model_prices_and_context_window.json): fix azure gpt-4o pricing
fix code quality checks
fix(base_routing_strategy.py): fix base to handle no running event loop
_arize_otel_logger
test_arize_callback
fix(test_base_routing_strategy.py): add unit testing for new base routing strategy test
feat(base_routing_strategy.py): handle updating in-memory keys
fix(lowest_tpm_rpm_v2.py): support batch writing increments to redis
test_arize_callback
test arize logging
get_openai_client_cache_key
test_openai_client_reuse
test openai common utils
fix common utils
use common caching logic for openai/azure clients
use common logic for re-using openai clients
fix(response_metadata.py): log the litellm_model_name
test(test_tpm_rpm_routing_v2.py): initial test, for asserting async pre call check works as expected
fix ensure async client test
test_dynamic_azure_params
fix(litellm_logging.py): fix update
fix(lowest_tpm_rpm_v2.py): fix updating limits
fix code qa
fix arize config
fix(lowest_tpm_rpm_routing_v2.py): fix deployment update to use correct keys
fix(lowest_tpm_rpm_v2.py): update key to use model name
refactor create_litellm_proxy_request_started_spen
fix arize logging
fix - Arize - only log LLM I/O
test_async_dynamic_arize_config
fix _get_span_processor
use _get_headers_dictionary
add arize_api_key to StandardCallbackDynamicParams
test_arize_set_attributes
use safe dumps for arize ai
Union[TranscriptionResponse, Coroutine[Any, Any, TranscriptionResponse]]:
fix type errors on transcription azure
fix code quality
Fix TTFT prioritization for streaming in LowestLatencyLoggingHandler
test_azure_instruct
fix test_ensure_initialize_azure_sdk_client_always_used
fix azure chat logic
test_azure_embedding_max_retries_0
fix typing errors
initialize_azure_sdk_client
:test_completion_azure_ad_toke
test: refactor testing to handle routing correctly
fix azure embedding test
ui new build
ui fix linting error
fix amebedding issue on ssl azure
fix linting error
test(test_get_llm_provider.py): cover scenario where xai not in model  name
fix common utils
fix(get_llm_provider.py): Fixes https://github.com/BerriAI/litellm/issues/9291
fix using azure openai clients
test(test_get_llm_provider.py): Minimal repro for https://github.com/BerriAI/litellm/issues/9291
use get_azure_openai_client
fix logic for intializing openai clients
use ssl on initialize_azure_sdk_client
_init_azure_client_for_cloudflare_ai_gateway
fix re-using azure openai client
fix - correctly re-use azure openai client
_get_azure_openai_client
rename to _get_azure_openai_client
build(pyproject.toml): add boto3 to proxy deps on pyproject
docs - update release notes with known issue
docs release notes Azure OpenAI
bump: version 1.63.11 → 1.63.12
handle _get_async_http_client for OpenAI
Update test_arize_ai.py
restore previous formatting
Fix test and add comments
revert space_key change and add tests for arize integration
test: update testing
fix ProxyUpdateSpend
fix linting error
show sso settings on ui
ui polish order of settings
fix settings endpoints code qa
Update sidebars.js w/ phoenix observability
fix is_internal_user_role
expose flag to disable_spend_updates
Fix wrong import and use space_id instead of space_key for Arize integration
fix linting error
unit test reset budget job
use jsonify_team_object for updating teams
test_reset_budget_job
fix _reset_budget_for_team
fix _reset_budget_for_key/team/user
fix(ollama/completions/transformation.py): pass prompt, untemplated on `/completions` request
update_internal_user_settings
test(test_assemble_streaming_responses.py): update test to use correct type
fix DefaultInternalUserParams
DefaultInternalUserParams
fix(streaming_handler.py): support logging complete streaming response on cache hit
fix get_sso_settings
is_internal_user_role
DefaultInternalUserParams
fix internal user sso settings
ui fix rendering budget duration settings on ui
fix(streaming_handler.py): emit deep copy of completed chunk
fix redis serialization issue with Redis + lowest latency strategy
update internal user settings on ui
Update test_sagemaker.py to match changed parameters
Update handler.py to use prepared_request.body for input
update_internal_user_settings
fix display of settings
docs(config_settings.md): document new env var
fix(http_handler.py): fix typing error
UISSOSettings
Update handler.py to use prepared_request.body
ui sso settings
ui networking getSSOSettingsCall
model_dump()
add types for internal user settings
fix(http_handler.py): support reading ssl security level from env var
docs(vertexai): fix typo in required env variables
feat(cost_calculator.py): support reading litellm response cost header in client sdk
add UISSOSettings
fix(redis_cache.py): add 5s default timeout
Update prompt_caching.md to fix typo
test: loosen test
docs(bedrock.md): add aws bedrock runtime endpoint to docs
test: make test less flaky
fix(logging_utils.py): revert change
fix(streaming_handler.py): raise stop iteration post-finish reason
fix(litellm_logging.py): remove unused import
fix(streaming_handler.py): pass complete streaming response on completion
fix(streaming_handler.py): return model response on finished chunk
Update perplexity.md
feat: make masterkey secret configurable
docs docker run cmd
fix docker img deploy - deploy stable releases from main-stable
fix ghcr build
fix ghcr deploy
fix(converse_transformation.py): fix linting error
push changes for llm credential
fix link
docs ui credentials
clean up
update release notes
test(test_bedrock_completion.py): ensure model id in model name just works
fix(converse_transformation.py): fix encoding model
fix stable release docs
fix docker img tag displayed on stable releases
commit restructure
release notes
docs release notes
ui show logs page
docs Stop storing Spend Logs in DB
fix role based access controls doc
Multiple OIDC providers
docs JWT
docs token auth
update doc
docs prompt caching
docs reasoning content open router
fix img
docs stable release
refactor(litellm_logging.py): delegate returning a complete response to the streaming_handler
docs ui improvements
test(test_streaming_handler.py): add unit test to ensure model response stream with usage is always used
test(test_streaming_handler.py): assert chunk is non-empty when usage block given
security section
fix(utils.py): Prevents final chunk w/ usage from being ignored
docs aws k/v pairs
ui fixes
docs link to openweb ui correctly
stash changes
docs stash changes
fix order of fixes
docs - working management ui
bump: version 1.63.10 → 1.63.11
ui new build
add tool tip for foundry
fix example for azure foundry
ui new build
docs azure ai foundry prompt
ui new build
fix(factory.py): reduce ollama pt LOC < 50
reanme toModelConnectionTest
fix code quality
undo changes to route llm request
fix test connection
fix(team_endpoints.py): fix linting error
fix(key_edit_view.tsx): fix showing available models on key edit for non-team key
fix(config.yml): fix ci/cd config.yml
fix: fix test
explain litellm mode
fix(create_key_button.tsx): retrieve all available team models on team select
feat(proxy_server.py): support retrieving models for a team, if user is a member - via `/models?team_id`
fix params to test connection
fix mode
testConnectionRequest
test_model_connection
build: new ui build
bump: version 1.63.9 → 1.63.10
fix(top_key_view.tsx): fix bar chart to use key alias
refactor(transform_request/): remove experimental transform request tab from ui
fix(model_info_view.tsx): call db for model info
fix dup close
workign run test connection many times
fix Test Modes
fix endpoint_data
decent test connection
cleaner view
feat(team_endpoints.py): unfurl 'all-proxy-models' on team info endpoints
fix dont show extra modal
delete bloat file
working showing user the raw request / response
fix mode
docs(routing.md): cleanup docs
fix(router.py): add new test
fix: fix linting error
feat(ui/): allow admin to reuse existing model credentials
working test connection
feat(endpoints.py): support adding credentials by model id
ui add health/test_connection
fix endpoint
use prepareModelAddRequest
feat(endpoints.py): enable retrieving existing credentials by model name
est Model Connectio
docs working test error display
Docs: Update configs.md
add health/test_connection
Add test connection
fix route llm request to allow non-router models
Update test_ollama_pt
Add new line
Support 'system' role ollama
fix(endpoints.py): update credentials should update before storing
Add unit test
style: cleanup credential from leftnav - now in models tab
backend instant delete model
instant update delete model flow
Map total tokens to prompt_tokens too
models - instant update ui on editing values
fix ui
ui - instantly show new ui keys
ui - instantly show changes to create key table
docs: Add centralized credential management docs
docs(snowflake.md): cleanup docs
bump: version 1.63.8 → 1.63.9
doc fix snowflake
ui new build
ui new build
fix ui linting
fix code quality check
exception_type
Key ID
ci(config.yml): bump websocket version on ci/cd
docs(contributing_code.md): update docs
docs(contributing_code.md): update contribution docs with examples of running linting tests
build(makefile): add mypy linting to makefile
ui fix top key view
fix top key view
working button toggle
use table view
fix: fix linting errors
feat(converse_transformation.py): fix type for bedrock cache usage block
working modal view
fix: fix linting errors
test: fix tests
working modal
feat(model_management_endpoints.py): support audit logs on `/model/add` and `/model/update` endpoints
allow clicking into keys
fix api_key
keyInfoV1Call
feat: fix linting errors
working hover
feat(model_management_endpoints.py): emit audit logs on model delete
feat(internal_user_endpoints.py): add audit logs on /user/update
response api fix typo
fix @pytest.mark.skip(reason="lakera deprecated their v1 endpoint.")
fix linting errors
feat(internal_user_endpoints.py): emit audit log on `/user/new` event
Add stubbed routes to pass initial auth tests
Removed unnecessary code and refactored
fix(invoke_handler.py): support cache token tracking on converse streaming
test_bad_request_bad_param_error
Add exception mapping for responses API
test_async_bad_request_bad_param_error
test(tests/litellm): add unit test for transform usage function
feat(converse_transformation.py): translate converse usage block with cache creation values to openai format
fix exception_type
fix(aim.py): fix linting error
add basic validation tests for e2e responses create endpoint
test: patch test to avoid lakera changes to sensitivity
working e2e tests for responses api
rename folder to test openai endpoints
fix auth add responses API to llm routes
rename test config
fix(azure.py): track azure llm api latency metric
fix: bump websockets version on ci/cd
fix: fix linting error
mark /responses [Beta]
fix: fix learnlm test
add healthcheck
add healthcheck
updates docs
ruff format
Lock poetry after rebase
CR
Support post-call guards for stream and non-stream responses
docs(reasoning_content.md): clarify 'thinking' param support is from v1.63.0+
add test code
fix(model_dashboard.tsx): move llm credentials closer to add model tab
(gemini)Handle HTTP 201 status code in Vertex AI response
fix: fix linting error
fix(add_credentials_tab.tsx): cleanup ui
bump: version 1.63.7 → 1.63.8
feat(credentials/): working e2e flow for add / update models on LiteLLM UI
docs response api
gemini updates: gemma 3, flash 2 thinking update, learnlm
chore(init): update Azure default API version to 2025-02-01-preview
docs naming on sidebar
fix(columns.tsx): add llm credentials column on model table
feat(add_model_tab.tsx): working e2e add model flow on UI
responses_api
chore(init): update Azure default API version to 2024-12-01-preview
feat: refactor add models tab on UI to enable setting credentials
test_initialize_router_endpoints
_update_kwargs_with_default_litellm_params
test_initialize_router_endpoints
test_generic_api_call_with_fallbacks_basic
_update_kwargs_with_default_litellm_params
feat(credential_accessor.py): fix upserting new credentials via accessor
get_custom_headers
test_openai_responses_litellm_router_with_metadata
test_ensure_initialize_azure_sdk_client_always_used
fix response api handling
fix async_moderation_hook
feat(credentials.tsx): working POST request to add new credentials to db
fix linting
fix linting error
feat(add_credentials_tab.tsx): style improvements
_update_kwargs_with_default_litellm_params
feat(credentials.tsx): add modal for adding new credentials
LITELLM_METADATA_ROUTES
test_openai_responses_litellm_router_no_metadata
feat: re-add llm credentials to models page
_handle_llm_api_exception
feat(time_to_first_token.tsx): make viewing ttft free
feat(model_dashboard.tsx): re-add model analytics
fix responses_api
working spend tracking + logging for response api
feat: initial commit adding support for credentials on proxy ui
working responses_api
use correct get custom headers
add responses_api
use ProxyBaseLLMRequestProcessing
test: fix test
fix: skip datazone models
build(model_prices_and_context_window.json): add data zone pricing
build(model_prices_and_context_window.json): add gpt-4o-mini-realtime-preview azure model cost
build(model_prices_and_context_window.json): add o1 data zone pricing
test_openai_responses_litellm_router
build(model_prices_and_context_window.json): add azure o1 eu/us pricing
build(model_prices_and_context_window.json): add azure eu/us data zone pricing
fix: fix method signature in test
factory_function
_generic_api_call_with_fallbacks
refactor: update method signature
fix(azure_ai/transformation.py): support passing api version to azure ai services endpoint
build(pyproject.toml): bump openai version for responses types
"openai==1.66.1" in testing
test_validate_environment
TestResponseAPILoggingUtils
TestResponsesAPIRequestUtils
fix(llm_caching_handler.py): handle no current event loop error
Optional[Dict]
fix(llm_caching_handler.py): Add event loop to llm client cache info
fix mypy linting errors
fix(pass_through_endpoints.py): fix linting error
typing_extensions Annotated
pip install openai==1.66.1
remove infinit loop for streaming
bump to openai==1.66.1
llm_responses_api_testing
bump to 1.66.1
fix code quality checks
openai 1.66.1
BaseLiteLLMOpenAIResponseObject
test openai responses streaming
fix ResponseAPILoggingUtils
Fix Metadata not updating in UI
test_basic_openai_responses_api
_handle_logging_completed_response
explictly pass params to partial func
ResponsesAPIRequestUtils
STREAM_SSE_DONE_STRING
re-use base_llm_http_handler
add aresponses
add SyncResponsesAPIStreamingIterator
add conftest
Mark Cohere Embedding 3 models as Multimodal
working streaming logging + cost tracking
_get_assembled_streaming_response
revert to older logging implementation
working streaming logging
ResponsesAPIStreamEvents
docs(bedrock.md): add amazon nova to docs
add debug logging
Bump @babel/helpers from 7.26.0 to 7.26.10 in /docs/my-website
Bump @babel/runtime-corejs3 from 7.26.0 to 7.26.10 in /docs/my-website
bump: version 1.63.6 → 1.63.7
fix(base_image_gen_test.py): weaken assertion, working locally failing on ci/cd
validate_responses_match
log input of response API
_transform_response_api_usage_to_chat_usage
fix(azure/audio_transcriptions.py): support azure cost tracking
fix order of imports
fix(invoke_handler.py): fix converse chunk parsing to only return empty dict on tool use
fix typing for aresponses
add responses api to call types
Response API cost tracking
fix(audio_transcriptions.py): fix setting client
fix(azure_ai/): fix transformation to handle when models don't support tool_choice
fix(o_series_handler.py): handle async calls
test: update test for correct aws region
test: fix test - delete env var before running
fix(internal_user_endpoints.py): allow internal user to query their own info, without knowing their id
add async streaming support
ResponsesAPIStreamingIterator
working ResponsesAPIStreamingIterator
test: skip flaky tests
Update model_prices_and_context_window_backup.json
changed to chat/transformations
changed completion to chat for modes
test: fix test
fix: fix linting errors
fix(handler.py): same as last commit
fix(azure/completions): migrate completions endpoint to support base azure llm class
fix: fix merge conflicts
fix: fix max parallel requests client
fix: fix linting error
fix: remove unused imports
test: fix test
fix: fix linting error
feat(azure.py): fix azure client init
test: remove redundant tests
fix: consistent usage of http_client across azure client init
ResponsesAPIStreamingResponse
test: fix tests
working basic openai response api request
ResponsesAPIOptionalRequestParams
working test_basic_openai_responses_api
refactor(azure.py): refactor to have client init work across all endpoints
transform_response_api_response
working async_response_api_handler
add transform_response_api_response
add ResponsesAPIResponse
add transform_request for OpenAI responses API
BaseResponsesAPIConfig
add validate_environment, get_complete_url
get_optional_params_responses_api
add BaseResponsesAPIConfig
working transform
add OpenAIResponsesAPIConfig
add OpenAIResponsesAPIConfig
refactor(batches/main.py): working refactored azure client init on batches
add aysnc aresponses
working import litellm.responses
openai reasoning initial types
refactor(azure/audio_transcriptions.py): support client init with common logic
refactor(azure.py): working client init logic in azure image generation
refactor(azure.py): working azure client init on audio speech endpoint
refactor(azure.py): refactor acompletion to use base azure sdk client
new test for llm_responses_api_coverage
added mock_tests
updated snowflake docs
add postgres-volumes comment
fix gemini/gemini-2.0-pro-exp-02-05 on model cost map
test(test_azure_common_utils.py): add unit testing for common azure client params function
ui new build
refactor(azure/common_utils.py): refactor azure client param logic
build(model_prices_and_context_window.json): add gemini/gemini-2.0-pro-exp pricing
fix viewing page as internal user viewer
only show create key / actions for users with write access
don't show test key page to internal user
fix(client_initialization_utils.py): refactor azure client init logic
fix encoding in tests
test: skip redundant test
oops
Feedback
Bing Search Pass Thru
fix if
fix: Handle ManagedIdentityCredential in Azure AD token provider
removed hardcoding and added models to model_prices
removed handler and refactored to deepseek/chat format
test: fix tests
test: fix test
test: add direct test - fix code qa check
fix: fix linting error
fix: fix test
fix: fix linting error
fix: remove client init tests for router - dup behaviour - provider caching already exists
bump: version 1.63.5 → 1.63.6
ui new build
get_complete_url
update test
add test
encode bedrock model id
cleanup
feat(credential_endpoints/endpoints.py): don't return credentials on get
test original file instead of the backup
fix(transformation.py): fix linting error
test string checked for model access control
can_team_access_model
test: update testing - having removed the router client init logic
update backup json as well so test passes
fix indentation
remove incorrect `moderations` mode
feat: working e2e credential management - support reusing existing credentials
move test file to correct location
sync model db copy
docs(pr-template): update unit test command in checklist
Add azure_ai/mistral-nemo
Add Azure DeepSeek V3 / Update DeepSeek R1 pricing
test_can_team_access_model fix
generate_key
Adding/Updating Azure Phi-4 model variants Note: Azure has not yet published any cost for the newer variants (mini and multimodal) There is conflicting information about context size and model capabilities in different documents.
feat: complete crud endpoints for credential management on proxy
Navbar logout url
getProxyUISettings
use helper setProxySettings
Adding vertex_ai/imagen-3.0-generate-002
fix: fix type
fix(endpoints.py): encrypt credentials before storing in db
feat(endpoints.py): support writing credentials to db
feat(endpoints.py): initial set of crud endpoints for reusable credentials on proxy
feat(credential_accessor.py): support loading in credentials from credential_list
docs contributing instructions
feat(proxy_server.py): move credential list to being a top-level param
fix(router.py): comment out azure/openai client init - not necessary
Pre-Submission checklist
fix(base_invoke_transformation.py): support extra_headers on bedrock invoke route
feat(azure.py): add azure bad request error support
Pre-Submission checklist
docs contributing
docs contributing to litellm
feat(proxy_server.py): check code before defaulting to status code
feat(openai.py): bubble all error information back to client
refactor: instrument body param to bubble up on exception
removed supported models from docs
Update model_prices_and_context_window.json
feedback
docs on contributing
Revert "changes"
add bedrock deepseek r1 model pricing
docs contributing section
fix linting error
Revert "ui new build"
ui new build
ui linting fix
ui linting fixes #2
ui fix linting errors
fix linting error
ui linting fix
ui linting fixes #2
ui fix linting errors
fix linting error
show eu api base on openai + text
OpenAI_Text
fix atext_completion
openweb tutorial
add support for Amazon Nova Canvas model (#7838)
bump: version 1.63.4 → 1.63.5
fix(utils.py): fix linting error
Support openrouter `reasoning_content` on streaming  (#9094)
fix: perplexity return both delta and message cause OpenWebUI repect text (#9081)
test: fix test
feat: prioritize api_key over tenant_id for more Azure AD token provi… (#8701)
fix: make type object subscriptable
Litellm dev 03 08 2025 p3 (#9089)
docs(index.md): add git diff to release notes
docs: add images to release notes
Revert "experimental - track anthropic messages as mode"
experimental - track anthropic messages as mode
fix test_list_key_helper_team_filtering
docs: cleanup
docs(index.md): cleanup
docs(index.md): cleanup
test_cost_azure_openai_prompt_caching
New stable release notes  (#9085)
(UI) - Fix show correct count of internal user keys on Users Page (#9082)
(Clean up) - Allow switching off storing Error Logs in DB (#9084)
(UI) - Fix, Allow Filter Keys by Team Alias, Key Alias and Org (#9083)
Fix batches api cost tracking + Log batch models in spend logs / standard logging payload (#9077)
support bytes.IO for audio transcription (#9071)
(UI) - Minor improvements to logs page (#9076)
Bug fix - String data: stripped from entire content in streamed Gemini responses (#9070)
fix(transform_request.tsx): allow dev to file translation error
UI - new API Playground for testing LiteLLM translation (#9073)
(UI) - Allow adding Cerebras, Sambanova, Perplexity, Fireworks, Openrouter, TogetherAI Models on Admin UI  (#9069)
(Docs) OpenWeb x LiteLLM Docker compose + Instructions on spend tracking + logging  (#9059)
fix logs page selected log (#9061)
ci: add helm unittest
(UI) - Keys Page - Show 100 Keys Per Page, Use full height, increase width of key alias (#9064)
docs: update docs
Mark several Claude models as being able to accept PDF inputs (#9054)
docs: Add project page for pgai (#8576)
(Feat)  - add pricing for eu.amazon.nova models (#9056)
docs(release_cycle.md): add stable tag info to release cycle
docs: add doc for rotating master key
bump: version 1.63.3 → 1.63.4
Support master key rotations  (#9041)
feat(handle_jwt.py): support multiple jwt url's
build(model_prices_and_context_window.json): update azure o1 mini pricing (#9046)
fix(team_endpoints.py): ensure 404 raised when team not found (#9038)
bump: version 1.63.2 → 1.63.3
ui new build
ui allow ui or eu api base adding model (#9042)
_create_redis_cache
Added PDL project (#8925)
(Docs) connect litellm to open web ui (#9040)
(AWS Secret Manager) - Using K/V pairs in 1 AWS Secret (#9039)
[Feat] - Display `thinking` tokens on OpenWebUI (Bedrock, Anthropic, Deepseek) (#9029)
Fix: Create RedisClusterCache when startup nodes provided in cache args of router (#9010)
docs(sidebars.js): add supported_endpoints slug
fix
test_e2e_batches_files
pricing
test fix await_batch_completion
test - remove anthropic_adapter tests. no longer used
test fix anthopic completion
bump: version 1.63.1 → 1.63.2
(Refactor) `/v1/messages` to follow simpler logic for Anthropic API spec (#9013)
docs: cleanup
docs: cleanup docs
docs: cleanup doc
docs: cleanup 'signature_delta' from docs
docs(index.md): add release note for anthropic update
bump: version 1.63.0 → 1.63.1
Litellm dev 03 05 2025 p3 (#9023)
Support `format` param for specifying image type  (#9019)
bump: version 1.62.4 → 1.63.0
Return `signature` on anthropic streaming + migrate to `signature` field instead of `signature_delta` [MINOR bump] (#9021)
added Snowflake config to ProviderConfigManager
build(deps): bump jinja2 from 3.1.4 to 3.1.6
Fix #7629 - Add tzdata package to Dockerfile (#8915) (#9009)
bump: version 1.62.4 → 1.62.5
fix: fix linting error
Revert "(UI) - Security Improvement, move to JWT Auth for Admin UI Sessions (#8995)"
docs fix router default settings
fix: fix linting error
fix(base_aws_llm.py): remove region name before sending in args (#8998)
fix: fix linting error
bump: version 1.62.3 → 1.62.4
Litellm dev 03 04 2025 p3 (#8997)
(UI) - Security Improvement, move to JWT Auth for Admin UI Sessions (#8995)
(bug fix) - Fix Cache Health Check for Redis when redis_version is float (#8979)
fix(common_utils.py): handle $id in response schema when calling vert… (#8991)
Support caching on reasoning content + other fixes (#8973)
added documentation for snowflake
Revert "(UI) - Improvements to session handling logic  (#8970)"
Revert "ui new build"
docs(data_security.md): cleanup docs
docs(data_security.md): update docs
fix: fix linting errors
Litellm dev 03 01 2025 p2 (#8944)
build: merge litellm_dev_03_01_2025_p2
bump: version 1.62.2 → 1.62.3
fix(route_llm_request.py): move to using common router, even for clie… (#8966)
ui new build
(UI) - Improvements to session handling logic  (#8970)
(UI) Fix session handling with cookies (#8969)
Removed unnecessary comments
Added models to model_prices_and_context
wrote tests for snowflake
bump: version 1.62.1 → 1.62.2
Fix deepseek 'reasoning_content' error  (#8963)
test: fix test
changes
[CHORE] Removed old code
[CHORE] Added proper typing
[CHORE] Fixed some style issues and leaks
build(model_prices_and_context_window.json): add new tracking for anthropic via invoke - clarify it does not support pdf input unlike converse
[FEAT] Added snowflake completion provider
quote DailyTagSpend in order to look for the right View (#8947)
build: Add Makefile for LiteLLM project with test targets
fix(proxy_server.py): fix setting router redis cache, if cache enable… (#8859)
fix(anthropic/chat/transformation.py): fix headers to be a set
Fix prometheus metrics w/ custom metrics + Handle sending `image_url` as str to openai  (#8935)
build: merge branch
Add `supports_pdf_input: true` for specific Bedrock Claude models (#8655)
ui new build
test_can_team_access_model
(UI) - Allow Internal Users to View their own logs (#8933)
(UI) Error Logs improvements - Store Raw proxy server request for success and failure (#8917)
(Bug fix) - don't log messages in `model_parameters` in StandardLoggingPayload (#8932)
Revert "(bug fix) - don't log messages, prompt, input in `model_parameters`  in StandardLoggingPayload (#8923)"
docs: fix dates
docs(index.md): add demo instance to docs for easy testing (#8931)
(bug fix) - dd tracer, only send traces when user opts into sending dd-trace (#8928)
Litellm stable release notes v1 61 20 (#8929)
(bug fix) - don't log messages, prompt, input in `model_parameters`  in StandardLoggingPayload (#8923)
ui remove search button on internal users tab that does not (#8926)
patch - auth checks for model access (#8924)
fix overly verbose non blocking error on dd get_request_response_payload
Update README.md
UI - Allow admin to control default model access for internal users (#8912)
bump: version 1.62.0 → 1.62.1
docs(release_cycle.md): document release cycle
test_chat_completion_bad_model_with_spend_logs
ui new build
(Feat) - Show Error Logs on LiteLLM UI  (#8904)
Fix bedrock passing `response_format: {"type": "text"}` (#8900)
converse_transformation: pass 'description' if set in response_format (#8907)
Fix calling claude via invoke route + response_format support for claude on invoke route (#8908)
Litellm dev 02 27 2025 p6 (#8891)
fix test_moderations_bad_model
Removed prints and added unit tests
fix: ollama chat async stream error propagation (#8870)
Show 'user_email' on key table on UI  (#8887)
(security fix) - Enforce model access restrictions on Azure OpenAI route (#8888)
move test openai error handling tests
Install Node.js
move test openai error handling
Update model settings data (#8871)
update sambanova docs (#8875)
Update model path and documentation for cerebras API call (#8862)
bump: version 1.61.20 → 1.62.0
ui new build
(Improvements) use `/openai/` pass through with OpenAI Ruby for Assistants API (#8884)
(Proxy improvement) - Raise `BadRequestError` when unknown model passed in request (#8886)
Add `created_by` and `updated_by` fields to Keys table (#8885)
Allow team/org filters to be searchable on the Create Key Page (#8881)
(fix) Pass through spend tracking - ensure `custom_llm_provider` is tracked for Vertex, Google AI Studio, Anthropic (#8882)
(Bug Fix) - Accurate token counting for `/anthropic/` API Routes on LiteLLM Proxy  (#8880)
(fix) Anthropic pass through cost tracking (#8874)
Add new gpt-4.5-preview model + other updates (#8879)
docs(bedrock.md): cleanup doc
fix(sagemaker/completion/handler.py): fix typo
docs(reasoning_content.md): add more info to docs
docs: add reasoning content to docs
fix(main.py): pass 'thinking' param on async completion call
bump: version 1.61.19 → 1.61.20
vertex ai anthropic thinking param support (#8853)
bump: version 1.61.18 → 1.61.19
bump: version 1.61.17 → 1.61.18
[Bug]: Deepseek error on proxy after upgrading to 1.61.13-stable (#8860)
fix caching on main branch (#8858)
fix load tests on litellm release notes
(UI + Backend) Fix Adding Azure, Azure AI Studio models on LiteLLM (#8856)
fix(converse_transformation.py): fix 'thinking' param check for claude-3-7 on bedrock
feat: enhance migrations job with additional configurable properties (#8636)
Mark Claude Haiku 3.5 as vision-capable (#8840)
Litellm dev bedrock anthropic 3 7 v2 (#8843)
chore: set ttlSecondsAfterFinished on the migration job in the litellm-helm chart (#8593)
fix(o_series_transformation.py): fix optional param check for o-serie… (#8787)
fix(get_litellm_params.py): handle no-log being passed in via kwargs (#8830)
build: merge squashed commit
test_prompt_caching
test_prompt_caching
test_aprompt_caching
test_prompt_caching
ui new build
(UI) - Create Key flow for existing users (#8844)
Added tags to additional keys that can be sent to athina
Adding openrouter claude-3.7-sonnet (#8826)
fix pass through tests vertex
fix test internal users
fix ui pagination users tab
ui new build
test fix deprecated gemini-1.0-pro on vertex
(Bug fix) dd-trace used by default on litellm proxy (#8817)
(Bug fix) - allow using Assistants GET, DELETE on `/openai` pass through routes (#8818)
fix _get_docker_run_command_stable_release
(Bug Fix) Using LiteLLM Python SDK with model=`litellm_proxy/` for embedding, image_generation, transcription, speech, rerank (#8815)
Adding Azure Phi-4 (#8808)
expected_result
add hypercorn dep to circle ci
Litellm dev 02 25 2025 p1 (#8816)
(Bug fix) - running litellm proxy on wndows (#8735)
(Bug fix) - reading /parsing request body when on hypercorn (#8734)
(Router) - If `allowed_fails` or `allowed_fail_policy` set, use that for single deployment cooldown logic (#8668)
test_can_team_access_model
can_team_access_model
can_team_access_model
fix vertex_ai claude 3.7 naming (#8807)
fix(UI): improve model name dispaly in model hub cards (#8749)
Icons on navbar profile dropdown (#8792)
test_user_email_metrics
test_update_user_unit_test
Adding Azure Phi-4
docs(anthropic.md): add claude-3-7-sonnet support
bump: version 1.61.16 → 1.61.17
ui new build
fix adding teams / orgs on litellm ui (#8776)
(UI) Allow adding MSFT SSO on UI  (#8779)
Litellm contributor prs 02 24 2025 (#8781)
(UI) Fixes for managing Internal Users (#8786)
bump: version 1.61.15 → 1.61.16
test: handle index error
Add anthropic thinking + reasoning content support (#8778)
Litellm contributor prs 02 24 2025 (#8775)
fix incorrect variable name in reliability section of docs (#8753)
Add anthropic3-7-sonnet (#8766)
fix: remove aws params from bedrock embedding request body (#8618) (#8696)
Update model_prices_and_context_window.json
fix missing comma
feat/postgres-volumes
fix(proxy/_types.py): fixes issue where internal user able to escalat… (#8740)
Add cohere v2/rerank support (#8421) (#8605)
fix(amazon_deepseek_transformation.py): remove </think> from stream o… (#8717)
Support arize phoenix on litellm proxy (#7756) (#8715)
bump: version 1.61.14 → 1.61.15
ui new build
fix model datatable
(UI) Edit Model flow improvements (#8729)
(Feat) - UI, Allow sorting models by Created_At and all other columns on the UI (#8725)
Correct spelling in user_management_heirarchy.md (#8716)
bump: version 1.61.13 → 1.61.14
fix: remove unused import
test: fix test
add bedrock llama vision support + cohere / infinity rerank - 'return_documents' support  (#8684)
Add cost tracking for rerank via bedrock (#8691)
bump: version 1.61.12 → 1.61.13
delete deprecated code test
ui new build
(Infra/DB) - Allow running older litellm version when out of sync with current state of DB  (#8695)
(Observability) - Add more detailed dd tracing on Proxy Auth, Bedrock Auth (#8693)
(Redis fix) - use mget_non_atomic  (#8682)
(Bug fix) - Cache Health not working when configured with prometheus service logger (#8687)
bump: version 1.61.11 → 1.61.12
fix(utils.py): handle token counter error when invalid message passed in  (#8670)
LiteLLM Contributor PRs (02/18/2025).  (#8643)
Add all `/key/generate` api params to UI + add metadata fields on team AND org add/update (#8667)
extract `<think>..</think>` block for amazon deepseek r1 and put in `reasoning_content`  (#8664)
build: extract <think>..</think> block for amazon deepseek r1 and put in reasoning_content
bump: version 1.61.10 → 1.61.11
ui new build
(Bug fix) prometheus - safely set latency metrics  (#8669)
(Bug Fix Redis) - Fix running redis.mget operations with `None` Keys (#8666)
(UI + Proxy) Cache Health Check Page - Cleanup/Improvements (#8665)
fix(spend_tracking_utils.py): move info to debug
bump: version 1.61.9 → 1.61.10
build: build ui (#8654)
fix(team_endpoints.py): allow team member to view team info (#8644)
test_message_with_name
fix code quality
bump: version 1.61.8 → 1.61.9
ui new build
ui QA fix, adding wildcard models
ui linting fix
patch on LiteLLM_AuditLogs
(Polish/Fixes) - Fixes for Adding Team Specific Models  (#8645)
Litellm dev 02 18 2025 p3 (#8640)
Litellm dev 02 18 2025 p2 (#8639)
test: handle unstable tests
Add OSS license check to ci/cd  (#8626)
Add Elroy to projects built with litellm (#8642)
Litellm dev 02 18 2025 p1 (#8630)
[Feature]: Redis Caching - Allow setting a namespace for redis cache (#8624)
fix(model_cost_map): fix json parse error on model cost map + add unit test (#8629)
feat: add oss license check for related packages (#8623)
add openrouter/google/gemini-2.0-flash-001 (#8619)
(Fix) Redis async context usage for Redis Cluster + 94% lower median latency when using Redis Cluster   (#8622)
Pass router tags in request headers - `x-litellm-tags`  (#8609)
build: merge commit 1b15568af7f8923a014e6e49f5ec09e5abf5f981 Author: Krrish Dholakia <krrishdholakia@gmail.com> Date:   Mon Feb 17 21:37:36 2025 -0800
ci/cd run again
test_openai_fine_tuning
test_openai_fine_tuning
test_openai_fine_tuning
bump: version 1.61.7 → 1.61.8
ui new build
fix linting
(UI) Improvements to Add Team Model Flow (#8603)
(UI) Refactor Add Models for Specific Teams (#8592)
(UI) Allow adding models for a Team (#8598) (#8601)
bump: version 1.61.6 → 1.61.7
build: build ui
Litellm stable UI 02 17 2025 p1 (#8599)
docs(routing.md): add section on weighted deployments
Revert "(UI) Allow adding models for a Team (#8598)" (#8600)
feat(ui): alert when adding model without STORE_MODEL_IN_DB (#8591)
(UI) Allow adding models for a Team (#8598)
Fix typo in main readme (#8574)
fix: update README.md API key and model example typos (#8590)
docs: update litellm user management heirarchy doc
docs(request_headers.md): document openai org id header handling in request_headers.md
test: fix test
bump: version 1.61.5 → 1.61.6
refactor(teams.tsx): refactor to display all teams, across all orgs (#8565)
test_openai_fine_tuning
test_openai_fine_tuning
ui new build
fix edit key
test_list_key_helper_team_filtering
ui new build
ui fix tsx linting
bump: version 1.61.4 → 1.61.5
(Patch/bug fix) - UI, filter out litellm ui session tokens on Virtual Keys Page (#8568)
(Bug Fix + Better Observability) - BudgetResetJob:  (#8562)
(Bug Fix) - Add Regenerate Key on Virtual Keys Tab (#8567)
Enable update/delete org members on UI  (#8560)
Add remaining org CRUD endpoints + support deleting orgs on UI  (#8561)
cleanup_azure_files
Revert "test fix use mock endpoints for e2e files and ft tests"
test fix use mock endpoints for e2e files and ft tests
(Feat) - return `x-litellm-attempted-fallbacks` in responses from litellm proxy  (#8558)
fix(main.py): fix key leak error when unknown provider given (#8556)
Optimize Alpine Dockerfile by removing redundant apk commands (#5016)
test_avertex_batch_prediction
build: ui build update (#8553)
fix(team_endpoints.py): fix team info check to handle team keys (#8529)
fix linting
test_mock_openai_retrieve_fine_tune_job
fix test
fix use mock tests for fine tuning api requests to openai
feat(openai/o_series_transformation.py): support native streaming for all openai o-series models (#8552)
Org Flow Improvements (#8549)
test fix openai batches and files
(perf) Fix memory leak on `/completions` route (#8551)
bump: version 1.61.3 → 1.61.4
fix(general_settings.tsx): filter out empty dictionaries post fallback delete (#8550)
(Feat) - Add `/bedrock/meta.llama3-3-70b-instruct-v1:0` tool calling support + cost tracking + base llm unit test for tool calling (#8545)
UI Fixes and Improvements (02/14/2025) p1  (#8546)
docs litellm x langfuse cookbook
update cookbook
docs litellm langfuse cookbook
cookbook litellm proxy langfuse (#8541)
docs(perplexity.md): removing `return_citations` documentation (#8527)
bump: version 1.61.2 → 1.61.3
ui new build
(UI) - Refactor View Key Table (#8526)
Litellm dev 02 13 2025 p2 (#8525)
fix(utils.py): fix vertex ai optional param handling (#8477)
bump: version 1.61.1 → 1.61.2
fix(router.py): add more deployment timeout debug information for tim… (#8523)
Add UI Support for Admins to Call /cache/ping and View Cache Analytics (#8475) (#8519)
(UI) fix log details page  (#8524)
Added custom_attributes to additional_keys which can be sent to athina (#8518)
add phoenix docs for observability integration (#8522)
Fix get_complete_url
UI Fixes p2  (#8502)
test: fix test
Add streaming test
Apply streaming-related transformations only for generate config
Litellm dev 02 12 2025 p1 (#8494)
fix: fix test (#8501)
test_async_router_context_window_fallback
test_supports_tool_choice
Litellm UI stable version 02 12 2025 (#8497)
(Redis Cluster) - Fixes for using redis cluster + pipeline (#8442)
ci(config.yml): mark daily docker builds with `-nightly` (#8499)
test_async_router_context_window_fallback
fix prom check startup (#8492)
pplx - fix supports tool choice openai param (#8496)
fix test_async_router_context_window_fallback
update load testing script
add sonar pricings (#8476)
docs: fix docs
docs(token_auth.md): clarify scopes can be a list or comma separated string
build(model_prices_and_context_window.json): handle azure model update
fix naming docker stable release
(Bug fix) - Using `include_usage` for /completions requests + unit testing (#8484)
Improved wildcard route handling on `/models` and `/model_group/info`  (#8473)
bump: version 1.61.0 → 1.61.1
ui new build
(round 4 fixes) - Team model alias setting (#8474)
fix e2e ui testing linting error
(UI) allow adding model aliases for teams (#8471)
fix test_aview_spend_per_user
Ui Fixes Teams  Setting #8347 (#8353)
Log applied guardrails on LLM API call (#8452)
Show Guardrails on UI  (#8447)
test: skip redundant test
test: update test
fix: fix linting error
style(guardrails.tsx): style improvements to guardrails ui
fix(guardrails.tsx): show guardrail name, status, mode on ui
feat(guardrails.py): return specific litellm params in `/guardrails/list` endpoint
fix(guardrails.tsx): style improvements
feat(guardrails.tsx): show configured guardrails on proxy ui
feat(leftnav.tsx): initial commit adding guardrails tab on UI
new ui build
(Feat) - Allow viewing Request/Response Logs stored in GCS Bucket (#8449)
(e2e testing) - add tests for using litellm `/team/` updates in multi-instance deployments with Redis (#8440)
Org UI Improvements (#8436)
Litellm dev 02 10 2025 p2 (#8443)
Litellm dev 02 10 2025 p1 (#8438)
bump: version 1.60.9 → 1.61.0
build: ui updates
Allow org admin to create teams on UI  (#8407)
Allow editing model api key + provider on UI  (#8406)
Revert "Added filter in Teams and fixed spacing & height issues in Teams tabs…" (#8416)
Added filter in Teams and fixed spacing & height issues in Teams tabs (#6192) (#8357)
bump: version 1.60.8 → 1.60.9
added gemini 2.0 models (#8412)
(Bug Fix) - Bedrock completions with aws_region_name (#8384)
expose port & required env vars & instructions for running in dev (#8404)
(Feat) - Allow calling Nova models on `/bedrock/invoke/` (#8397)
add supports_response_schema for /bedrock/nova models
bump: version 1.60.7 → 1.60.8
Litellm dev 02 07 2025 p3 (#8387)
(Feat) - Add `/bedrock/invoke` support for all Anthropic models  (#8383)
Anthropic Citations API Support (#8382)
Reimplement methods required for triton streaming
fix supports_response_schema bedrock/anthropic models
Handle azure deepseek reasoning response (#8288) (#8366)
Litellm dev 02 07 2025 p2 (#8377)
fix: add azure/o1-2024-12-17 to model_prices_and_context_window.json (#8371)
fix: dictionary changed size during iteration error (#8327) (#8341)
fix(nvidia_nim/embed.py): add 'dimensions' support (#8302)
build(pyproject.toml): bump version
Update deepseek API prices for 2025-02-08 (#8363)
build(ui): updates
bump: version 1.60.6 → 1.60.7
build(ui/): update ui build
Fix azure max retries error  (#8340)
Litellm dev 02 06 2025 p3 (#8343)
UI Updates (#8345)
docs assembly ai
fix assembly pass through cost tracking
docs assembly ai eu endpoints
bump: version 1.60.5 → 1.60.6
fix test_get_model_info_gemini
ui new build
(QA+UI) - e2e flow for adding assembly ai passthrough endpoints (#8337)
(bug fix router.py) - safely handle `choices=[]` on llm responses  (#8342)
databricks/meta-llama-3.3-70b-instruct
fix(utils.py): handle key error in msg validation (#8325)
Fixed meta llama 3.3 key for Databricks API (#8093)
(Bug Fix - Langfuse) - fix for when model response has `choices=[]` (#8339)
Update local_debugging.md (#8308)
Fix pricing for Gemini 2.0 Flash 001 (#8320)
Add aistudio GEMINI 2.0 to model_prices_and_context_window.json (#8335)
Add Arize Cookbook for Turning on LiteLLM Proxy (#8336)
Add gemini-2.0-flash pricing + model info  (#8303)
fix: docs links (#8294)
Improve rpm check on keys  (#8301)
Fix edit team on ui  (#8295)
Azure OpenAI improvements - o3 native streaming, improved tool call + response format handling  (#8292)
bump: version 1.60.4 → 1.60.5
fix test_models_by_provider
(Refactor) - migrate bedrock invoke to `BaseLLMHTTPHandler` class (#8290)
fixed issues #8126 and #8127 (#8275) (#8299)
fix add back sambanova/Qwen2.5-72B-Instruct
add assembly ai cost tracking (#8298)
(UI) - Add Assembly AI provider to UI  (#8297)
(Security fix) - remove code block that inserts master key hash into DB  (#8268)
Added compatibility guidance, etc. for xAI Grok model (#8282)
Added a guide for users who want to use LiteLLM with AI/ML API. (#7058)
Litellm staging (#8270)
Fix deepseek calling - refactor to use base_llm_http_handler (#8266)
run ci/cd again
bump: version 1.60.3 → 1.60.4
[BETA] Support OIDC `role` based access to proxy (#8260)
fix(internal_user_endpoints.py): fix try-except for team not in db
(Feat) - Add support for structured output on `bedrock/nova` models + add util `litellm.supports_tool_choice` (#8264)
build: Squashed commit of the following:
build: update .gitignore
ci/cd run again
all e2e langfuse tests now run on test_langfuse_e2e_test.p
Fix passing top_k parameter for Bedrock Anthropic models (#8131) (#8269)
ui new build
(Bug fix) - Langfuse / Callback settings stored in DB (#8251)
(feat) - track org_id in SpendLogs (#8253)
add supports_tool_choice (#8265)
fix(prometheus.py): fix setting key budget metrics (#8234)
Add hyperbolic deepseek v3 model configurations (#8232)
Feedback
add supports_response_schema
fix loosen httpx restriction on pip (#8255)
Update model_prices_and_context_window.json (#8256)
Update model_prices_and_context_window.json (#8249)
Litellm UI fixes 8123 v2 (#8208) (#8245)
Internal User Endpoint - vulnerability fix + response type fix  (#8228)
bump: version 1.60.2 → 1.60.3
fix(openai/): allows 'reasoning_effort' param to be passed correctly (#8227)
bump: version 1.60.1 → 1.60.2
fix test test_is_assemblyai_route
(Feat) - New pass through add assembly ai passthrough endpoints (#8220)
docs: fix typo in lm_studio.md (#8222)
fix(factory.py): fix bedrock http:// handling
test: update test to handle gemini token counter change
test(base_llm_unit_tests.py): add test to ensure drop params is respe… (#8224)
Add xAI and fix some old model config (#8218)
ui new build
(UI Fixes for add new model flow) (#8216)
Fix max output tokens (#8207)
build: ui updates (#8206)
docs: Updating the available VoyageAI models in the docs (#8215)
Added deprecation date for gemini-1.5 models (#8210)
LiteLLM Minor Fixes & Improvements (01/16/2025) - p2  (#7828)
build: bump version
Easier user onboarding via SSO  (#8187)
Complete o3 model support  (#8183)
fix(main.py): fix passing openrouter specific params (#8184)
Azure AI Foundry - Deepseek R1 (#8188)
(UI + SpendLogs) - Store SpendLogs in UTC Timezone, Fix filtering logs by start/end time (#8190)
[Bug Fix] - `/vertex_ai/` was not detected as llm_api_route on pass through but `vertex-ai` was (#8186)
docs update log stream event
Update bedrock.md
ui new build
Add azure/deepseek-r1 to model_prices_and_content_window.json (#8177)
test: add more unit testing for team member endpoints (#8170)
Improved O3 + Azure O3 support (#8181)
Litellm dev contributor prs 01 31 2025 (#8168)
build: bump root schema.prisma
build(schema.prisma): add new `sso_user_id` to LiteLLM_UserTable (#8167)
Litellm dev 01 31 2025 p2 (#8164)
fix linting error
doc fix
Adjust model pricing metadata (#8147)
bump: version 1.59.12 → 1.60.0
new release
Adding Azure OpenAI o3-mini costs & specs (#8166)
(Feat) add bedrock/deepseek custom import models (#8132)
(UI) Allow adding custom pricing when adding new model (#8165)
(Fixes) OpenAI Streaming Token Counting + Fixes usage track when `litellm.turn_off_message_logging=True` (#8156)
build: fix model cost map (#8153)
FEATURE: OpenAI o3-mini (#8151)
docs: fix dead links
New stable release - release notes (#8148)
bump: version 1.59.11 → 1.59.12
Doc updates + management endpoint fixes  (#8138)
Litellm dev 01 30 2025 p2 (#8134)
Doc updates - add key rotations to docs (#8136)
fix test_generate_and_update_key
bump: version 1.59.10 → 1.59.11
ui new build
fix: propagating json_mode to acompletion (#8133)
(UI) fix adding Vertex Models (#8129)
(Refactor / QA) - Use `LoggingCallbackManager` to append callbacks and ensure no duplicate callbacks are added (#8112)
[Fix Ui ]: Taha-hexcode (#8082) (#8122)
fix test gemini-2.0-flash-thinking-exp-01-21
(UI) Fix SpendLogs page - truncate `bedrock` models + show `end_user` (#8118)
add groq/deepseek-r1-distill-llama-70b (#8078)
adding new model (#8089)
fix code qa check
feat(databricks/chat/transformation.py): add tools and 'tool_choice' param support (#8076)
docs(bedrock.md): update docs to show how to use converse like route for internal proxy usage
run ci/cd again
Litellm dev 01 29 2025 p2 (#8102)
Litellm dev 01 29 2025 p1 (#8097)
ui chat ui fixes (#8105)
test_generate_and_update_key
bump: version 1.59.9 → 1.59.10
ui new build
ui new build
ci/cd run again
(UI) Allow using a model / credentials for pass through routes  (#8099)
(Feat) pass through vertex - allow using credentials defined on litellm router for vertex pass through (#8100)
(UI) - View Logs Page - Refinement  (#8087)
ci/cd run again
test fix test_async_create_batch use only openai for testing, hitting azure limits
ui new build
(UI) Fixes for Adding model page - keep existing page as default, have 2nd tab for wildcard models (#8073)
(fix) - proxy reliability, ensure duplicate callbacks are not added to proxy  (#8067)
(beta ui - spend logs view fixes & Improvements 1)  (#8062)
ui new build
ui fix selecting a provider
Litellm dev 01 27 2025 p3 (#8047)
Fix bedrock model pricing + add unit test using bedrock pricing api  (#7978)
Bedrock document processing fixes (#8005)
fix(utils.py): handle failed hf tokenizer request during calls (#8032)
fix test test_stream_chunk_builder_openai_audio_output_usage - use direct dict comparison
feat(handle_jwt.py): initial commit adding custom RBAC support on jwt… (#8037)
deepseek api testing - deepseek is currently hanging
set timeout for deepseek testing
(doc) Add nvidia as provider (#8023)
fix stale issue mgmt
fix stale issue mgmt
action for stale (#8045)
bump: version 1.59.8 → 1.59.9
ui new build
fix add model flow (#8043)
docs smol agents
fix smol agents doc
Add smolagents (#8026)
(UI) - allow assigning wildcard models to a team / key (#8041)
add openrouter/deepseek/deepseek-r1 (#8038)
(UI enhancement) - allow onboarding wildcard models on UI  (#8034)
(UI) - Adding new models enhancement - show provider logo  (#8033)
docs(bedrock.md): cleanup doc
Bing Search Pass Thru
Litellm dev 01 25 2025 p4 (#8006)
Fix custom pricing - separate provider info from model info  (#7990)
bump: version 1.59.7 → 1.59.8
(Fix) langfuse - setting `LANGFUSE_FLUSH_INTERVAL`  (#8007)
(QA / testing) - Add e2e tests for key model access auth checks (#8000)
fix check on guardrails (#8008)
Litellm dev 01 25 2025 p2 (#8003)
(Feat) set guardrails per team  (#7993)
(Prometheus) - emit key budget metrics on startup (#8002)
(QA / testing) - Add unit testing for key model access checks  (#7999)
add type annotation for litellm.api_base (#7980) (#7994)
docs(bedrock.md): add doc on calling bedrock via internal gateway
refactor: cleanup dead codeblock (#7936)
linting fix
Litellm dev 01 24 2025 p4 (#7992)
test_init_custom_logger_compatible_class_as_callback
bump: version 1.59.6 → 1.59.7
Ensure base_model cost tracking works across all endpoints  (#7989)
fix(spend_tracking_utils.py): revert api key pass through fix (#7977)
ui new build
(Feat) - Add GCS Pub/Sub Logging integration for sending DB `SpendLogs` to BigQuery (#7976)
(UI) - Usage page show days when spend is 0 and round spend figures on charts to 2 sig figs (#7991)
(Testing) e2e testing for team budget enforcement checks (#7988)
fix(langsmith.py): add `/api/v1` to langsmith base url
Revert "test_team_and_key_budget_enforcement"
test_team_and_key_budget_enforcement
(Feat) - allow setting `default_on` guardrails  (#7973)
docs - store_prompts_in_spend_logs
Ollama ssl verify = False + Spend Logs reliability fixes (#7931)
Retry for replicate completion response of status=processing (#7901) (#7965)
Add datadog health check support + fix bedrock converse cost tracking w/ region name specified (#7958)
ui new build -
bump: version 1.59.5 → 1.59.6
Litellm dev 01 23 2025 p2 (#7962)
fix LiteLLM_ManagementEndpoint_MetadataFields
gcs bucket dont run truncation (#7964)
(UI) Set guardrails on Team Create and Edit page (#7963)
(Feat) allow setting guardrails on a team on the API  (#7959)
(Feat) - emit `litellm_team_budget_reset_at_metric` and `litellm_api_key_budget_remaining_hours_metric` on prometheus  (#7946)
test_chat_completion_ratelimit add retry on test
fix code quality check
(UI) - Set/edit guardrails on a virtual key  (#7954)
ui_view_spend_logs (#7952)
docs: fix typo (#7953)
(Testing + Refactor) - Unit testing for team and virtual key budget checks  (#7945)
Revert "set litellm_team_budget_reset_at_metric"
set litellm_team_budget_reset_at_metric
add litellm_team_budget_reset_at_metric
Add `attempted-retries` and `timeout` values to response headers + more testing (#7926)
bump: version 1.59.4 → 1.59.5
fix test_async_create_batch
(Refactor) Langfuse - remove `prepare_metadata`, langfuse python SDK now handles non-json serializable objects (#7925)
Litellm dev 01 22 2025 p4 (#7932)
fix(utils.py): move adding custom logger callback to success event in… (#7905)
add deepseek-reasoner (#7935)
Litellm dev 01 22 2025 p1 (#7933)
test: skip test - Bedrock now supports this behavior
test: mock fireworks ai test - unstable api
Revert "fix: fix test"
fix: fix test
build(deps): bump undici from 6.21.0 to 6.21.1 in /docs/my-website (#7902)
(Testing) - Add e2e testing for langfuse logging with tags (#7922)
docs on custom tags with langfuse
docs fix order of logging integrations
done
bump: version 1.59.3 → 1.59.4
Deepseek r1 support + watsonx qa improvements (#7907)
bump: version 1.59.2 → 1.59.3
fix litellm_overhead_latency_metric
Update MLflow calllback and documentation (#7809)
litellm_overhead_latency_metric
fix set_llm_deployment_success_metrics
bump: version 1.59.1 → 1.59.2
(Feat - prometheus) - emit `litellm_overhead_latency_metric` (#7913)
Litellm dev 01 21 2025 p1 (#7898)
(Code quality) - Ban recursive functions in codebase (#7910)
(Feat) Add x-litellm-overhead-duration-ms and "x-litellm-response-duration-ms" in response from LiteLLM  (#7899)
(fix langfuse tags) - read tags from `StandardLoggingPayload` (#7903)
(Bug fix) - Allow setting `null` for `max_budget`, `rpm_limit`, `tpm_limit` when updating values on a team (#7912)
fix: add default credential for azure (#7095) (#7891)
fix(proxy_server.py): fix get model info when litellm_model_id is set + move model analytics to free (#7886)
(e2e testing + minor refactor) - Virtual Key Max budget check (#7888)
Litellm dev 01 20 2025 p3 (#7890)
Litellm dev 01 20 2025 p1 (#7884)
(Feat) `datadog_llm_observability` callback - emit `request_tags` on logs  (#7883)
fix(fireworks_ai/): fix global disable flag with transform messages helper (#7847)
feat: add new together_ai models (#7882)
typo fix README.md (#7879)
Fix typo Update alerting.md (#7880)
add us.amazon.nova-lite-v1:0 to model cost map
docs - Custom Retention Policies
docs Data Retention Policy
JWT Auth - `enforce_rbac` support + UI team view, spend calc fix (#7863)
Auth checks on invalid fallback models  (#7871)
add bedrock stability to model_prices_and_context_window.json (#7869)
feat(health_check.py): set upperbound for api when making health check call (#7865)
LiteLLM Minor Fixes & Improvements (01/18/2025) - p1 (#7857)
docs data sec
litellm sec scans (#7864)
litellm security page
docs Security Certifications
docs data privacy
fix python 3 install / usage
bump: version 1.59.0 → 1.59.1
ui new build
Revert "Remove UI build output" (#7861)
(UI Logs) - add pagination + filtering by key name/team name (#7860)
(UI - View Logs Table) - Show country of origin for logs  (#7856)
Fix: Problem with langfuse_tags when using litellm proxy with langfus… (#7825)
build(ui/): update ui
fix(admins.tsx): fix logic for getting base url and create common get base url component (#7854)
e2e ui testing fixes
refactor (#7851)
fix
remore ui build
LiteLLM Minor Fixes & Improvements (2024/16/01)  (#7826)
ui release note
ui logs - view messages / responses
Improve Proxy Resiliency: Cooldown single-deployment model groups if 100% calls failed in high traffic (#7823)
`/key/delete` - allow team admin to delete team keys  (#7846)
refactor: make bedrock image transformation requests async (#7840)
ui - new build
view logs
QA: ensure all bedrock regional models have same `supported_`  as base + Anthropic nested pydantic object support (#7844)
bump: version 1.58.4 → 1.59.0
[Hashicorp - secret manager] - use vault namespace for tls auth  (#7834)
ui new build
rename ui tab to ExperimentOutlined
ui new build
[fix dd llm obs] - use env vars for setting dd tags, service name (#7835)
(UI - View SpendLogs Table)  (#7842)
fix(key_management_endpoints.py): fix default allowed team member roles (#7843)
add key and team level budget (#7831)
bump: version 1.58.3 → 1.58.4
Revert "fix custom logger"
fix custom logger
Revert "fix: fix test"
Revert "test_completion_mistral_api_mistral_large_function_call"
fix: fix test
_handle_tool_call_message linting
test_watsonx_token_in_env_var
test_completion_mistral_api_mistral_large_function_call
sec fix minor (#7810)
(Fix + Testing) - Add `dd-trace-run` to litellm ci/cd pipeline + fix bug caused by `dd-trace` patching OpenAI sdk  (#7820)
(fix) IBM Watsonx using ZenApiKey (#7821)
(datadog llm observability) - fixes + improvements for using `datadog llm observability` logging integration  (#7824)
run ci/cd again
test_completion_mistral_api_mistral_large_function_call
test_async_vertexai_streaming_response
llama-v3p1-8b-instruct
fireworks ai use llama-v3p1-8b-instruct
test: fix unit test
test commit on main
build: bump certifi version - see if that fixes asyncio ssl issue on python 3.13 (#7800)
test: initial commit enforcing testing on all anthropic pass through … (#7794)
test: initial test to enforce all functions in user_api_key_auth.py h… (#7797)
Add back in non root image fixes (#7781) (#7795)
Litellm dev 01 14 2025 p2 (#7772)
(helm) - allow specifying envVars on values.yaml + add helm lint test (#7789)
Update instructor tutorial (#7784)
Fix wrong URL for internal user invitation (#7762)
[integrations/lunary] Improve Lunary documentaiton (#7770)
fix confusing save button label (#7778)
feat(helm): add securityContext and pull policy values to migration job (#7652)
build: bump: version 1.58.2 → 1.58.3
Litellm dev 01 2025 p4 (#7776)
build(pyproject.toml): bump uvicorn depedency requirement (#7773)
bump: version 1.58.1 → 1.58.2
(fix) `BaseAWSLLM`  - cache IAM role credentials when used  (#7775)
(Feat) prometheus - emit remaining team budget metric on proxy startup (#7777)
docs iam role based access for bedrock (#7774)
Litellm dev 01 13 2025 p2 (#7758)
Support temporary budget increases on keys  (#7754)
Litellm dev 01 14 2025 p1 (#7771)
fix (#7769)
docs benchmark
update benchmarks
bump: version 1.58.0 → 1.58.1
(fix) health check - allow setting `health_check_model` (#7752)
(prometheus - minor bug fix) - `litellm_llm_api_time_to_first_token_metric` not populating for bedrock models (#7740)
(litellm SDK perf improvements) - handle cases when unable to lookup model in model cost map  (#7750)
fix http parsing utils (#7753)
(core sdk fix) - fix fallbacks stuck in infinite loop (#7751)
huggingface/mistralai/Mistral-7B-Instruct-v0.3
(docs) Update vertex.md old code example
test_team_access_groups
bump: version 1.57.11 → 1.58.0
(proxy perf) - only read request body 1 time per request (#7728)
fix svc logger (#7727)
bump: version 1.57.10 → 1.57.11
add azure o1 pricing (#7715)
Revert "fix _read_request_body to re-use parsed body already (#7722)" (#7724)
fixes for img gen cost cal
fix img gen cost
use set for public routes
fix optimize get llm provider
fix _read_request_body to re-use parsed body already (#7722)
(litellm sdk speedup) - use `_model_contains_known_llm_provider` in `response_cost_calculator` to check if the model contains a known litellm provider (#7721)
(litellm SDK perf improvement) - use `verbose_logger.debug` and `_cached_get_model_info_helper` in `_response_cost_calculator`  (#7720)
(litellm sdk speedup router) - adds a helper `_cached_get_model_group_info` to use when trying to get deployment tpm/rpm limits (#7719)
bump: version 1.57.9 → 1.57.10
[BETA] Add OpenAI `/images/variations` + Topaz API support (#7700)
(perf) - only use response_cost_calculator 1 time per request. (Don't re-use the same helper twice per call ) (#7709)
(sdk perf fix) - only print args passed to litellm when debugging mode is on  (#7708)
fix _read_request_body (#7706)
fix get_llm_provider for aiohttp openai
(perf sdk) - minor changes to cost calculator to run helpers only when necessary (#7704)
use _get_model_info_helper (#7703)
Litellm dev 01 11 2025 p3 (#7702)
docs(enterprise.md): cleanup docs and add faq
docs(enterprise.md): clarify sla for patching vulnerabilities
fix(model_hub.tsx): clarify cost in model hub is per 1m tokens (#7687)
docs: new release notes
docs(logging.md): docs(logging.md): add docs on s3 bucket logging with team alias prefix
build: new ui build (#7685)
bump: version 1.57.8 → 1.57.9
Litellm dev 01 10 2025 p3 (#7682)
Litellm dev 01 10 2025 p2 (#7679)
fix showing release notes
bump: version 1.57.7 → 1.57.8
[Bug fix]: Proxy Auth Layer - Allow Azure Realtime routes as llm_api_routes (#7684)
fix proxy pre call hook - only use if user is using alerting (#7683)
uvicorn allow setting num workers (#7681)
(performance improvement - litellm sdk + proxy) - ensure litellm does not create unnecessary threads when running async functions  (#7680)
LiteLLM Minor Fixes & Improvements (01/10/2025) - p1 (#7670)
feat: allow to pass custom parent run id (#7651)
(litellm sdk - perf improvement) - optimize `pre_call_check`  (#7673)
(litellm sdk - perf improvement) - use O(1) set lookups for checking llm providers / models  (#7672)
speed up use_custom_pricing_for_model (#7674)
latency fix _cache_key_object (#7676)
bump: version 1.57.6 → 1.57.7
fix uvloop critical fix
docs(config_settings.md): update docs to include new athina env var
Use environment variable for Athina logging URL (#7628)
fix(vertex_ai/gemini/transformation.py): handle 'http://' in gemini p… (#7660)
fix(main.py): fix lm_studio/ embedding routing (#7658)
bump: version 1.57.5 → 1.57.6
build(ui/): update ui build
feat(ui_sso.py): Allows users to use test key pane, and have team budget limits be enforced for their use-case (#7666)
(minor latency fixes / proxy) - use verbose_proxy_logger.debug() instead of litellm.print_verbose (#7664)
bump: version 1.57.4 → 1.57.5
use asyncio tasks for logging db metrics (#7663)
(Feat - Batches API) add support for retrieving vertex api batch jobs (#7661)
(proxy perf improvement) - use `uvloop` for higher RPS (10%-20% higher RPS)  (#7662)
(proxy - RPS) - Get 2K RPS at 4 instances, minor fix `aiohttp_openai/`  (#7659)
fix 1 - latency fix (#7655)
Litellm dev 01 08 2025 p1 (#7640)
LiteLLM Minor Fixes & Improvements (01/08/2025) - p2  (#7643)
docs(intro.md): add a section on 'why pass through endpoints'
build(model_prices_and_context_window.json): omni-moderation-latest-intents
bump: version 1.57.3 → 1.57.4
(helm) - bug fix - allow using `migrationJob.enabled` variable within job (#7639)
fix is llm api route check (#7631)
ci/cd run again
Allow assigning teams to org on UI + OpenAI `omni-moderation` cost model tracking (#7566)
Litellm dev 01 07 2025 p2 (#7622)
(feat) - allow building litellm proxy from pip package (#7633)
fix docs
update load test docs
sort rn
docs v1.57.3
Litellm dev 01 07 2025 p3 (#7635)
fix(utils.py): fix select tokenizer for custom tokenizer (#7599)
update docs
bump: version 1.57.2 → 1.57.3
Litellm dev 01 01 2025 p2 (#7615)
(Feat) soft budget alerts on keys (#7623)
Litellm dev 01 07 2025 p1 (#7618)
(Fix) security of base image  (#7620)
ci/cd run again
update tests
docs: cleanup keys
bump: version 1.57.1 → 1.57.2
`aiohttp_openai/` fixes - allow using `aiohttp_openai/gpt-4o`  (#7598)
ci/cd run again
Litellm dev 01 06 2025 p1 (#7594)
bump: version 1.57.0 → 1.57.1
docs(prompt_management.md): update docs to show how to point to load balanced model name
Litellm dev 01 06 2025 p2 (#7597)
Litellm dev 01 06 2025 p3 (#7596)
Refresh VoyageAI models, prices and context (#7472)
(proxy perf improvement) - remove redundant `.copy()` operation  (#7564)
(Feat) - allow including dd-trace in litellm base image  (#7587)
fix _return_user_api_key_auth_obj (#7591)
(perf) - fixes for aiohttp handler to hit 1K RPS (#7590)
test: skip tests pending vertex credentials
use latest bucket for testing
test: fix test
fix: test
test: cleanup test
use pathrise-convert-1606954137718
ci/cd run again
vertex testing use pathrise-convert-1606954137718
test: update test amazing vertex
ci/cd update vertex acct
Prevent istio injection for db migrations cron job (#7513)
FriendliAI: Documentation Updates (#7517)
add `fireworks_ai/accounts/fireworks/models/deepseek-v3` (#7567)
latency fix proxy (#7563)
build(ui/): build new ui
bump: version 1.56.10 → 1.57.0
Support deleting keys by key_alias  (#7552)
fix(groq/chat/transformation.py): fix groq response_format transformation (#7565)
fix [PROXY] returned data from litellm_pre_call_util (#7558)
Create and view organizations + assign org admins on the Proxy UI  (#7557)
(Feat) Hashicorp Secret Manager - Allow storing virtual keys in secret manager  (#7549)
fix get_custom_logger_compatible_class (#7554)
add cohere/command-r7b-12-2024 (#7553)
(Fix) - Slack Alerting , don't send duplicate spend report when used on multi instance settings  (#7546)
(Fix) - Docker build error with pyproject.toml (#7550)
bump: version 1.56.9 → 1.56.10
feat(router.py): support request prioritization for text completion c… (#7540)
Support checking provider-specific `/models` endpoints for available models based on key  (#7538)
fix(aws_secret_manager_V2.py): Error reading secret from AWS Secrets Manager: (#7541)
bump: version 1.56.8 → 1.56.9
(fix proxy perf) use `_read_request_body` instead of ast.literal_eval to get better performance  (#7545)
[Feature]: - allow print alert log to console (#7534)
Revert "fix: add missing parameters order, limit, before, and after in get_as…" (#7542)
fix _make_common_async_call
test_aiohttp_openai
(fix) `aiohttp_openai/` route - get to 1K RPS on single instance  (#7539)
Add missing prefix for deepseek (#7508)
fix: add missing parameters order, limit, before, and after in get_assistants method for openai (#7537)
Litellm dev 01 02 2025 p1 (#7516)
(Feat) - Hashicorp secret manager, use TLS cert authentication  (#7532)
docs pass through routes
Fix langfuse prompt management on proxy  (#7535)
Bump anthropic.claude-3-5-haiku-20241022-v1:0 to new limits. (#7118)
fix - access metadata (#7523)
fix - don't print hcorp secrets in debug logs (#7529)
(fix) GCS bucket logger - apply truncate_standard_logging_payload_content to standard_logging_payload and ensure GCS flushes queue on fails  (#7519)
fix unused imports
bump: version 1.56.7 → 1.56.8
bump: version 1.56.6 → 1.56.7
fix - use tp executor (#7509)
(perf) use `aiohttp` for `custom_openai`  (#7514)
Revert "(fix) GCS bucket logger - apply `truncate_standard_logging_payload_co…" (#7515)
Litellm dev 01 02 2025 p2 (#7512)
docs enable_pre_call_checks
docs(humanloop.md): add humanloop docs
Litellm dev 01 01 2025 p3 (#7503)
test_aview_spend_per_user
test_aadmin_only_routes
ci/cd run again
ci/cd run again
(fix) GCS bucket logger - apply `truncate_standard_logging_payload_content` to `standard_logging_payload` and ensure GCS flushes queue on fails (#7500)
update litellm_proxy_unit_testing
Litellm dev 01 01 2025 p1 (#7498)
Litellm dev 12 30 2024 p2 (#7495)
ci/cd run again
doc update
(Feat) Add support for reading secrets from Hashicorp vault (#7497)
(docs) Add docs on load testing benchmarks  (#7499)
(Feat) - LiteLLM Use `UsernamePasswordCredential` for Azure OpenAI (#7496)
(feat) POST `/fine_tuning/jobs` support passing vertex specific hyper params  (#7490)
Prometheus - custom metrics support + other improvements  (#7489)
bump: version 1.56.5 → 1.56.6
(Feat) - Add PagerDuty Alerting Integration  (#7478)
fix ollama embedding model response #7451 (#7473)
Added missing quote (#7481)
Litellm dev 12 31 2024 p1 (#7488)
Fix team-based logging to langfuse + allow custom tokenizer on `/token_counter` endpoint (#7493)
(docs) Add docs on using Vertex with Fine Tuning APIs  (#7491)
(fix) `v1/fine_tuning/jobs` with VertexAI  (#7487)
bump: version 1.56.4 → 1.56.5
HumanLoop integration for Prompt Management (#7479)
Litellm dev 12 30 2024 p1 (#7480)
doc on streaming usage litellm proxy
test_rerank_response_assertions (#7476)
(fix) `litellm.amoderation` - support using `model=openai/omni-moderation-latest`, `model=omni-moderation-latest`, `model=None`  (#7475)
localeCompare
docs(index.md): fix doc link
Litellm dev 12 28 2024 p1 (#7463)
docs(index.md): add deepgram to release notes
docs(deepgram.md): add table clarifying supported openai endpoint
doc update order
docs(deepgram.md): add deepgram model support to docs
update release note
test_e2e_batches_files
ci(reset_stable.yml): fix to run on release created events
docs(spending_monitoring.md): add section on disabling spend logs to db
update clean up jobs
docs(spend_monitoring.md): cleanup doc
Litellm dev 12 28 2024 p2 (#7458)
update - new test for test_text_completion_health_check
fix ft job test - add resource cleanup
Litellm dev 12 28 2024 p3 (#7464)
fix ahealth_check
cleanup_azure_ft_models
bump: version 1.56.3 → 1.56.4
(Refactor) - Re use litellm.completion/litellm.embedding etc for health checks  (#7455)
(Bug Fix) Add health check support for realtime models  (#7453)
ui new build
fix OR deepseek (#7425)
(Security fix) - Upgrade to `fastapi==0.115.5 `  (#7447)
(Admin UI - 2)  UI chat should render the output in markdown (#7460)
chat ui improvement (#7459)
docs spend monitoring (#7461)
remove unused code (#7456)
Update model_prices_and_context_window.json (#7452)
docs release notes
add keywords
v1.56.3 release notes
build: bump version
Litellm dev 12 27 2024 p2 1 (#7449)
LiteLLM Minor Fixes & Improvements (12/27/2024) - p1  (#7448)
Update model_prices_and_context_window.json (#7345)
(Feat) - new endpoint `GET /v1/fine_tuning/jobs/{fine_tuning_job_id:path}`  (#7427)
(Bug fix) missing `model_group` field in logs for aspeech call types  (#7392)
docs guardrails
docs add guardrail spec
docs update gemini/ link
Add Gemini embedding doc (#7436)
✨ (Feat) Log Guardrails run, guardrail response on logging integrations  (#7445)
ci/cd run again
add openrouter o1 (#7424)
(feat) `/guardrails/list` show guardrail info params  (#7442)
test_langfuse_logging_audio_transcriptions
fix ft testing
Revert "Refresh VoyageAI models and prices and context (#7443)" (#7446)
Refresh VoyageAI models and prices and context (#7443)
Litellm dev 12 26 2024 p4 (#7439)
ci/cd run again
e2e_ui_testing
fix(key_management_endpoints.py): enforce user_id / team_id checks on key generate (#7437)
docs(index.md): new release notes
bump: version 1.56.1 → 1.56.2
build(pyproject.toml): fix pyproject to bump correctly
Litellm dev 12 26 2024 p3 (#7434)
(fix) initializing OTEL Logging on LiteLLM Proxy - ensure OTEL logger is initialized only once  (#7435)
Support budget/rate limit tiers for keys  (#7429)
docs guardrail params (#7430)
(docs) - show all supported Azure OpenAI endpoints in overview  (#7428)
(Feat) Add logging for `POST v1/fine_tuning/jobs`  (#7426)
docs: cleanup doc
docs(fireworks_ai.md): add audio transcription to fireworks ai doc
fix if "/openai/" in route:
bump: version 1.56.0 → 1.56.1
Add `/openai` pass through route on litellm proxy (#7412)
docs - batches cost tracking (#7422)
(security fix) - bump fast api, fastapi-sso, python-multipart -  fix snyk vulnerabilities (#7417)
Litellm dev 12 25 2024 p3 (#7421)
Litellm dev 12 25 2025 p2 (#7420)
Litellm dev 12 25 2024 p1 (#7411)
fix docs warning (#7419)
ui - bump sec issues (#7418)
docs: cleanup docker compose comments (#7414)
(feat) Support Dynamic Params for `guardrails`  (#7415)
update docs base docker
docs files api
Litellm dev 12 24 2024 p2 (#7400)
bump: version 1.55.11 → 1.55.12
Litellm dev 12 24 2024 p4 (#7407)
(Feat) add `"/v1/batches/{batch_id:path}/cancel" endpoint  (#7406)
Litellm dev 12 24 2024 p3 (#7403)
(feat) `/batches` - track `user_api_key_alias`, `user_api_key_team_alias` etc for /batch requests  (#7401)
(feat) `/batches` Add support for using `/batches` endpoints in OAI format  (#7402)
test: override openai o1 prompt caching test - openai backend not caching right now
test(test_cost_calc.py): fix test to handle llm api errors
Add 'end_user', 'user' and 'requested_model' on more prometheus metrics (#7399)
bump: version 1.55.10 → 1.55.11
LiteLLM Minor Fixes & Improvements (12/23/2024) - p3 (#7394)
update release notes
update release notes
release notes
docs batches
docs add files to supported endpoints
bump: version 1.55.9 → 1.55.10
dd logger fix - handle objects that can't be JSON dumped (#7393)
test_router_get_available_deployments
(feat) Add cost tracking for /batches requests OpenAI  (#7384)
(feat) Add basic logging support for `/batches` endpoints  (#7381)
(Feat) Add input_cost_per_token_batches, output_cost_per_token_batches for OpenAI cost tracking Batches API  (#7391)
[Bug Fix]: Errors in LiteLLM When Using Embeddings Model with Usage-Based Routing (#7390)
LiteLLM Minor Fixes & Improvements (12/23/2024) - P2  (#7386)
Litellm dev 12 23 2024 p1 (#7383)
(security fix) - update base image for all docker images to `python:3.13.1-slim`  (#7388)
cleanup ui folder (#7363)
Complete 'requests' library removal  (#7350)
add img to release notes
update release notes
Litellm docs update (#7365)
docs
docs add 1.55.8 changelog
release notes v1.55.8
Document team admins + Enforce assigning team admins as an enterprise feature (#7359)
Litellm enforce enterprise features (#7357)
test fix
ui - new build
fix(__init__.py): correctly return azure_text models in models_by_provider dictionary
apply linting fixes
(Admin UI) - maintain history on chat UI (#7351)
(Admin UI) correctly render provider name in /models with wildcard routing  (#7349)
(chore) - enforce model budgets on virtual keys as enterprise feature  (#7353)
(refactor) - fix from enterprise.utils import ui_get_spend_by_tags  (#7352)
(Admin UI) - Test Key Tab - Allow using `UI Session` instead of manually creating a virtual key  (#7348)
(Admin UI) - Test Key Tab - Allow typing in `model` name  + Add wrapping for text response  (#7347)
ci(reset_stable.yml): modify to work with all kinds of releases
use helper for image gen tests (#7343)
(fix) LiteLLM Proxy fix GET `/files/{file_id:path}/content"` endpoint  (#7342)
bump: version 1.55.8 → 1.55.9
Litellm dev 2024 12 20 p1 (#7335)
Litellm dev 12 20 2024 p3 (#7339)
[Bug fix ]: Triton /infer handler incompatible with batch responses (#7337)
Controll fallback prompts client-side  (#7334)
fix pyproject to v1.55.8
Fix LiteLLM documentation (#7333)
Litellm dev 2024 12 19 p3 (#7322)
fix linting errors
docs: refactor admin ui docs
bump: version 1.55.7 → 1.55.8
Litellm dev 12 19 2024 p2 (#7315)
docs infinity rerank api docs
docs base rerank config
docs add ref prs
(feat) add infinity rerank models  (#7321)
build(reset_stable.yml): rename branch to 'litellm_stable_release_branch'
build(reset_stable.yml): add new workflow to reset litellm_stable to latest release
(code refactor) - Add `BaseRerankConfig`. Use `BaseRerankConfig` for `cohere/rerank` and `azure_ai/rerank` (#7319)
[Bug Fix]: ImportError: cannot import name 'T' from 're' (#7314)
(code quality) run ruff rule to ban unused imports  (#7313)
bump: version 1.55.6 → 1.55.7
o1 - add image param handling (#7312)
bump: version 1.55.5 → 1.55.6
fix use 1 file _PROXY_track_cost_callback (#7304)
(proxy admin ui) - show Teams sorted by `Team Alias`  (#7296)
(feat proxy) v2 - model max budgets  (#7302)
fix(health.md): add rerank model health check information (#7295)
fix(hosted_vllm/transformation.py): return fake api key, if none give… (#7301)
(feat - proxy) Add `status_code` to `litellm_proxy_total_requests_metric_total`  (#7293)
Replace deprecated Pydantic Config class with model_config BerriAI/litellm#6902 (#6903) (#7300)
Litellm dev readd prompt caching (#7299)
added sambanova cloud models (#7187)
update docs
tag budgets fixes
fix _select_model_name_for_cost_calc docstring
Litellm security fixes (#7282)
fix(proxy_server.py): pass model access groups to get_key/get_team mo… (#7281)
build: bump version
(fix) unable to pass input_type parameter to Voyage AI embedding mode  (#7276)
Correct max_tokens on Model DB (#7284)
(feat) proxy Azure Blob Storage - Add support for `AZURE_STORAGE_ACCOUNT_KEY` Auth (#7280)
Add Azure Llama 3.3 (#7283)
LiteLLM Minor Fixes & Improvements (12/16/2024) - p1 (#7263)
Litellm dev 12 17 2024 p3 (#7279)
Litellm dev 12 17 2024 p2 (#7277)
LITELLM: Remove `requests` library usage (#7235)
fix(utils.py): fix openai-like api response format parsing (#7273)
bump: version 1.55.3 → 1.55.4
docs(input.md): document 'extra_headers' param support (#7268)
(feat) Add Bedrock knowledge base pass through endpoints  (#7267)
(feat) Add Azure Blob Storage Logging Integration  (#7265)
docs add response format on main pages
Update README.md
ci/cd run again
docs update
bump: version 1.55.2 → 1.55.3
Litellm dev 12 14 2024 p1 (#7231)
(feat) Add Tag-based budgets on litellm router / proxy  (#7236)
ui new build
Litellm remove circular imports (#7232)
Add new Gemini 2.0 Flash model to Vertex AI. (#7193)
Fix vllm import (#7224)
ui fix tags getting proxy settings (#7234)
(code quality) Add ruff check to ban `print` in repo  (#7233)
ui new build
(UI) Fix Usage Tab - Don't make expensive UI queries after SpendLogs crosses 1M Rows (#7229)
(UI fix) - Allow editing Key Metadata  (#7230)
ui fix key table
ui - new build
(proxy) - Auth fix, ensure re-using safe request body for checking `model` field  (#7222)
fix(main.py): fix retries being multiplied when using openai sdk (#7221)
Litellm add router to base llm testing (#7202)
build(deps): bump nanoid from 3.3.7 to 3.3.8 in /ui/litellm-dashboard (#7216)
(feat - Router / Proxy ) Allow setting budget limits per LLM deployment  (#7220)
Litellm dev 12 13 2024 p1 (#7219)
bump: version 1.55.1 → 1.55.2
Litellm dev 12 11 2024 v2 (#7215)
Litellm dev 12 12 2024 (#7203)
bump: version 1.55.0 → 1.55.1
(fix) latency fix - revert prompt caching check on litellm router  (#7211)
(minor fix proxy) Clarify Proxy Rate limit errors are showing hash of litellm virtual key  (#7210)
fix testing retry audio test 3 times
(feat) UI - Disable Usage Tab once SpendLogs is 1M+ Rows (#7208)
fix: Support WebP image format and avoid token calculation error (#7182)
(docs) Document StandardLoggingPayload Spec  (#7201)
(feat) add `error_code`, `error_class`, `llm_provider`  to `StandardLoggingPayload`  (#7200)
(Feat) DataDog Logger - Add `HOSTNAME` and `POD_NAME` to DataDog logs  (#7189)
build(deps): bump nanoid from 3.3.7 to 3.3.8 in /ui (#7198)
(feat) add `response_time` to StandardLoggingPayload - logged on `datadog`, `gcs_bucket`, `s3_bucket` etc  (#7199)
test: update hf test to check if client closed
ci/cd run release pipeline
fix hf failing streaming test
bump: version 1.54.1 → 1.55.0
fix(acompletion): support fallbacks on acompletion (#7184)
build(model_prices_and_context_window.json): add new dbrx llama 3.3 model
fix test_vertexai_model_garden_model_completion
fix(get_supported_openai_params.py): cleanup (#7176)
fix merge conflicts
build: Squashed commit of https://github.com/BerriAI/litellm/pull/7171
fix merge conflicts
fix - handle merge conflicts
build: Squashed commit of https://github.com/BerriAI/litellm/pull/7170
add enforce_llms_folder_style (#7175)
build: Squashed commit of https://github.com/BerriAI/litellm/pull/7165
(Refactor) Code Quality improvement - rename `text_completion_codestral.py` -> `codestral/completion/` (#7172)
Code Quality Improvement - move `aleph_alpha` to deprecated_providers  (#7168)
Code Quality Improvement - use `vertex_ai/` as folder name for vertexAI  (#7166)
(Refactor) Code Quality improvement  - remove `/prompt_templates/` , `base_aws_llm.py` from `/llms` folder (#7164)
build(deps): bump nanoid from 3.3.7 to 3.3.8 in /docs/my-website (#7159)
Code Quality Improvement - remove `tokenizers/` from /llms (#7163)
Litellm merge pr (#7161)
Litellm vllm refactor (#7158)
Litellm ollama refactor (#7162)
Revert "LiteLLM Common Base LLM Config (pt.4): Move Ollama to Base LLM Config…" (#7160)
Code Quality Improvement - remove `file_apis`, `fine_tuning_apis`  from `/llms`  (#7156)
LiteLLM Common Base LLM Config (pt.4): Move Ollama to Base LLM Config (#7157)
remove symlink (#7155)
fix import
rename `llms/OpenAI/` -> `llms/openai/` (#7154)
refactor(sagemaker/): separate chat + completion routes + make them b… (#7151)
LiteLLM Common Base LLM Config (pt.3): Move all OAI compatible providers to base llm config (#7148)
refactor(fireworks_ai/): inherit from openai like base config (#7146)
(Refactor) Code Quality improvement - stop redefining LiteLLMBase (#7147)
docs: document code quality (#7149)
(Refactor) Code Quality improvement - Use Common base handler for `anthropic_text/` (#7143)
(Refactor) Code Quality improvement - Use Common base handler for Cohere /generate API (#7122)
(Refactor) Code Quality improvement - Use Common base handler for `cloudflare/` provider  (#7127)
(Refactor) Code Quality improvement - Use Common base handler for `clarifai/` (#7125)
(Refactor) Code Quality improvement - use Common base handler for Cohere  (#7117)
Litellm code qa common config (#7113)
bump: version 1.54.0 → 1.54.1
Litellm dev 12 07 2024 (#7086)
fix use consistent naming (#7092)
refactor - use consistent file naming convention `AI21/` -> `ai21`  (#7090)
fix llama-3.3-70b-versatile
ci/cd queue new release
litellm db fixes LiteLLM_UserTable (#7089)
fix deepinfra
bump: version 1.53.9 → 1.54.0
added deepinfra/Meta-Llama-3.1-405B-Instruct (#7084)
(Feat) Add StructuredOutputs support for Fireworks.AI (#7085)
(bug fix) SpendLogs update DB catch all possible DB errors for retrying  (#7082)
Add MLflow to the side bar (#7031)
(feat) Track `custom_llm_provider` in LiteLLMSpendLogs (#7081)
bump: version 1.53.8 → 1.53.9
feat(langfuse/): support langfuse prompt management (#7073)
Litellm dev 12 06 2024 (#7067)
bump: version 1.53.7 → 1.53.8
(feat) Allow enabling logging message / response for specific virtual keys  (#7071)
groq add groq/llama-3.3 (#7076)
Provider Budget Routing - Get Budget, Spend Details (#7063)
litellm not honoring OPENAI_ORGANIZATION env var (#7066)
Correct Vertex Embedding Model Data/Prices (#7069)
feat: Add gemini-exp-1206 model configuration with 2M input tokens (#7064)
LiteLLM Minor Fixes & Improvements (12/05/2024)  (#7051)
(fix) adding public routes when using custom header  (#7045)
ui new build
fix router test_audio_speech_router
ui - use session storage (#7054)
docs show supported providers on batches api doc
(fix) litellm router.aspeech  (#6962)
bye (#6982)
build(model_prices_and_context_window.json): add bedrock region models to model cost map (#7044)
ui new build
fix viewing keys (#7042)
bump: version 1.53.6 → 1.53.7
LiteLLM Minor Fixes & Improvements (12/05/2024) (#7037)
bump: version 1.53.5 → 1.53.6
(feat) add Vertex Batches API support in OpenAI format  (#7032)
add rerank-v3.5 (#7035)
(UI) Load time improvement - Sub 2s load time for Home Page ⚡️ (#7014)
fix application exception during init (#7027)
docs(bedrock.md): clarify what version supports converse/invoke routes
docs(bedrock.md): add nova model notice
docs add FAq to life of a request
docs(azure.md): add proxy spend tracking for azure models to azure docs
fix(main.py): trigger new build
fix: migration job for existing db (#6792)
test: fix test
Add prompt caching flag for Azure OpenAI gpt-4o-2024-08-06 (#7020)
bump: version 1.53.4 → 1.53.5
fix(key_management_endpoints.py): override metadata field value on up… (#7008)
queue new release
test_multilingual_requests
ci/cd run release pipeline
test_multilingual_requests deepseek
bump: version 1.53.3 → 1.53.4
(fix) 'utf-8' codec can't encode characters error on OpenAI  (#7018)
(refactor) - migrate `router.deployment_callback_on_success` to use StandardLoggingPayload  (#7015)
(fix) allow gracefully handling DB connection errors on proxy  (#7017)
(UI) Sub 1s Internal User Tab load time (#7007)
fix - data dog (#7013)
ci: fix order of question on issue template
build: fix test
build(label-mlops.yml): fix check
build(label-mlops.yml): add tag to mlops user requests
ci/cd queue release
fix testing - langfuse apis are flaky, we unit test team / key based logging in test_langfuse_unit_tests.py
fix test_completion_fine_tuned_model
test_transcription
run ci/cd again
mark test_transcription as flaky
handle Azure rate limit errors for test_transcription
Litellm dbrx structured outputs support (#6993)
(fixes) datadog logging - handle 1MB max log size on DD  (#6996)
Litellm 12 02 2024 (#6994)
docs(json_mode.md): update json docs
fix(main.py): fix vertex meta llama api call
test: add retry on flaky test
refactor: replace dbrx with 'openai_like'
test: fix test
feat(databricks/chat): support structured outputs on databricks
bump: version 1.53.2 → 1.53.3
(fixes) datadog logging - handle 1MB max log size on DD  (#6996)
(fix) logging Auth errors on datadog (#6995)
Litellm test ci cd (#6997)
Litellm 12 02 2024 (#6994)
Litellm dev 11 30 2024 (#6974)
fix: trigger new build
Update team_endpoints.py (#6983)
Update bug_report.yml
Update bug_report.yml
Update bug_report.yml
Update feature_request.yml
Update bug_report.yml
Update feature_request.yml
Update feature_request.yml
Update feature_request.yml

2.13.0-25-04-17 / 2025-04-17 12:10:22
No new changes in the release

2.14.0-25-05-16 / 2025-05-16 11:29:13
No new changes in the release

2.14.1-25-05-27 / 2025-05-27 08:37:58
No new changes in the release

2.15.0-25-06-02 / 2025-06-02 10:33:11
No new changes in the release

2.16.0-25-06-06 / 2025-06-06 08:32:37
No new changes in the release

2.17.0-25-07-01 / 2025-07-01 08:39:08
Feat/add vertexai support (#19)

2.17.1-25-07-01 / 2025-07-02 09:29:34
No new changes in the release
